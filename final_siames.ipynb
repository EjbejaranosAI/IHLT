{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EjbejaranosAI/IHLT/blob/main/final_siames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7q5DjXenGU7"
      },
      "source": [
        "# Semantic textual similarity\n",
        "## Final Project IHLT - UPC 2022/2023\n",
        "### Authors : Roberto Ariosa - Edison Bejarano\n",
        "\n",
        "1. Data\n",
        "\n",
        "2. What we are doing?\n",
        "#### Techniques for preprocessing text for similarity comparison\n",
        "\n",
        "- Stemming: is a process that involves reducing words to their base form, or stem, in order to normalize the text and remove variations in word endings. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the stem \"run\" by a stemming algorithm.\n",
        "\n",
        "\n",
        "- Lemmatization: is a process that involves reducing words to their base form, or lemma, in order to normalize the text and remove variations in word endings. Unlike stemming, lemmatization takes into account the context of the word in order to determine its lemma, resulting in more accurate and meaningful reductions. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the lemma \"run\" by a lemmatization algorithm.\n",
        "\n",
        "- Tf-idf weighting: Is a method for assigning a weight to each word in a document based on its relative importance. The weight is calculated by multiplying the term frequency (tf) of the word by the inverse document frequency (idf) of the word across all documents in a corpus. This weighting scheme gives higher weight to words that are more frequent within a document but less frequent across the corpus, making them more important for characterizing the document.\n",
        "\n",
        "- NES : Function used the Natural Language Toolkit (nltk) to identify named entities in a given sentence. The sentence parameter is the sentence in which named entities should be identified, and the binary parameter determines whether named entities should be grouped together or returned as individual tokens. The function returns a set of the named entities and individual words found in the sentence.\n",
        "\n",
        "\n",
        "These techniques can be used in combination with each other or with stopwords removal to preprocess text and improve the accuracy of similarity comparison. For example, you could use stemming or lemmatization to normalize the words in the phrases, and then use tf-idf weighting to assign importance to each word based on its frequency within the phrases and across a larger corpus. This would allow you to compare the similarity of the phrases in a more meaningful and accurate way\n",
        "\n",
        "\n",
        "3. Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPTeKi61rUdk"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FfVq8u93rUdl",
        "outputId": "9987cb3a-f8d4-4871-9ce8-d7207e4aa169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 430 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 604 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 778 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 860 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 952 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.6 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 4.1 MB/s \n",
            "\u001b[?25h/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2022-12-21 23:16:42.209004: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 654 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install -q spacy nltk numpy pandas scikit-learn pyjarowinkler lazypredict ipykernel\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpS_2NKJrUdm"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lazypredict pyjarowinkler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SXMnPM-rk8U",
        "outputId": "73c713db-6920-47a8-e1c6-6a6aaf800143"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lazypredict in /usr/local/lib/python3.8/dist-packages (0.2.12)\n",
            "Requirement already satisfied: pyjarowinkler in /usr/local/lib/python3.8/dist-packages (1.8)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.8/dist-packages (from lazypredict) (0.90)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from lazypredict) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from lazypredict) (7.1.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (from lazypredict) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lazypredict) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightgbm->lazypredict) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from lightgbm->lazypredict) (1.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->lazypredict) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->lazypredict) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->lazypredict) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lazypredict) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67vwZWxBcGZF",
        "outputId": "4d860259-a4f3-47eb-bf53-2e239bef8b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from functools import partial\n",
        "from argparse import Namespace\n",
        "from pyjarowinkler import distance\n",
        "from collections.abc import Iterable\n",
        "from nltk.wsd import lesk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk import pos_tag, ne_chunk, Tree\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import make_scorer\n",
        "from typing import List\n",
        "from lazypredict.Supervised import REGRESSORS, LazyRegressor\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('conll2000')\n",
        "nltk.download('brown')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet_ic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS52eWLnnJvr"
      },
      "source": [
        "## Download data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0hykeMvnNtn",
        "outputId": "51914aeb-8ac8-45cb-d3ef-7d67bcd7a7c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2003  100  2003    0     0  47690      0 --:--:-- --:--:-- --:--:-- 47690\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  122k  100  122k    0     0   505k      0 --:--:-- --:--:-- --:--:--  503k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  115k  100  115k    0     0   345k      0 --:--:-- --:--:-- --:--:--  345k\n"
          ]
        }
      ],
      "source": [
        "#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/trial.tgz https://gebakx.github.io/ihlt/sts/resources/trial.tgz\n",
        "#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/train.tgz https://gebakx.github.io/ihlt/sts/resources/train.tgz\n",
        "#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/test-gold.tgz https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a91zUN7nIF_h"
      },
      "source": [
        "# Bring data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpDBKAHfpe-Z",
        "outputId": "5a78545a-65b6-426b-bfc4-b9d9c590cf20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/\n",
            "train/00-readme.txt\n",
            "train/STS.output.MSRpar.txt\n",
            "train/STS.input.SMTeuroparl.txt\n",
            "train/STS.input.MSRpar.txt\n",
            "train/STS.gs.MSRpar.txt\n",
            "train/STS.input.MSRvid.txt\n",
            "train/STS.gs.MSRvid.txt\n",
            "train/correlation.pl\n",
            "train/STS.gs.SMTeuroparl.txt\n",
            "trial/\n",
            "trial/STS.input.txt\n",
            "trial/00-readme.txt\n",
            "trial/STS.gs.txt\n",
            "trial/STS.ouput.txt\n",
            "test-gold/\n",
            "test-gold/STS.input.MSRpar.txt\n",
            "test-gold/STS.gs.MSRpar.txt\n",
            "test-gold/STS.input.MSRvid.txt\n",
            "test-gold/STS.gs.MSRvid.txt\n",
            "test-gold/STS.input.SMTeuroparl.txt\n",
            "test-gold/STS.gs.SMTeuroparl.txt\n",
            "test-gold/STS.input.surprise.SMTnews.txt\n",
            "test-gold/STS.gs.surprise.SMTnews.txt\n",
            "test-gold/STS.input.surprise.OnWN.txt\n",
            "test-gold/STS.gs.surprise.OnWN.txt\n",
            "test-gold/STS.gs.ALL.txt\n",
            "test-gold/00-readme.txt\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/Colab_Notebooks_personal/2_IHLT/final_project/train.tgz .\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks_personal/2_IHLT/final_project/trial.tgz .\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks_personal/2_IHLT/final_project/test-gold.tgz .\n",
        "\n",
        "!tar zxvf /content/train.tgz\n",
        "!tar zxvf /content/trial.tgz \n",
        "!tar zxvf /content/test-gold.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLgFj_E7nKdQ"
      },
      "source": [
        "# Usesful functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwjS8RDVrUdq"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ #\n",
        "# Jaccard similarity Function\n",
        "# ------------------------------ #\n",
        "def jaccard_similarity(s1: List[str], s2: List[str]):\n",
        "    return 1 - jaccard_distance(set(s1), set(s2))\n",
        "\n",
        "# ------------------------------ #\n",
        "# Jaccard Similarity List\n",
        "# ------------------------------ #\n",
        "def jaccard_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n",
        "    sims = []\n",
        "    for l1, l2 in zip(s1, s2):\n",
        "        sim = jaccard_similarity(l1, l2)\n",
        "        sims.append(sim)\n",
        "    return np.array(sims)\n",
        "\n",
        "\n",
        "def dice_similarity(s1, s2):\n",
        "    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n",
        "    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n",
        "    s1 = set(s1)\n",
        "    s2 = set(s2)\n",
        "    intersection = s1.intersection(s2)\n",
        "    return 2 * len(intersection) / (len(s1) + len(s2))\n",
        "\n",
        "def dice_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n",
        "    sims = []\n",
        "    for l1, l2 in zip(s1, s2):\n",
        "        sim = dice_similarity(l1, l2)\n",
        "        sims.append(sim)\n",
        "    return np.array(sims)\n",
        "\n",
        "# ------------------------------ #\n",
        "# Jarowinkler Similarity\n",
        "# ------------------------------ #   \n",
        "def calculateJarowinklerSimilarity(dataframe, column1, column2):\n",
        "\n",
        "    aux = []\n",
        "    for row in dataframe.itertuples():\n",
        "            \n",
        "        # Longest one selected\n",
        "        if len(row[column1]) >= len(row[column2]):\n",
        "            sentence1 = row[column1]\n",
        "            sentence2 = row[column2]\n",
        "        else:\n",
        "            sentence1 = row[column2]\n",
        "            sentence2 = row[column1]\n",
        "\n",
        "        similarities_array = []\n",
        "        for word1 in sentence1:\n",
        "            max = 0\n",
        "\n",
        "        for word2 in sentence2:\n",
        "            similarity = distance.get_jaro_distance(str(word1), str(word2), winkler=True, scaling=0.1)\n",
        "            \n",
        "            if max < similarity:\n",
        "                max = similarity\n",
        "            \n",
        "        similarities_array.append(max)\n",
        "\n",
        "        aux.append(np.array(similarities_array).mean())\n",
        "\n",
        "    return aux\n",
        "\n",
        "# ------------------------------ #\n",
        "#       Overlap Similarity\n",
        "# ------------------------------ # \n",
        "def overlap_distance(sentence1, sentence2):\n",
        "  # Zip the characters from the two strings together\n",
        "  pairs = zip(sentence1, sentence2)\n",
        "\n",
        "  # Initialize a counter for the overlap distance\n",
        "  overlap = 0\n",
        "\n",
        "  # Iterate over the pairs of characters\n",
        "  for a, b in pairs:\n",
        "    # If the characters are the same, increment the overlap counter\n",
        "    if a == b:\n",
        "      overlap += 1\n",
        "\n",
        "  # Return the overlap distance\n",
        "  return overlap\n",
        "\n",
        "# ------------------------------ #\n",
        "#    Overlap Similarity list\n",
        "# ------------------------------ # \n",
        "\n",
        "def overlap_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n",
        "    sims = []\n",
        "    for l1, l2 in zip(s1, s2):\n",
        "        sim = overlap_distance(l1, l2)\n",
        "        sims.append(sim)\n",
        "    return np.array(sims)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_VF58N6nQre"
      },
      "outputs": [],
      "source": [
        "tag_dict = {\n",
        "        \"NN\": \"n\",\n",
        "        \"NNS\": \"n\",\n",
        "        \"NNP\": \"n\",\n",
        "        \"NNPS\": \"n\",\n",
        "        \"VB\": \"v\",\n",
        "        \"VBD\": \"v\",\n",
        "        \"VBG\": \"v\",\n",
        "        \"VBN\": \"v\",\n",
        "        \"VBP\": \"v\",\n",
        "        \"VBZ\": \"v\",\n",
        "        \"RB\": \"r\",\n",
        "        \"RBR\": \"r\",\n",
        "        \"RBS\": \"r\",\n",
        "        \"JJ\": \"a\",\n",
        "        \"JJR\": \"a\",\n",
        "        \"JJS\": \"a\",\n",
        "  }\n",
        "\n",
        "# ------------------------------ #\n",
        "#         Get Wordnet POS\n",
        "# ------------------------------ #\n",
        "def get_wordnet_pos(word):\n",
        "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  \n",
        "        \n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "#Auxiliar spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "special_pattern = re.compile(r\"[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+\")\n",
        "\n",
        "# ------------------------------ #\n",
        "#   Function to tokenize spacy\n",
        "# ------------------------------ #\n",
        "def spacy_tokenize(sentence):\n",
        "  return [ word.text.lower() for word in nlp.tokenizer(sentence) ]\n",
        "\n",
        "def tokenize_column_spacy(column):\n",
        "  tokenize = [spacy_tokenize(sentence) for sentence in column]\n",
        "  \n",
        "  return tokenize\n",
        "  \n",
        "# ------------------------------ #\n",
        "#   Function to lemmatize spacy\n",
        "# ------------------------------ #\n",
        "def spacy_lemmatize(sentence: str):\n",
        "  return [ word.lemma_.lower() for word in nlp.tokenizer(sentence) ]\n",
        "  \n",
        "# ------------------------------ #\n",
        "#   Function to tokenize\n",
        "# ------------------------------ #\n",
        "def tokenize_column(column):\n",
        "    #put in lowercase\n",
        "    tokenizator = [nltk.word_tokenize(sentence) for sentence in column]\n",
        "    #Lowercase the tokens\n",
        "    return [ [ word.lower() for word in sentence ] for sentence in tokenizator ]\n",
        "\n",
        "\n",
        "#--------------------------------------------#\n",
        "#  Function to NES\n",
        "#--------------------------------------------#\n",
        "def NES(sentence: str, binary: bool):\n",
        "    x = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "    res = nltk.ne_chunk(x, binary=binary)\n",
        "    necs_and_words = set()\n",
        "    for chunk in res:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            # Add NE\n",
        "            token = ' '.join(term[0] for term in chunk)\n",
        "            necs_and_words.add(token)\n",
        "        else:\n",
        "            token = chunk[0]\n",
        "            if token.isalnum():\n",
        "                necs_and_words.add(token.lower())\n",
        "    return necs_and_words\n",
        "\n",
        " #--------------------------------------------#\n",
        " # Function to get entities from a column\n",
        " # -------------------------------------------# \n",
        "def get_entities_new(column):\n",
        "    entities = []\n",
        "    for sentence in column:\n",
        "        entities.append(NES(sentence, False))\n",
        "    return entities\n",
        "\n",
        "# ------------------------------ #\n",
        "# Lemmatization text process\n",
        "# ------------------------------ #\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# ------------------------------ #\n",
        "#   Function to lemmatize\n",
        "# ------------------------------ #\n",
        "def lemmatize(tokenized_text: List[List[str]]):\n",
        "  \n",
        "  lemmas = []\n",
        "\n",
        "  for sentence in tqdm(tokenized_text):\n",
        "    sentence_lemmas = []\n",
        "    for word in sentence:\n",
        "      sentence_lemmas.append(lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())))\n",
        "    lemmas.append(sentence_lemmas)\n",
        "\n",
        "  return lemmas\n",
        "\n",
        "# ------------------------------ #\n",
        "#   Stopwords initialization\n",
        "# ------------------------------ #\n",
        "stopwords_list = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "stopwords_list = stopwords_list.union(set(string.punctuation))\n",
        "stopwords_list = stopwords_list.union(set(['.', ',', ';', '.\"']))\n",
        "\n",
        "# ------------------------------ #\n",
        "#   Function to remove stopwords\n",
        "# ------------------------------ #\n",
        "def remove_stopwords(column: List[List[str]]):\n",
        "  #Lowercase the tokens\n",
        "  return [ [ word.lower() for word in sentence if word not in stopwords_list ]  for sentence in column ]\n",
        "\n",
        "\n",
        "# ------------------------------ #\n",
        "#   Function to synonimize\n",
        "# ------------------------------ #\n",
        "def synonimize_column(column):\n",
        "  #put in lowercase\n",
        "  tokenized = [nltk.word_tokenize(sentence) for sentence in column]\n",
        "  #Lowercase the tokens\n",
        "  tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n",
        "  #Synonimize\n",
        "  synonimized = [ [ word for word in sentence if word not in stopwords_list ] for sentence in tokenized ]\n",
        "\n",
        "  return synonimized\n",
        "\n",
        "\n",
        "# ------------------------------ #\n",
        "#   Function to synset\n",
        "# ------------------------------ #\n",
        "def get_synset_column(tokenized_text: List[List[str]]):\n",
        "  synset = []\n",
        "  for sentence in tokenized_text:\n",
        "    pos = nltk.pos_tag(sentence)\n",
        "    lemmas = []\n",
        "    for pair in pos:\n",
        "      if pair[1][0] in tag_dict.keys():\n",
        "        lemma = wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n",
        "        lemmas.append(lemma)\n",
        "      else:\n",
        "        lemma = pair[0]\n",
        "        lemmas.append(lemma)\n",
        "    synset.append(lemmas)\n",
        "  \n",
        "  return synset\n",
        "\n",
        "\n",
        "# ------------------------------ #\n",
        "#  Function to NE(Name entities)\n",
        "# ------------------------------ #\n",
        "def apply_ne(tokenized_text: List[str]):\n",
        "    # tokenize the sentence and find the POS tag for each token\n",
        "    sentences_ne = list(ne_chunk(pos_tag(tokenized_text), binary=True))\n",
        "    result = []\n",
        "    for el in sentences_ne:\n",
        "        if isinstance(el, Tree):\n",
        "            leaves = el.leaves()\n",
        "            result.append(\" \".join(word[0] for word in leaves))\n",
        "        else:\n",
        "            result.append(el[0])\n",
        "    return result\n",
        "\n",
        "# used apply_ne function to get NE from a column\n",
        "def get_name_entities(column: List[List[str]]):\n",
        "  ne = []\n",
        "  for sentence in column:\n",
        "    ne.append(apply_ne(sentence))\n",
        "  return ne\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------ #\n",
        "#  Function to get ngrams\n",
        "# ------------------------------ #the\n",
        "def get_ngrams_column(column: List[List[str]], n: int):\n",
        "  ngrams = []\n",
        "  for sentence in column:\n",
        "    ngrams.append(apply_ngram(sentence, n))\n",
        "  return ngrams\n",
        "\n",
        "\n",
        "def apply_ngram(sentence: List[str], n: int):\n",
        "    if len(sentence) < n:\n",
        "        return [tuple(sentence)]\n",
        "    return list(nltk.ngrams(sentence, n))\n",
        "\n",
        "\n",
        "# ------------------------------ #\n",
        "#     Function to get lesk \n",
        "# ------------------------------ #\n",
        "def get_lesk_column(column):\n",
        "  lesk_text = []\n",
        "\n",
        "  for sentence in column:\n",
        "    synset = [lesk(sentence, word) for word in sentence]\n",
        "    synset = {word for word in synset if word is not None}\n",
        "    lesk_text.append(synset)\n",
        "\n",
        "  return lesk_text\n",
        "\n",
        "\n",
        "# -------------------------------------- #\n",
        "#     Different similarities for synsets\n",
        "# -------------------------------------- #\n",
        "def get_synset(tokenized_text: str, synsets):\n",
        "  key_list = []\n",
        "  sentence_tagged = nltk.pos_tag(tokenized_text)\n",
        "  for pair in sentence_tagged:\n",
        "    wordnet_tag = get_wordnet_pos(pair[1])\n",
        "    if wordnet_tag is not None:\n",
        "      pair = (pair[0], wordnet_tag)\n",
        "      synset = wordnet.synsets(pair[0], pair[1])\n",
        "      if synset:\n",
        "          synsets[pair[0]] = (synset[0], synset[0].pos())\n",
        "          key_list.append(pair[0])\n",
        "  return synsets, key_list\n",
        "\n",
        "\n",
        "def get_synset_similarity(column1, column2, distance: str):\n",
        "  \n",
        "  all_similarities = []\n",
        "  brown_ic = nltk.corpus.wordnet_ic.ic('ic-brown.dat')\n",
        "\n",
        "  for sentence1, sentence2 in tqdm(zip(column1, column2), total=max(len(column1), len(column2))):\n",
        "    synsets, keys1 = get_synset(sentence1, {})\n",
        "    synsets, keys2 = get_synset(sentence2, synsets)\n",
        "    \n",
        "    similarities = []\n",
        "    for word1 in keys1:\n",
        "      for word2 in keys2:\n",
        "        if synsets[word1][1] != synsets[word2][1]:\n",
        "          continue\n",
        "        similarity = None\n",
        "        if distance == 'path':\n",
        "          similarity = synsets[word1][0].path_similarity(synsets[word2][0])\n",
        "        elif distance == 'lch':\n",
        "          similarity = synsets[word1][0].lch_similarity(synsets[word2][0])\n",
        "        elif distance == 'wup':\n",
        "          similarity = synsets[word1][0].wup_similarity(synsets[word2][0])\n",
        "        elif distance == 'lin':\n",
        "          try:\n",
        "            similarity = synsets[word1][0].lin_similarity(synsets[word2][0], brown_ic)\n",
        "          except:\n",
        "            similarity = 0\n",
        "        similarities.append(similarity)\n",
        "    if len(similarities) > 0:\n",
        "      all_similarities.append(np.mean(similarities))\n",
        "    else:\n",
        "      all_similarities.append(0)\n",
        "  return all_similarities\n",
        "  \n",
        "def apply_jaccard_lesk(sentence1: str, sentence2: str):\n",
        "\n",
        "  # Apply lesk to sentence 1\n",
        "  synset1 = [ lesk(sentence1, word) for word in sentence1 ]\n",
        "  synset1 = { word for word in synset1 if word is not None }\n",
        "\n",
        "  # Apply lesk to sentence 1\n",
        "  synset2 = [ lesk(sentence2, word) for word in sentence2 ]\n",
        "  synset2 = { word for word in synset2 if word is not None }\n",
        "\n",
        "  # Calculate distance\n",
        "  distance = jaccard_distance(synset1, synset2)\n",
        "\n",
        "  return distance\n",
        "\n",
        "\n",
        "def lemma_spacy(sentences):\n",
        "  sentences = [special_chars_out(s) for s in sentences]\n",
        "  token_lemmatize = [spacy_lemmatize(phrase) for phrase in sentences]\n",
        "  return token_lemmatize\n",
        "\n",
        "\n",
        "def special_chars_out(sentence: str):\n",
        "  \n",
        "  sentence = sentence.replace(\"'ve\", \" have\")\n",
        "  sentence = sentence.replace(\"n't\", \" not\")\n",
        "  sentence = sentence.replace(\"'ll\", \" will\")  \n",
        "  sentence = sentence.replace(\"'m\", \" am\")  \n",
        "  sentence = sentence.replace(\"'re\", \" are\")\n",
        "  \n",
        "  sentence = re.sub(special_pattern, \" \", sentence)  \n",
        "\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwgHesgMrUdt"
      },
      "outputs": [],
      "source": [
        "# Functions of preprocessing\n",
        "def read_data(text_datas: List[str], gs_datas: List[str]):\n",
        "  all_df_text = []\n",
        "  for text_data, gs_data in zip(text_datas, gs_datas):\n",
        "    df_text = pd.read_csv(text_data, sep=r'\\t', engine='python', header=None)\n",
        "    df_text.columns = [\"text1\", \"text2\"]\n",
        "    df_text['gs'] = pd.read_csv(gs_data, sep='\\t', header=None)\n",
        "    all_df_text.append(df_text.dropna())\n",
        "  return pd.concat(all_df_text)\n",
        "\n",
        "def get_dataset(path: str) -> pd.DataFrame:\n",
        "  files = sorted(os.listdir(path))\n",
        "  input_files = [ os.path.join(path, file) for file in files if 'input' in file ]\n",
        "  gs_files = [ os.path.join(path, file) for file in files if 'gs' in file ]\n",
        "  df = read_data(input_files, gs_files)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxqJDlCQsVtY",
        "outputId": "b14b2ff1-b541-4090-fdd0-58d9a6b00b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm2VxdulsYgE"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfqKCsKhN3fR"
      },
      "source": [
        "### Data information\n",
        "- trial : includes the definition of the scores, a sample of 5 sentence pairs and the input and output formats. It is not needed, but it is useful for prototyping.\n",
        "\n",
        "- train : training data from paraphrasing data sets, input and output formats.\n",
        "\n",
        "- test : test data from paraphrasing data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdEvE55Xsb0L"
      },
      "outputs": [],
      "source": [
        "#train_path = '../final_project/train'\n",
        "#trial_path = '../final_project/trial'\n",
        "#test_path  = '../final_project/test-gold'\n",
        "\n",
        "train_path = '/content/train'\n",
        "trial_path = '/content/trial'\n",
        "test_path  = '/content/test-gold'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge1UzZJlrUdu"
      },
      "source": [
        "# **Similarities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tx_t_KsL_Fl"
      },
      "outputs": [],
      "source": [
        "def get_features(df: pd.DataFrame):\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 0. NLTK Words features\n",
        "    #--------------------------------------------#\n",
        "    #print(\"NLTK Words features\")\n",
        "    \n",
        "    #nltk_words_text1 = []\n",
        "    #nltk_words_text2 = []\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 1. Tokenize features\n",
        "    #--------------------------------------------#    \n",
        "    tokenized_text1 = tokenize_column(df['text1'])\n",
        "    tokenized_text2 = tokenize_column(df['text2'])\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 2. Lemmatize features\n",
        "    #--------------------------------------------#\n",
        "    lemmatize_text1 = lemmatize(tokenized_text1)\n",
        "    lemmatize_text2 = lemmatize(tokenized_text2)\n",
        "\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 3. Stopwords features\n",
        "    #--------------------------------------------#   \n",
        "    stopwords_text1 = remove_stopwords(lemmatize_text1)\n",
        "    stopwords_text2 = remove_stopwords(lemmatize_text2)\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 4. Synonims features\n",
        "    #--------------------------------------------#\n",
        "    synonyms_text1 = []\n",
        "    synonyms_text2 = []\n",
        "    # Use sysnstesizer to get synonyms\n",
        "    for i in tqdm(range(len(tokenized_text1))):\n",
        "        synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n",
        "        synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n",
        "\n",
        "    \n",
        "    #--------------------------------------------#\n",
        "    # 5. NES features\n",
        "    #--------------------------------------------#\n",
        "    NES_column_text1 = get_entities_new(df['text1'])\n",
        "    NES_column_text2 = get_entities_new(df['text2'])\n",
        "\n",
        "    \n",
        "    #--------------------------------------------#\n",
        "    # 6. Name entities features\n",
        "    #--------------------------------------------#\n",
        "    name_entities_text1 = get_name_entities(lemmatize_text1)\n",
        "    name_entities_text2 = get_name_entities(lemmatize_text2)\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 7. Ngrams features\n",
        "    #--------------------------------------------#\n",
        "\n",
        "    ngrams_column_2_text1 = get_ngrams_column(lemmatize_text1, 2)\n",
        "    ngrams_column_2_text2 = get_ngrams_column(lemmatize_text2, 2)\n",
        "\n",
        "    ngrams_column_3_text1 = get_ngrams_column(lemmatize_text1, 3)\n",
        "    ngrams_column_3_text2 = get_ngrams_column(lemmatize_text2, 3)\n",
        "\n",
        "    ngrams_column_4_text1 = get_ngrams_column(lemmatize_text1, 4)\n",
        "    ngrams_column_4_text2 = get_ngrams_column(lemmatize_text2, 4)\n",
        "\n",
        "    ngrams_column_5_text1 = get_ngrams_column(lemmatize_text1, 5)\n",
        "    ngrams_column_5_text2 = get_ngrams_column(lemmatize_text2, 5)\n",
        "\n",
        "    ngrams_column_6_text1 = get_ngrams_column(lemmatize_text1, 6)\n",
        "    ngrams_column_6_text2 = get_ngrams_column(lemmatize_text2, 6)\n",
        "\n",
        "    ngrams_column_7_text1 = get_ngrams_column(lemmatize_text1, 7)\n",
        "    ngrams_column_7_text2 = get_ngrams_column(lemmatize_text2, 7)\n",
        "\n",
        "    ngrams_column_8_text1 = get_ngrams_column(lemmatize_text1, 8)\n",
        "    ngrams_column_8_text2 = get_ngrams_column(lemmatize_text2, 8)\n",
        "\n",
        "    ngrams_column_9_text1 = get_ngrams_column(lemmatize_text1, 9)\n",
        "    ngrams_column_9_text2 = get_ngrams_column(lemmatize_text2, 9)\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 8. Lesk features\n",
        "    #--------------------------------------------#\n",
        "    # Lesk features\n",
        "    lesk_text1 = get_lesk_column(tokenized_text1)\n",
        "    lesk_text2 = get_lesk_column(tokenized_text2)\n",
        "\n",
        "    # --------------------------------------------#\n",
        "    # 9. Spacy words features\n",
        "    # --------------------------------------------#\n",
        "    print(\"Spacy words features\")\n",
        "    spacy_words_text1 = tokenize_column_spacy(df['text1'])\n",
        "    spacy_words_text2 = tokenize_column_spacy(df['text2'])\n",
        "\n",
        "    # --------------------------------------------#\n",
        "    # 10. Spacy lemmatize features\n",
        "    # --------------------------------------------#\n",
        "    print(\"Spacy lemmatize features\")\n",
        "    spacy_lemmatize_text1 = lemma_spacy(df['text1'])\n",
        "    spacy_lemmatize_text2 = lemma_spacy(df['text2'])\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 11.Lemma synonyms features\n",
        "    #--------------------------------------------#\n",
        "    lemma_synonyms_text1 = []\n",
        "    lemma_synonyms_text2 = []\n",
        "    # Use sysnstesizer to get synonyms\n",
        "    for i in tqdm(range(len(tokenized_text1))):\n",
        "        lemma_synonyms_text1.append([syn for w in lemmatize_text1[i] for syn in wordnet.synsets(w)])\n",
        "        lemma_synonyms_text2.append([syn for w in lemmatize_text2[i] for syn in wordnet.synsets(w)])\n",
        "\n",
        "    #print(\"Word synonyms features\"\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 12. Synset features\n",
        "    #--------------------------------------------#\n",
        "    print(\"Synset features\")\n",
        "    synset_text1 = get_synset_column(tokenized_text1)\n",
        "    synset_text2 = get_synset_column(tokenized_text2)\n",
        "\n",
        "    #--------------------------------------------#\n",
        "    # 13. Synset similarities\n",
        "    #--------------------------------------------#\n",
        "    print(\"Synset similarities\")\n",
        "    average_path = get_synset_similarity(tokenized_text1, tokenized_text2, \"path\")\n",
        "    average_lch = get_synset_similarity(tokenized_text1, tokenized_text2, \"lch\")\n",
        "    average_wup = get_synset_similarity(tokenized_text1, tokenized_text2, \"wup\")\n",
        "    average_lin = get_synset_similarity(tokenized_text1, tokenized_text2, \"lin\")\n",
        "\n",
        "\n",
        "    features = [\n",
        "        # Jaccard similarity\n",
        "        jaccard_similarity_list(tokenized_text1, tokenized_text2),\n",
        "        jaccard_similarity_list(lemmatize_text1, lemmatize_text2),\n",
        "        jaccard_similarity_list(stopwords_text1, stopwords_text2),\n",
        "        jaccard_similarity_list(synonyms_text1, synonyms_text2),\n",
        "        jaccard_similarity_list(NES_column_text1, NES_column_text2),\n",
        "        jaccard_similarity_list(name_entities_text1, name_entities_text2),\n",
        "        jaccard_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n",
        "        jaccard_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n",
        "        jaccard_similarity_list(ngrams_column_4_text1, ngrams_column_4_text2),\n",
        "        jaccard_similarity_list(ngrams_column_5_text1, ngrams_column_5_text2),\n",
        "        jaccard_similarity_list(ngrams_column_6_text1, ngrams_column_6_text2),\n",
        "        jaccard_similarity_list(ngrams_column_7_text1, ngrams_column_7_text2),\n",
        "        jaccard_similarity_list(ngrams_column_8_text1, ngrams_column_8_text2),\n",
        "        jaccard_similarity_list(ngrams_column_9_text1, ngrams_column_9_text2),\n",
        "        jaccard_similarity_list(lesk_text1, lesk_text2),\n",
        "        jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n",
        "        jaccard_similarity_list(spacy_lemmatize_text1, spacy_lemmatize_text2),\n",
        "\n",
        "\n",
        "        # jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n",
        "        # jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n",
        "        # jaccard_similar\n",
        "        jaccard_similarity_list(lemma_synonyms_text1,lemma_synonyms_text2),\n",
        "        jaccard_similarity_list(synset_text1, synset_text2),\n",
        "        #jaccard_similarity_list(synset_text1, synset_text2),\n",
        "        \n",
        "        \n",
        "        # Dice similarity\n",
        "        dice_similarity_list(tokenized_text1, tokenized_text2),\n",
        "        dice_similarity_list(lemmatize_text1, lemmatize_text2),\n",
        "        dice_similarity_list(stopwords_text1, stopwords_text2),\n",
        "        dice_similarity_list(synonyms_text1, synonyms_text2),\n",
        "        dice_similarity_list(NES_column_text1, NES_column_text2),\n",
        "        dice_similarity_list(name_entities_text1, name_entities_text2),\n",
        "        dice_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n",
        "        dice_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n",
        "        dice_similarity_list(ngrams_column_4_text1, ngrams_column_4_text2),\n",
        "        dice_similarity_list(ngrams_column_5_text1, ngrams_column_5_text2),\n",
        "        dice_similarity_list(ngrams_column_6_text1, ngrams_column_6_text2),\n",
        "        dice_similarity_list(ngrams_column_7_text1, ngrams_column_7_text2),\n",
        "        dice_similarity_list(ngrams_column_8_text1, ngrams_column_8_text2),\n",
        "        dice_similarity_list(ngrams_column_9_text1, ngrams_column_9_text2),\n",
        "        dice_similarity_list(lesk_text1, lesk_text2),\n",
        "        dice_similarity_list(spacy_words_text1, spacy_words_text2),\n",
        "        dice_similarity_list(spacy_lemmatize_text1, spacy_lemmatize_text2),\n",
        "\n",
        "        #jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n",
        "        #jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n",
        "        #jaccard_similarity_\n",
        "        dice_similarity_list(lemma_synonyms_text1,lemma_synonyms_text2),\n",
        "        dice_similarity_list(synset_text1, synset_text2),\n",
        "        #jaccard_similarity_list(synset_text1, synset_text2),\n",
        "\n",
        "        # Synset similarities\n",
        "        average_path,\n",
        "        average_lch,\n",
        "        average_wup,\n",
        "        average_lin,\n",
        "    ]\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptpHWemZrUdw"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwbLuSSHrUdw"
      },
      "source": [
        "## Get training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fifhUTGhRKBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "1bfe0755-3baf-4d95-bd65-1f3f282f47d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2234, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               text1  \\\n",
              "0  But other sources close to the sale said Viven...   \n",
              "1  Micron has declared its first quarterly profit...   \n",
              "2  The fines are part of failed Republican effort...   \n",
              "3  The American Anglican Council, which represent...   \n",
              "4  The tech-loaded Nasdaq composite rose 20.96 po...   \n",
              "\n",
              "                                               text2   gs  \n",
              "0  But other sources close to the sale said Viven... 4.00  \n",
              "1  Micron's numbers also marked the first quarter... 3.75  \n",
              "2  Perry said he backs the Senate's efforts, incl... 2.80  \n",
              "3  The American Anglican Council, which represent... 3.40  \n",
              "4  The technology-laced Nasdaq Composite Index <.... 2.40  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-93ec36dc-2d84-4389-ac30-c82a117aceae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>But other sources close to the sale said Viven...</td>\n",
              "      <td>But other sources close to the sale said Viven...</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Micron has declared its first quarterly profit...</td>\n",
              "      <td>Micron's numbers also marked the first quarter...</td>\n",
              "      <td>3.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The fines are part of failed Republican effort...</td>\n",
              "      <td>Perry said he backs the Senate's efforts, incl...</td>\n",
              "      <td>2.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The American Anglican Council, which represent...</td>\n",
              "      <td>The American Anglican Council, which represent...</td>\n",
              "      <td>3.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n",
              "      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n",
              "      <td>2.40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-93ec36dc-2d84-4389-ac30-c82a117aceae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-93ec36dc-2d84-4389-ac30-c82a117aceae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-93ec36dc-2d84-4389-ac30-c82a117aceae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_dataset = get_dataset(train_path)\n",
        "print(train_dataset.shape)\n",
        "train_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNVz2MTPrUdx",
        "outputId": "5e4af404-8b8c-41a7-c342-1aecab9ea8a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2234,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y_train = train_dataset['gs'].values\n",
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kyji124ZrUdx"
      },
      "source": [
        "## Get features of the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxW0BdIYrUdx",
        "outputId": "97c86d49-02fd-4d90-a8d4-a4e1ff12c0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2234/2234 [00:05<00:00, 445.46it/s]\n",
            "100%|██████████| 2234/2234 [00:04<00:00, 454.15it/s]\n",
            "100%|██████████| 2234/2234 [00:01<00:00, 1584.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy words features\n",
            "Spacy lemmatize features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2234/2234 [00:01<00:00, 1543.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset features\n",
            "Synset similarities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2234/2234 [01:14<00:00, 29.97it/s]\n",
            "100%|██████████| 2234/2234 [01:05<00:00, 33.88it/s]\n",
            "100%|██████████| 2234/2234 [01:26<00:00, 25.83it/s]\n",
            "100%|██████████| 2234/2234 [00:21<00:00, 104.90it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 2234)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "X_train_features: np.ndarray = get_features(train_dataset)\n",
        "X_train_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-H1etqrUdy",
        "outputId": "d7c8dd14-0a65-42f0-e333-5fe776062a48"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 2234)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "X_train_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yumDx0jnrUdy"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpB7_tLdrUdy"
      },
      "source": [
        "## Get the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "OQj8-VUlrUdy",
        "outputId": "19222eab-29b5-40b6-8113-949713c4022b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2817, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               text1  \\\n",
              "0  The problem likely will mean corrective change...   \n",
              "1  The technology-laced Nasdaq Composite Index .I...   \n",
              "2  \"It's a huge black eye,\" said publisher Arthur...   \n",
              "3  SEC Chairman William Donaldson said there is a...   \n",
              "4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n",
              "\n",
              "                                               text2   gs  \n",
              "0  He said the problem needs to be corrected befo... 4.40  \n",
              "1  The broad Standard & Poor's 500 Index .SPX inc... 0.80  \n",
              "2  \"It's a huge black eye,\" Arthur Sulzberger, th... 3.60  \n",
              "3  \"I think there's a building confidence that th... 3.40  \n",
              "4  In New York, Vivendi shares were 1.4 percent d... 1.40  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f4c0a7f5-08e0-448a-b3cd-2cf88dfde709\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>gs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The problem likely will mean corrective change...</td>\n",
              "      <td>He said the problem needs to be corrected befo...</td>\n",
              "      <td>4.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The technology-laced Nasdaq Composite Index .I...</td>\n",
              "      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n",
              "      <td>0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n",
              "      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n",
              "      <td>3.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SEC Chairman William Donaldson said there is a...</td>\n",
              "      <td>\"I think there's a building confidence that th...</td>\n",
              "      <td>3.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n",
              "      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n",
              "      <td>1.40</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4c0a7f5-08e0-448a-b3cd-2cf88dfde709')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f4c0a7f5-08e0-448a-b3cd-2cf88dfde709 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f4c0a7f5-08e0-448a-b3cd-2cf88dfde709');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "test_dataset = get_dataset(test_path)\n",
        "print(test_dataset.shape)\n",
        "test_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkej3HWurUdy"
      },
      "source": [
        "## Get features of the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QwMa6W7rUdy",
        "outputId": "1591dbaa-abf5-4d39-f313-90f2140e4b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2817/2817 [00:04<00:00, 672.11it/s]\n",
            "100%|██████████| 2817/2817 [00:04<00:00, 664.35it/s]\n",
            "100%|██████████| 2817/2817 [00:01<00:00, 2099.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy words features\n",
            "Spacy lemmatize features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2817/2817 [00:01<00:00, 2389.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset features\n",
            "Synset similarities\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2817/2817 [00:44<00:00, 63.59it/s]\n",
            "100%|██████████| 2817/2817 [00:36<00:00, 77.03it/s]\n",
            "100%|██████████| 2817/2817 [00:53<00:00, 53.09it/s]\n",
            "100%|██████████| 2817/2817 [00:17<00:00, 159.79it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42, 2817)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_test_features: np.ndarray = get_features(test_dataset)\n",
        "X_test_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijQCT6b6rUdz",
        "outputId": "d20475c1-910a-4ff8-c108-98975f2d0ec6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2817,)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "y_test = test_dataset['gs'].values\n",
        "y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRirg1pCrUdz"
      },
      "source": [
        "## Normalize all features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhihvAArUdz"
      },
      "outputs": [],
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_features.T)\n",
        "X_train_features_norm = scaler.transform(X_train_features.T)\n",
        "X_test_features_norm = scaler.transform(X_test_features.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADs3xr7prUd0"
      },
      "source": [
        "## Select the best features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sny1wqTKrUd0"
      },
      "outputs": [],
      "source": [
        "best_features = [0, 1, 2, 3, 4, 38, 39, 40, 41]\n",
        "X_train_features_norm = X_train_features_norm[:, best_features]\n",
        "X_test_features_norm = X_test_features_norm[:, best_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lIc3Rd9rUd1"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxJebVsmrUd2",
        "outputId": "2416a4b1-4969-4ad5-e0a8-4bfd708ddf5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_features shape:  (2234, 9)\n",
            "y_train shape:  (2234,)\n",
            "X_test_features shape:  (2817, 9)\n",
            "y_test shape:  (2817,)\n"
          ]
        }
      ],
      "source": [
        "# Print all shapes\n",
        "print(\"X_train_features shape: \", X_train_features_norm.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"X_test_features shape: \", X_test_features_norm.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rv03kqzrUd3"
      },
      "source": [
        "### Train a simple regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_TLMjz0rUd3",
        "outputId": "3f77c3bf-aee9-4229-adfa-7b055853d377"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Train\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train_features_norm, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLQSKwAKrUd4",
        "outputId": "c24e2eea-ead9-4c65-e18a-f342d79f2fe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pearson:  0.6893760521234433\n",
            "Test pearson:  -0.021679359805346278\n"
          ]
        }
      ],
      "source": [
        "# Evaluate\n",
        "y_pred_train = reg.predict(X_train_features_norm)\n",
        "y_pred_test = reg.predict(X_test_features_norm)\n",
        "\n",
        "print(\"Train pearson: \", pearsonr(y_train, y_pred_train)[0])\n",
        "print(\"Test pearson: \", pearsonr(y_test, y_pred_test)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA0tOmNcrUd4"
      },
      "source": [
        "### Train multiple regression models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Is1Cj0rUd4",
        "outputId": "5897dd49-97ca-4e19-afca-9f31237bb124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of regressors: 41\n"
          ]
        }
      ],
      "source": [
        "# Select all of the models that we are going to use\n",
        "REGRESSORS = [ c for c in REGRESSORS if c[0] != 'QuantileRegressor' ]\n",
        "print(\"Number of regressors:\", len(REGRESSORS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88HVuzY-rUd4"
      },
      "outputs": [],
      "source": [
        "# Build pearson score function\n",
        "def pearsonr_scorer(y_true, y_pred):\n",
        "    assert len(y_true) == len(y_pred)\n",
        "    score = pearsonr(y_true, y_pred)[0]\n",
        "    return score\n",
        "\n",
        "pearson_scorer = make_scorer(pearsonr_scorer)\n",
        "pearson_scorer.__name__ = 'pearson_scorer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdFfrk7wrUd5",
        "outputId": "1df0e9ae-2da7-46ed-a53f-b1ccaedc6af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'tuple' object has no attribute '__name__'\n",
            "Invalid Regressor(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 37/41 [00:16<00:01,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12:31:49] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41/41 [00:17<00:00,  2.40it/s]\n"
          ]
        }
      ],
      "source": [
        "# Fit all models\n",
        "reg = LazyRegressor(predictions=True, regressors=REGRESSORS, custom_metric=pearsonr_scorer)\n",
        "regresion_models, regresion_predictions = reg.fit(X_train_features_norm, X_test_features_norm, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0fvyd_k5rUd5",
        "outputId": "eb9b5325-4558-4e9e-d1ce-5938d47efbe7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Adjusted R-Squared  R-Squared  RMSE  \\\n",
              "Model                                                                \n",
              "PassiveAggressiveRegressor                  -5.12      -5.10  2.89   \n",
              "HuberRegressor                              -0.95      -0.94  1.63   \n",
              "LinearRegression                            -0.87      -0.86  1.60   \n",
              "TransformedTargetRegressor                  -0.87      -0.86  1.60   \n",
              "Lars                                        -0.87      -0.86  1.60   \n",
              "LarsCV                                      -0.87      -0.86  1.60   \n",
              "LassoLarsCV                                 -0.87      -0.86  1.60   \n",
              "Ridge                                       -0.87      -0.86  1.60   \n",
              "KernelRidge                                 -9.82      -9.79  3.85   \n",
              "LassoLarsIC                                 -0.86      -0.86  1.60   \n",
              "LassoCV                                     -0.86      -0.86  1.60   \n",
              "BayesianRidge                               -0.86      -0.86  1.60   \n",
              "ElasticNetCV                                -0.86      -0.86  1.60   \n",
              "LinearSVR                                   -0.97      -0.96  1.64   \n",
              "RidgeCV                                     -0.86      -0.85  1.60   \n",
              "SGDRegressor                                -0.88      -0.87  1.60   \n",
              "ElasticNet                                  -0.10      -0.10  1.23   \n",
              "OrthogonalMatchingPursuit                   -0.82      -0.82  1.58   \n",
              "GaussianProcessRegressor                    -8.68      -8.65  3.64   \n",
              "NuSVR                                       -1.00      -1.00  1.66   \n",
              "OrthogonalMatchingPursuitCV                 -0.93      -0.93  1.63   \n",
              "PoissonRegressor                            -0.56      -0.56  1.46   \n",
              "RANSACRegressor                             -1.27      -1.27  1.76   \n",
              "ExtraTreeRegressor                          -1.91      -1.90  1.99   \n",
              "HistGradientBoostingRegressor               -1.28      -1.27  1.77   \n",
              "SVR                                         -1.21      -1.21  1.74   \n",
              "LGBMRegressor                               -1.31      -1.30  1.78   \n",
              "MLPRegressor                                -1.23      -1.22  1.75   \n",
              "TweedieRegressor                            -0.45      -0.45  1.41   \n",
              "ExtraTreesRegressor                         -1.21      -1.20  1.74   \n",
              "AdaBoostRegressor                           -0.77      -0.76  1.56   \n",
              "DecisionTreeRegressor                       -1.90      -1.89  1.99   \n",
              "RandomForestRegressor                       -1.25      -1.25  1.76   \n",
              "XGBRegressor                                -1.22      -1.21  1.74   \n",
              "GradientBoostingRegressor                   -1.23      -1.22  1.75   \n",
              "BaggingRegressor                            -1.35      -1.35  1.80   \n",
              "KNeighborsRegressor                         -1.31      -1.30  1.78   \n",
              "Lasso                                       -0.02      -0.02  1.18   \n",
              "DummyRegressor                              -0.02      -0.02  1.18   \n",
              "LassoLars                                   -0.02      -0.02  1.18   \n",
              "\n",
              "                               Time Taken  pearsonr_scorer  \n",
              "Model                                                       \n",
              "PassiveAggressiveRegressor           0.08             0.02  \n",
              "HuberRegressor                       0.05            -0.02  \n",
              "LinearRegression                     0.03            -0.02  \n",
              "TransformedTargetRegressor           0.02            -0.02  \n",
              "Lars                                 0.03            -0.02  \n",
              "LarsCV                               0.04            -0.02  \n",
              "LassoLarsCV                          0.07            -0.02  \n",
              "Ridge                                0.02            -0.02  \n",
              "KernelRidge                          0.35            -0.02  \n",
              "LassoLarsIC                          0.03            -0.02  \n",
              "LassoCV                              0.16            -0.02  \n",
              "BayesianRidge                        0.02            -0.02  \n",
              "ElasticNetCV                         0.18            -0.02  \n",
              "LinearSVR                            0.11            -0.02  \n",
              "RidgeCV                              0.03            -0.03  \n",
              "SGDRegressor                         0.05            -0.03  \n",
              "ElasticNet                           0.02            -0.05  \n",
              "OrthogonalMatchingPursuit            0.10            -0.05  \n",
              "GaussianProcessRegressor             1.67            -0.05  \n",
              "NuSVR                                3.09            -0.06  \n",
              "OrthogonalMatchingPursuitCV          0.16            -0.06  \n",
              "PoissonRegressor                     0.13            -0.06  \n",
              "RANSACRegressor                      0.41            -0.06  \n",
              "ExtraTreeRegressor                   0.06            -0.07  \n",
              "HistGradientBoostingRegressor        0.44            -0.07  \n",
              "SVR                                  0.74            -0.07  \n",
              "LGBMRegressor                        0.23            -0.07  \n",
              "MLPRegressor                         4.89            -0.07  \n",
              "TweedieRegressor                     0.03            -0.08  \n",
              "ExtraTreesRegressor                  0.69            -0.08  \n",
              "AdaBoostRegressor                    0.14            -0.08  \n",
              "DecisionTreeRegressor                0.09            -0.08  \n",
              "RandomForestRegressor                1.55            -0.08  \n",
              "XGBRegressor                         0.27            -0.09  \n",
              "GradientBoostingRegressor            0.59            -0.09  \n",
              "BaggingRegressor                     0.16            -0.09  \n",
              "KNeighborsRegressor                  0.08            -0.09  \n",
              "Lasso                                0.03              NaN  \n",
              "DummyRegressor                       0.02              NaN  \n",
              "LassoLars                            0.02              NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-caf25c79-d938-40b3-bf83-07e9a99dbc47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Adjusted R-Squared</th>\n",
              "      <th>R-Squared</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>Time Taken</th>\n",
              "      <th>pearsonr_scorer</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PassiveAggressiveRegressor</th>\n",
              "      <td>-5.12</td>\n",
              "      <td>-5.10</td>\n",
              "      <td>2.89</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HuberRegressor</th>\n",
              "      <td>-0.95</td>\n",
              "      <td>-0.94</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearRegression</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TransformedTargetRegressor</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lars</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LarsCV</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.04</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLarsCV</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ridge</th>\n",
              "      <td>-0.87</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KernelRidge</th>\n",
              "      <td>-9.82</td>\n",
              "      <td>-9.79</td>\n",
              "      <td>3.85</td>\n",
              "      <td>0.35</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLarsIC</th>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoCV</th>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BayesianRidge</th>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ElasticNetCV</th>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.86</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.18</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LinearSVR</th>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>1.64</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeCV</th>\n",
              "      <td>-0.86</td>\n",
              "      <td>-0.85</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDRegressor</th>\n",
              "      <td>-0.88</td>\n",
              "      <td>-0.87</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ElasticNet</th>\n",
              "      <td>-0.10</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OrthogonalMatchingPursuit</th>\n",
              "      <td>-0.82</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>1.58</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GaussianProcessRegressor</th>\n",
              "      <td>-8.68</td>\n",
              "      <td>-8.65</td>\n",
              "      <td>3.64</td>\n",
              "      <td>1.67</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NuSVR</th>\n",
              "      <td>-1.00</td>\n",
              "      <td>-1.00</td>\n",
              "      <td>1.66</td>\n",
              "      <td>3.09</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>OrthogonalMatchingPursuitCV</th>\n",
              "      <td>-0.93</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PoissonRegressor</th>\n",
              "      <td>-0.56</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.13</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RANSACRegressor</th>\n",
              "      <td>-1.27</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>1.76</td>\n",
              "      <td>0.41</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreeRegressor</th>\n",
              "      <td>-1.91</td>\n",
              "      <td>-1.90</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.06</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HistGradientBoostingRegressor</th>\n",
              "      <td>-1.28</td>\n",
              "      <td>-1.27</td>\n",
              "      <td>1.77</td>\n",
              "      <td>0.44</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVR</th>\n",
              "      <td>-1.21</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>1.74</td>\n",
              "      <td>0.74</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LGBMRegressor</th>\n",
              "      <td>-1.31</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MLPRegressor</th>\n",
              "      <td>-1.23</td>\n",
              "      <td>-1.22</td>\n",
              "      <td>1.75</td>\n",
              "      <td>4.89</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TweedieRegressor</th>\n",
              "      <td>-0.45</td>\n",
              "      <td>-0.45</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ExtraTreesRegressor</th>\n",
              "      <td>-1.21</td>\n",
              "      <td>-1.20</td>\n",
              "      <td>1.74</td>\n",
              "      <td>0.69</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AdaBoostRegressor</th>\n",
              "      <td>-0.77</td>\n",
              "      <td>-0.76</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeRegressor</th>\n",
              "      <td>-1.90</td>\n",
              "      <td>-1.89</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestRegressor</th>\n",
              "      <td>-1.25</td>\n",
              "      <td>-1.25</td>\n",
              "      <td>1.76</td>\n",
              "      <td>1.55</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBRegressor</th>\n",
              "      <td>-1.22</td>\n",
              "      <td>-1.21</td>\n",
              "      <td>1.74</td>\n",
              "      <td>0.27</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GradientBoostingRegressor</th>\n",
              "      <td>-1.23</td>\n",
              "      <td>-1.22</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.59</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BaggingRegressor</th>\n",
              "      <td>-1.35</td>\n",
              "      <td>-1.35</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNeighborsRegressor</th>\n",
              "      <td>-1.31</td>\n",
              "      <td>-1.30</td>\n",
              "      <td>1.78</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Lasso</th>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.03</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DummyRegressor</th>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.02</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoLars</th>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.02</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caf25c79-d938-40b3-bf83-07e9a99dbc47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-caf25c79-d938-40b3-bf83-07e9a99dbc47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-caf25c79-d938-40b3-bf83-07e9a99dbc47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "regresion_models.sort_values(by='pearsonr_scorer', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZbmz0vOrUd6",
        "outputId": "87af1c65-6fef-4356-8ec1-5f844c38a598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 4.71979734\n",
            "Validation score: -1.505735\n",
            "Iteration 2, loss = 2.14367023\n",
            "Validation score: -0.839765\n",
            "Iteration 3, loss = 1.49076401\n",
            "Validation score: -0.297101\n",
            "Iteration 4, loss = 1.14684008\n",
            "Validation score: -0.015613\n",
            "Iteration 5, loss = 0.95295384\n",
            "Validation score: 0.115487\n",
            "Iteration 6, loss = 0.82427153\n",
            "Validation score: 0.226772\n",
            "Iteration 7, loss = 0.70840598\n",
            "Validation score: 0.304941\n",
            "Iteration 8, loss = 0.63150698\n",
            "Validation score: 0.403209\n",
            "Iteration 9, loss = 0.57486295\n",
            "Validation score: 0.457073\n",
            "Iteration 10, loss = 0.54702394\n",
            "Validation score: 0.492627\n",
            "Iteration 11, loss = 0.52554875\n",
            "Validation score: 0.522288\n",
            "Iteration 12, loss = 0.50982342\n",
            "Validation score: 0.536281\n",
            "Iteration 13, loss = 0.50270128\n",
            "Validation score: 0.546131\n",
            "Iteration 14, loss = 0.49512578\n",
            "Validation score: 0.555069\n",
            "Iteration 15, loss = 0.48434199\n",
            "Validation score: 0.558638\n",
            "Iteration 16, loss = 0.47785536\n",
            "Validation score: 0.564118\n",
            "Iteration 17, loss = 0.47487023\n",
            "Validation score: 0.567721\n",
            "Iteration 18, loss = 0.46751262\n",
            "Validation score: 0.574555\n",
            "Iteration 19, loss = 0.46565905\n",
            "Validation score: 0.576364\n",
            "Iteration 20, loss = 0.46101382\n",
            "Validation score: 0.579231\n",
            "Iteration 21, loss = 0.45685486\n",
            "Validation score: 0.579945\n",
            "Iteration 22, loss = 0.45378253\n",
            "Validation score: 0.577544\n",
            "Iteration 23, loss = 0.44988864\n",
            "Validation score: 0.583076\n",
            "Iteration 24, loss = 0.45207006\n",
            "Validation score: 0.588375\n",
            "Iteration 25, loss = 0.44827610\n",
            "Validation score: 0.587995\n",
            "Iteration 26, loss = 0.44363961\n",
            "Validation score: 0.592460\n",
            "Iteration 27, loss = 0.44074698\n",
            "Validation score: 0.595338\n",
            "Iteration 28, loss = 0.43830335\n",
            "Validation score: 0.596740\n",
            "Iteration 29, loss = 0.43573326\n",
            "Validation score: 0.593198\n",
            "Iteration 30, loss = 0.43342247\n",
            "Validation score: 0.591307\n",
            "Iteration 31, loss = 0.43075511\n",
            "Validation score: 0.594105\n",
            "Iteration 32, loss = 0.43131253\n",
            "Validation score: 0.598770\n",
            "Iteration 33, loss = 0.43125381\n",
            "Validation score: 0.595728\n",
            "Iteration 34, loss = 0.42219582\n",
            "Validation score: 0.587725\n",
            "Iteration 35, loss = 0.43575504\n",
            "Validation score: 0.590733\n",
            "Iteration 36, loss = 0.42120066\n",
            "Validation score: 0.598355\n",
            "Iteration 37, loss = 0.42068156\n",
            "Validation score: 0.605212\n",
            "Iteration 38, loss = 0.41569990\n",
            "Validation score: 0.591686\n",
            "Iteration 39, loss = 0.42167687\n",
            "Validation score: 0.604740\n",
            "Iteration 40, loss = 0.41002227\n",
            "Validation score: 0.602012\n",
            "Iteration 41, loss = 0.40819439\n",
            "Validation score: 0.596959\n",
            "Iteration 42, loss = 0.41035647\n",
            "Validation score: 0.595760\n",
            "Iteration 43, loss = 0.40644446\n",
            "Validation score: 0.606743\n",
            "Iteration 44, loss = 0.41485733\n",
            "Validation score: 0.594565\n",
            "Iteration 45, loss = 0.41049433\n",
            "Validation score: 0.608886\n",
            "Iteration 46, loss = 0.40006234\n",
            "Validation score: 0.599621\n",
            "Iteration 47, loss = 0.39851431\n",
            "Validation score: 0.609427\n",
            "Iteration 48, loss = 0.40085229\n",
            "Validation score: 0.611407\n",
            "Iteration 49, loss = 0.39736676\n",
            "Validation score: 0.616529\n",
            "Iteration 50, loss = 0.39803443\n",
            "Validation score: 0.597082\n",
            "Iteration 51, loss = 0.40694620\n",
            "Validation score: 0.603773\n",
            "Iteration 52, loss = 0.39878089\n",
            "Validation score: 0.608532\n",
            "Iteration 53, loss = 0.39358338\n",
            "Validation score: 0.606101\n",
            "Iteration 54, loss = 0.40112268\n",
            "Validation score: 0.611006\n",
            "Iteration 55, loss = 0.39576347\n",
            "Validation score: 0.611018\n",
            "Iteration 56, loss = 0.39315299\n",
            "Validation score: 0.584931\n",
            "Iteration 57, loss = 0.41196276\n",
            "Validation score: 0.594434\n",
            "Iteration 58, loss = 0.40512821\n",
            "Validation score: 0.593410\n",
            "Iteration 59, loss = 0.38535971\n",
            "Validation score: 0.604147\n",
            "Iteration 60, loss = 0.39247480\n",
            "Validation score: 0.595980\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Train pearson:  0.7904968501746005\n",
            "Test pearson:  -0.058674455827135964\n"
          ]
        }
      ],
      "source": [
        "# Train MLP model\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(200, 50), learning_rate='adaptive', early_stopping=True, max_iter=1000, verbose=True)\n",
        "mlp.fit(X_train_features_norm, y_train)\n",
        "\n",
        "y_pred_train = mlp.predict(X_train_features_norm)\n",
        "print(\"Train pearson: \", pearsonr(y_train, y_pred_train)[0])\n",
        "\n",
        "y_pred_test = mlp.predict(X_test_features_norm)\n",
        "print(\"Test pearson: \", pearsonr(y_test, y_pred_test)[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Siames network"
      ],
      "metadata": {
        "id": "3hU6PvIA5GdE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoyuNNAWrUd6"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the `en_core_web_md` model, which includes pre-trained word embeddings.\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "# Define the sub-network architecture.\n",
        "def create_subnetwork():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  return model\n",
        "\n",
        "# Define the inputs and outputs of the Siamese network.\n",
        "input_a = Input(shape=(max_len,))\n",
        "input_b = Input(shape=(max_len,))\n",
        "\n",
        "subnetwork = create_subnetwork()\n",
        "output_a = subnetwork(input_a)\n",
        "output_b = subnetwork(input_b)\n",
        "\n",
        "similarity = dot([output_a, output_b], axes=-1, normalize=True)\n",
        "siamese_model = Model([input_a, input_b], similarity)\n",
        "\n",
        "# Compile the model and specify the loss function and optimizer to use.\n",
        "siamese_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "# To generate word embeddings using `spaCy`, you can iterate over the tokens in your text data and use the `.vector` attribute of each token.\n",
        "sentences_a = []\n",
        "sentences_b = []\n",
        "labels = []\n",
        "\n",
        "for a, b, label in zip(sentences_a, sentences_b, labels):\n",
        "  # Tokenize the sentences and create a list of embeddings for each one.\n",
        "  embeddings_a = [token.vector for token in nlp(a)]\n",
        "  embeddings_b = [token.vector for token in nlp(b)]\n",
        "  \n",
        "  # Pad the sequences to the same length.\n",
        "  padded_a = pad_sequences([embeddings_a], maxlen=max_len, padding='post')\n",
        "  padded_b = pad_sequences([embeddings_b], maxlen=max_len, padding='post')\n",
        "  \n",
        "  # Add the padded sequences and labels to the list.\n",
        "  sentences_a.append(padded_a)\n",
        "  sentences_b.append(padded_b)\n",
        "  labels.append(label)\n",
        "\n",
        "# Train the model on the prepared dataset.\n",
        "siamese_model.fit([sentences_a, sentences_b], labels, batch_size=32, epochs=10)\n",
        "\n",
        "# To evaluate the model, you can pass it a pair of sentences and use the output to measure their similarity.\n",
        "similarity = siamese_model.predict([sentence_a, sentence_b])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Siamese network is a type of neural network architecture that is used for learning similarity between two input objects. It consists of two or more identical sub-networks, which share the same weights and architecture. The sub-networks are trained to process the input objects and generate feature vectors, which are then compared to measure the similarity between the input objects.\n",
        "\n",
        "One common application of Siamese networks is in natural language processing tasks, where they can be used to measure the similarity between sentences. To do this, the input to each sub-network would be a sentence, and the output would be a feature vector representing the sentence. The feature vectors can then be compared using a distance measure, such as cosine similarity, to determine the similarity between the sentences.\n",
        "\n",
        "There are a number of different approaches to training a Siamese network for sentence similarity. One approach is to use a dataset of pairs of sentences, where each pair is labeled as either similar or not similar. The network can be trained to classify the pairs into these two categories using a binary cross-entropy loss function. Alternatively, the network can be trained to directly predict the similarity between the pairs using a regression loss function, such as mean squared error.\n",
        "\n",
        "It's also possible to use a Siamese network in conjunction with a pre-trained language model, such as BERT, to improve the quality of the feature vectors and increase the accuracy of the similarity measurement. This can be done by fine-tuning the language model on a dataset of sentence pairs, and then using the trained language model as one of the sub-networks in the Siamese network."
      ],
      "metadata": {
        "id": "0ssx_bhx6V6d"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "41ac88366789838e094cd1c6bdb6cdd75528fbdfac17cfaa697c2532672c6f1f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}