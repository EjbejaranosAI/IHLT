{"cells":[{"cell_type":"markdown","metadata":{"id":"A7q5DjXenGU7"},"source":["## Libraries"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["!pip install -q spacy nltk numpy pandas scikit-learn pyjarowinkler lazypredict"]},{"cell_type":"code","execution_count":263,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1670661492620,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"67vwZWxBcGZF","outputId":"9baddf20-b957-4e36-840a-a4e819f7f2d5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/rob/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/rob/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/rob/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/rob/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /home/rob/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package gutenberg to /home/rob/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":263,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import re\n","import nltk\n","import spacy\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","from itertools import chain\n","from functools import partial\n","from argparse import Namespace\n","from pyjarowinkler import distance\n","\n","from nltk.wsd import lesk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.metrics import jaccard_distance\n","from nltk.corpus import stopwords, wordnet\n","\n","from scipy.stats import pearsonr\n","\n","from sklearn import linear_model\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.preprocessing import StandardScaler\n","\n","from lazypredict.Supervised import LazyClassifier\n","\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","nltk.download('gutenberg')"]},{"cell_type":"markdown","metadata":{"id":"TS52eWLnnJvr"},"source":["## Download data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1669293565602,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"k0hykeMvnNtn","outputId":"51914aeb-8ac8-45cb-d3ef-7d67bcd7a7c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2003  100  2003    0     0  47690      0 --:--:-- --:--:-- --:--:-- 47690\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  122k  100  122k    0     0   505k      0 --:--:-- --:--:-- --:--:--  503k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  115k  100  115k    0     0   345k      0 --:--:-- --:--:-- --:--:--  345k\n"]}],"source":["#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/trial.tgz https://gebakx.github.io/ihlt/sts/resources/trial.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/train.tgz https://gebakx.github.io/ihlt/sts/resources/train.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/test-gold.tgz https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz"]},{"cell_type":"markdown","metadata":{"id":"a91zUN7nIF_h"},"source":["# Bring data"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1203,"status":"ok","timestamp":1670661368611,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"jpDBKAHfpe-Z","outputId":"500addc2-c71a-4bfc-d5f4-ac57ba7d9dcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["train/\n","train/00-readme.txt\n","train/STS.output.MSRpar.txt\n","train/STS.input.SMTeuroparl.txt\n","train/STS.input.MSRpar.txt\n","train/STS.gs.MSRpar.txt\n","train/STS.input.MSRvid.txt\n","train/STS.gs.MSRvid.txt\n","train/correlation.pl\n","train/STS.gs.SMTeuroparl.txt\n","trial/\n","trial/STS.input.txt\n","trial/00-readme.txt\n","trial/STS.gs.txt\n","trial/STS.ouput.txt\n","test-gold/\n","test-gold/STS.input.MSRpar.txt\n","test-gold/STS.gs.MSRpar.txt\n","test-gold/STS.input.MSRvid.txt\n","test-gold/STS.gs.MSRvid.txt\n","test-gold/STS.input.SMTeuroparl.txt\n","test-gold/STS.gs.SMTeuroparl.txt\n","test-gold/STS.input.surprise.SMTnews.txt\n","test-gold/STS.gs.surprise.SMTnews.txt\n","test-gold/STS.input.surprise.OnWN.txt\n","test-gold/STS.gs.surprise.OnWN.txt\n","test-gold/STS.gs.ALL.txt\n","test-gold/00-readme.txt\n"]}],"source":["!tar zxvf ../final_project/train.tgz\n","!tar zxvf ../final_project/trial.tgz\n","!tar zxvf ../final_project/test-gold.tgz\n","\n","!rm ../final_project/train.tgz\n","!rm ../final_project/test-gold.tgz \n","!rm ../final_project/trial.tgz\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TLgFj_E7nKdQ"},"source":["# Usesful functions"]},{"cell_type":"code","execution_count":127,"metadata":{"id":"1_VF58N6nQre"},"outputs":[],"source":["from typing import List\n","\n","\n","def apply_jaccard_lesk(sentence1: str, sentence2: str):\n","\n","  # Apply lesk to sentence 1\n","  synset1 = [ lesk(sentence1, word) for word in sentence1 ]\n","  synset1 = { word for word in synset1 if word is not None }\n","\n","  # Apply lesk to sentence 1\n","  synset2 = [ lesk(sentence2, word) for word in sentence2 ]\n","  synset2 = { word for word in synset2 if word is not None }\n","\n","  # Calculate distance\n","  distance = jaccard_distance(synset1, synset2)\n","\n","  return distance\n","\n","# ------------------------------ #\n","# Lemmatization text process\n","# ------------------------------ #\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"NN\": \"n\",\n","                \"NNS\": \"n\",\n","                \"NNP\": \"n\",\n","                \"NNPS\": \"n\",\n","                \"VB\": \"v\",\n","                \"VBD\": \"v\",\n","                \"VBG\": \"v\",\n","                \"VBN\": \"v\",\n","                \"VBP\": \"v\",\n","                \"VBZ\": \"v\",\n","                \"RB\": \"r\",\n","                \"RBR\": \"r\",\n","                \"RBS\": \"r\",\n","                \"JJ\": \"a\",\n","                \"JJR\": \"a\",\n","                \"JJS\": \"a\",}\n","        \n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","def lemmatize(column):\n","  lemmas = []\n","  for sentence in tqdm(column):\n","    sentence_lemmas = []\n","    for word in nltk.word_tokenize(sentence):\n","      sentence_lemmas.append(lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())))\n","    lemmas.append(sentence_lemmas)\n","  return lemmas\n","\n","  #return  [ list(lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())) for word in nltk.word_tokenize(sentence)) for sentence in column ]  \n","\n","# ------------------------------ #\n","#   Stopwords initialization\n","# ------------------------------ #\n","stopwords = nltk.corpus.stopwords.words(\"english\")\n","stopwords[:10]\n","stopwords += string.punctuation\n","stopwords += ['.', ',', ';', '.\"']\n","\n","# ------------------------------ #\n","# Similarity Function\n","# ------------------------------ #\n","def jaccard_similarity(s1: List[str], s2: List[str]):\n","    s1 = set(s1)\n","    s2 = set(s2)\n","    intersection = len(s1.intersection(s2))\n","    union = len(s1) + len(s2) - intersection\n","    return float(intersection) / float(union)\n","\n","\n","def jaccard_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    return np.array(list(map(jaccard_similarity, s1, s2)))\n","\n","\n","def calculateJarowinklerSimilarity(dataframe, column1, column2):\n","\n","  aux = []\n","  for row in dataframe.itertuples():\n","    \n","    # Longest one selected\n","    if len(row[column1]) >= len(row[column2]):\n","      sentence1 = row[column1]\n","      sentence2 = row[column2]\n","    else:\n","      sentence1 = row[column2]\n","      sentence2 = row[column1]\n","\n","    similarities_array = []\n","    for word1 in sentence1:\n","      max = 0\n","\n","      for word2 in sentence2:\n","        similarity = distance.get_jaro_distance(str(word1), str(word2), winkler=True, scaling=0.1)\n","           \n","        if max < similarity:\n","          max = similarity\n","        \n","      similarities_array.append(max)\n","\n","    aux.append(np.array(similarities_array).mean())\n","\n","  return aux\n","\n","\n","# ------------------------------ #\n","# Lemmatization text process\n","# ------------------------------ #\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","\n","    return tag_dict.get(tag, wordnet.NOUN)"]},{"cell_type":"markdown","metadata":{"id":"wm2VxdulsYgE"},"source":["# Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"TfqKCsKhN3fR"},"source":["### Data information\n","- trial : includes the definition of the scores, a sample of 5 sentence pairs and the input and output formats. It is not needed, but it is useful for prototyping.\n","\n","- train : training data from paraphrasing data sets, input and output formats.\n","\n","- test : test data from paraphrasing data sets."]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1670661604957,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"QdEvE55Xsb0L"},"outputs":[],"source":["train_path = '../final_project/train'\n","trial_path = '../final_project/trial'\n","test_path  = '../final_project/test-gold'"]},{"cell_type":"code","execution_count":210,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670663705254,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"ng6PiMZJL8JS","outputId":"ab0f2b16-8538-41ad-b451-5bdf50700f86"},"outputs":[],"source":["def read_data(text_datas: list[str], gs_datas: list[str]):\n","  all_df_text = []\n","  for text_data, gs_data in zip(text_datas, gs_datas):\n","    df_text = pd.read_csv(text_data, sep=r'\\t', engine='python', header=None)\n","    df_text.columns = [\"text1\", \"text2\"]\n","    df_text['gs'] = pd.read_csv(gs_data, sep='\\t', header=None)\n","    all_df_text.append(df_text.dropna())\n","  return pd.concat(all_df_text)\n","\n","def get_dataset(path: str) -> pd.DataFrame:\n","  files = sorted(os.listdir(path))\n","  input_files = [ os.path.join(path, file) for file in files if 'input' in file ]\n","  gs_files = [ os.path.join(path, file) for file in files if 'gs' in file ]\n","  df = read_data(input_files, gs_files)\n","  return df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Similarities**"]},{"cell_type":"code","execution_count":211,"metadata":{},"outputs":[],"source":["tokenized_text1 = [nltk.word_tokenize(phrase) for phrase in df['text1']]\n"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["syn = wordnet.synsets('hello')"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Synset('hello.n.01')]\n"]}],"source":["print(syn)"]},{"cell_type":"code","execution_count":173,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/2234 [00:00<?, ?it/s]"]},{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[173], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m wordnet\n\u001b[1;32m      2\u001b[0m \u001b[39m#syn = wordnet.synsets('hello')[0]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m synset \u001b[39m=\u001b[39m [wordnet\u001b[39m.\u001b[39msynsets(phrase)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m tqdm(df[\u001b[39m'\u001b[39m\u001b[39mtext1\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(synset[\u001b[39m0\u001b[39m])\n","Cell \u001b[0;32mIn[173], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m wordnet\n\u001b[1;32m      2\u001b[0m \u001b[39m#syn = wordnet.synsets('hello')[0]\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m synset \u001b[39m=\u001b[39m [wordnet\u001b[39m.\u001b[39;49msynsets(phrase)[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;00m phrase \u001b[39min\u001b[39;00m tqdm(df[\u001b[39m'\u001b[39m\u001b[39mtext1\u001b[39m\u001b[39m'\u001b[39m])]\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(synset[\u001b[39m0\u001b[39m])\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["from nltk.corpus import wordnet\n","#syn = wordnet.synsets('hello')[0]\n","\n","synset = [wordnet.synsets(phrase)[0] for phrase in tqdm(df['text1'])]\n","print(synset[0])\n"]},{"cell_type":"code","execution_count":139,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":388,"status":"error","timestamp":1670662891072,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"0tx_t_KsL_Fl","outputId":"842eff73-ae95-4a78-d46c-ecf589fd656c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 6714.01it/s]\n","100%|██████████| 2234/2234 [00:00<00:00, 7651.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Lemmatize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:05<00:00, 441.02it/s]\n","100%|██████████| 2234/2234 [00:04<00:00, 476.19it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Stopwords features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 15164.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["NLTK Words features\n","Synset features\n","Spacy words features\n","Ngrams features\n","Word synonyms features\n"]},{"data":{"text/plain":["array([[0.5483871 , 0.42105263, 0.33333333, ..., 1.        , 0.56666667,\n","        0.44444444],\n","       [0.5483871 , 0.42105263, 0.34782609, ..., 1.        , 0.56666667,\n","        0.44444444],\n","       [0.5       , 0.46153846, 0.33333333, ..., 1.        , 0.52941176,\n","        0.16666667]])"]},"execution_count":139,"metadata":{},"output_type":"execute_result"}],"source":["def get_features(df: pd.DataFrame):\n","\n","    #--------------------------------------------#\n","    # 1. Tokenize features\n","    #--------------------------------------------#\n","    print(\"Tokenize features\")\n","    tokenized_text1 = [nltk.word_tokenize(phrase) for phrase in tqdm(df['text1'])]\n","    tokenized_text2 = [nltk.word_tokenize(phrase) for phrase in tqdm(df['text2'])]\n","\n","    #--------------------------------------------#\n","    # 2. Lemmatize features\n","    #--------------------------------------------#\n","    print(\"Lemmatize features\")\n","    lemmatize_text1 = lemmatize(df['text1'])\n","    lemmatize_text2 = lemmatize(df['text2'])\n","\n","    #--------------------------------------------#\n","    # 3. Stopwords features\n","    #--------------------------------------------#\n","    print(\"Stopwords features\")\n","    stopwords_text1 = []\n","    stopwords_text2 = []\n","\n","    for i in tqdm(range(len(tokenized_text1))):\n","        stopwords_text1.append([w for w in lemmatize_text1[i] if not w.lower() in stopwords])\n","        stopwords_text2.append([w for w in lemmatize_text2[i] if not w.lower() in stopwords])\n","\n","    #--------------------------------------------#\n","    # 4. NLTK Words features\n","    #--------------------------------------------#\n","    print(\"NLTK Words features\")\n","\n","    \n","\n","    #--------------------------------------------#\n","    # 5. Synset features\n","    #--------------------------------------------#\n","    print(\"Synset features\")\n","\n","    #--------------------------------------------#\n","    # 6. Spacy words features\n","    #--------------------------------------------#\n","    print(\"Spacy words features\")\n","\n","    #--------------------------------------------#\n","    # 7. Ngrams features\n","    #--------------------------------------------#\n","    print(\"Ngrams features\")\n","\n","    #--------------------------------------------#\n","    # 8.Word synonyms features\n","    #--------------------------------------------#\n","    print(\"Word synonyms features\")\n","\n","\n","    features = [\n","        jaccard_similarity_list(tokenized_text1, tokenized_text2),\n","        jaccard_similarity_list(lemmatize_text1, lemmatize_text2),\n","        jaccard_similarity_list(stopwords_text1, stopwords_text2),\n","    ]\n","    return np.array(features)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Training**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get training dataset"]},{"cell_type":"code","execution_count":212,"metadata":{"id":"fifhUTGhRKBS"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2234, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>4.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Micron has declared its first quarterly profit...</td>\n","      <td>Micron's numbers also marked the first quarter...</td>\n","      <td>3.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The fines are part of failed Republican effort...</td>\n","      <td>Perry said he backs the Senate's efforts, incl...</td>\n","      <td>2.80</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n","      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n","      <td>2.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  But other sources close to the sale said Viven...   \n","1  Micron has declared its first quarterly profit...   \n","2  The fines are part of failed Republican effort...   \n","3  The American Anglican Council, which represent...   \n","4  The tech-loaded Nasdaq composite rose 20.96 po...   \n","\n","                                               text2   gs  \n","0  But other sources close to the sale said Viven... 4.00  \n","1  Micron's numbers also marked the first quarter... 3.75  \n","2  Perry said he backs the Senate's efforts, incl... 2.80  \n","3  The American Anglican Council, which represent... 3.40  \n","4  The technology-laced Nasdaq Composite Index <.... 2.40  "]},"execution_count":212,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = get_dataset(train_path)\n","print(train_dataset.shape)\n","train_dataset.head()"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"data":{"text/plain":["(2234,)"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["y_train = train_dataset['gs'].values\n","y_train.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the training dataset"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 7156.03it/s]\n","100%|██████████| 2234/2234 [00:00<00:00, 6807.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Lemmatize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:04<00:00, 452.73it/s]\n","100%|██████████| 2234/2234 [00:04<00:00, 461.01it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Stopwords features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 15124.47it/s]"]},{"name":"stdout","output_type":"stream","text":["NLTK Words features\n","Synset features\n","Spacy words features\n","Ngrams features\n","Word synonyms features\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["(3, 2234)"]},"execution_count":214,"metadata":{},"output_type":"execute_result"}],"source":["X_train_features: np.ndarray = get_features(train_dataset)\n","X_train_features.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Testing**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get the test dataset"]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2817, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The problem likely will mean corrective change...</td>\n","      <td>He said the problem needs to be corrected befo...</td>\n","      <td>4.40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The technology-laced Nasdaq Composite Index .I...</td>\n","      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n","      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n","      <td>3.60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SEC Chairman William Donaldson said there is a...</td>\n","      <td>\"I think there's a building confidence that th...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n","      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n","      <td>1.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  The problem likely will mean corrective change...   \n","1  The technology-laced Nasdaq Composite Index .I...   \n","2  \"It's a huge black eye,\" said publisher Arthur...   \n","3  SEC Chairman William Donaldson said there is a...   \n","4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n","\n","                                               text2   gs  \n","0  He said the problem needs to be corrected befo... 4.40  \n","1  The broad Standard & Poor's 500 Index .SPX inc... 0.80  \n","2  \"It's a huge black eye,\" Arthur Sulzberger, th... 3.60  \n","3  \"I think there's a building confidence that th... 3.40  \n","4  In New York, Vivendi shares were 1.4 percent d... 1.40  "]},"execution_count":215,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset = get_dataset(test_path)\n","print(test_dataset.shape)\n","test_dataset.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the test dataset"]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2817/2817 [00:00<00:00, 9272.33it/s]\n","100%|██████████| 2817/2817 [00:00<00:00, 9510.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Lemmatize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2817/2817 [00:04<00:00, 622.26it/s] \n","100%|██████████| 2817/2817 [00:04<00:00, 622.94it/s] \n"]},{"name":"stdout","output_type":"stream","text":["Stopwords features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2817/2817 [00:00<00:00, 16974.67it/s]"]},{"name":"stdout","output_type":"stream","text":["NLTK Words features\n","Synset features\n","Spacy words features\n","Ngrams features\n","Word synonyms features\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"data":{"text/plain":["(3, 2817)"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["X_test_features: np.ndarray = get_features(test_dataset)\n","X_test_features.shape"]},{"cell_type":"code","execution_count":217,"metadata":{},"outputs":[{"data":{"text/plain":["(2817,)"]},"execution_count":217,"metadata":{},"output_type":"execute_result"}],"source":["y_test = test_dataset['gs'].values\n","y_test.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Normalize all features"]},{"cell_type":"code","execution_count":275,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2817,)\n","(2234,)\n","[ 0.84951116 -2.22262725  0.16681373 ...  0.72150539  0.08147656\n","  0.08147656]\n","[ 0.84951116 -2.22262725  0.16681373 ...  0.72150539  0.08147656\n","  0.08147656]\n"]}],"source":["# Normalize the data\n","\n","scaler = StandardScaler()\n","y_test_norm = scaler.fit_transform(y_test.reshape((len(y_test), 1)))[:, 0]\n","y_train_norm = scaler.fit_transform(y_train.reshape((len(y_train), 1)))[:, 0]\n","print(y_test_norm.shape)\n","print(y_train_norm.shape)\n","print(y_test_norm)\n","print(y_test_norm)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train the model"]},{"cell_type":"code","execution_count":276,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train_features shape:  (2234, 3)\n","y_train shape:  (2234,)\n","X_test_features shape:  (2817, 3)\n","y_test shape:  (2817,)\n"]}],"source":["# Print all shapes\n","print(\"X_train_features shape: \", X_train_features.T.shape)\n","print(\"y_train shape: \", y_train_norm.shape)\n","print(\"X_test_features shape: \", X_test_features.T.shape)\n","print(\"y_test shape: \", y_test_norm.shape)"]},{"cell_type":"code","execution_count":277,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4072025229562516\n"]}],"source":["# Create a linear regression\n","from sklearn.linear_model import LinearRegression\n","reg = LinearRegression()\n","reg.fit(X_train_features.T, y_train_norm)\n","print(reg.score(X_train_features.T, y_train_norm))"]},{"cell_type":"code","execution_count":278,"metadata":{},"outputs":[{"data":{"text/plain":["-0.5307562886856259"]},"execution_count":278,"metadata":{},"output_type":"execute_result"}],"source":["reg.score(X_test_features.T, y_test_norm)"]},{"cell_type":"code","execution_count":279,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 28/42 [00:28<00:03,  4.38it/s]"]}],"source":["from lazypredict.Supervised import LazyRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","\n","# fit all models\n","reg = LazyRegressor(predictions=True)\n","regresion_models, regresion_predictions = reg.fit(X_train_features.T, X_test_features.T, y_train_norm, y_test_norm)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Adjusted R-Squared</th>\n","      <th>R-Squared</th>\n","      <th>RMSE</th>\n","      <th>Time Taken</th>\n","    </tr>\n","    <tr>\n","      <th>Model</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>DummyRegressor</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLars</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>QuantileRegressor</th>\n","      <td>-0.03</td>\n","      <td>-0.03</td>\n","      <td>1.19</td>\n","      <td>268.13</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet</th>\n","      <td>-0.11</td>\n","      <td>-0.11</td>\n","      <td>1.24</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>TweedieRegressor</th>\n","      <td>-0.41</td>\n","      <td>-0.41</td>\n","      <td>1.39</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>RANSACRegressor</th>\n","      <td>-0.56</td>\n","      <td>-0.56</td>\n","      <td>1.46</td>\n","      <td>0.13</td>\n","    </tr>\n","    <tr>\n","      <th>PoissonRegressor</th>\n","      <td>-0.61</td>\n","      <td>-0.60</td>\n","      <td>1.48</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>PassiveAggressiveRegressor</th>\n","      <td>-0.77</td>\n","      <td>-0.77</td>\n","      <td>1.56</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor</th>\n","      <td>-0.78</td>\n","      <td>-0.78</td>\n","      <td>1.56</td>\n","      <td>0.05</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNetCV</th>\n","      <td>-0.80</td>\n","      <td>-0.80</td>\n","      <td>1.57</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>LarsCV</th>\n","      <td>-0.80</td>\n","      <td>-0.80</td>\n","      <td>1.57</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsCV</th>\n","      <td>-0.80</td>\n","      <td>-0.80</td>\n","      <td>1.57</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>LassoCV</th>\n","      <td>-0.80</td>\n","      <td>-0.80</td>\n","      <td>1.57</td>\n","      <td>0.09</td>\n","    </tr>\n","    <tr>\n","      <th>BayesianRidge</th>\n","      <td>-0.81</td>\n","      <td>-0.80</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>RidgeCV</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>Lars</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>TransformedTargetRegressor</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsIC</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>LinearRegression</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>LinearSVR</th>\n","      <td>-0.81</td>\n","      <td>-0.81</td>\n","      <td>1.58</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>SGDRegressor</th>\n","      <td>-0.82</td>\n","      <td>-0.81</td>\n","      <td>1.58</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuitCV</th>\n","      <td>-0.82</td>\n","      <td>-0.82</td>\n","      <td>1.58</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>HuberRegressor</th>\n","      <td>-0.85</td>\n","      <td>-0.85</td>\n","      <td>1.59</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuit</th>\n","      <td>-0.86</td>\n","      <td>-0.86</td>\n","      <td>1.60</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>NuSVR</th>\n","      <td>-1.05</td>\n","      <td>-1.05</td>\n","      <td>1.68</td>\n","      <td>0.38</td>\n","    </tr>\n","    <tr>\n","      <th>MLPRegressor</th>\n","      <td>-1.07</td>\n","      <td>-1.07</td>\n","      <td>1.68</td>\n","      <td>2.46</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor</th>\n","      <td>-1.20</td>\n","      <td>-1.20</td>\n","      <td>1.74</td>\n","      <td>0.18</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor</th>\n","      <td>-1.26</td>\n","      <td>-1.25</td>\n","      <td>1.76</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>HistGradientBoostingRegressor</th>\n","      <td>-1.27</td>\n","      <td>-1.27</td>\n","      <td>1.76</td>\n","      <td>0.52</td>\n","    </tr>\n","    <tr>\n","      <th>LGBMRegressor</th>\n","      <td>-1.28</td>\n","      <td>-1.27</td>\n","      <td>1.77</td>\n","      <td>0.10</td>\n","    </tr>\n","    <tr>\n","      <th>BaggingRegressor</th>\n","      <td>-1.31</td>\n","      <td>-1.31</td>\n","      <td>1.78</td>\n","      <td>0.06</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor</th>\n","      <td>-1.31</td>\n","      <td>-1.31</td>\n","      <td>1.78</td>\n","      <td>0.71</td>\n","    </tr>\n","    <tr>\n","      <th>XGBRegressor</th>\n","      <td>-1.36</td>\n","      <td>-1.36</td>\n","      <td>1.80</td>\n","      <td>0.15</td>\n","    </tr>\n","    <tr>\n","      <th>SVR</th>\n","      <td>-1.38</td>\n","      <td>-1.38</td>\n","      <td>1.81</td>\n","      <td>0.43</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreesRegressor</th>\n","      <td>-1.42</td>\n","      <td>-1.41</td>\n","      <td>1.82</td>\n","      <td>0.31</td>\n","    </tr>\n","    <tr>\n","      <th>DecisionTreeRegressor</th>\n","      <td>-1.68</td>\n","      <td>-1.68</td>\n","      <td>1.92</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreeRegressor</th>\n","      <td>-1.80</td>\n","      <td>-1.80</td>\n","      <td>1.96</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>KernelRidge</th>\n","      <td>-8.95</td>\n","      <td>-8.94</td>\n","      <td>3.69</td>\n","      <td>0.48</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianProcessRegressor</th>\n","      <td>-126686.10</td>\n","      <td>-126551.13</td>\n","      <td>416.87</td>\n","      <td>1.38</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               Adjusted R-Squared  R-Squared   RMSE  \\\n","Model                                                                 \n","DummyRegressor                              -0.02      -0.02   1.18   \n","LassoLars                                   -0.02      -0.02   1.18   \n","Lasso                                       -0.02      -0.02   1.18   \n","QuantileRegressor                           -0.03      -0.03   1.19   \n","ElasticNet                                  -0.11      -0.11   1.24   \n","TweedieRegressor                            -0.41      -0.41   1.39   \n","RANSACRegressor                             -0.56      -0.56   1.46   \n","PoissonRegressor                            -0.61      -0.60   1.48   \n","PassiveAggressiveRegressor                  -0.77      -0.77   1.56   \n","AdaBoostRegressor                           -0.78      -0.78   1.56   \n","ElasticNetCV                                -0.80      -0.80   1.57   \n","LarsCV                                      -0.80      -0.80   1.57   \n","LassoLarsCV                                 -0.80      -0.80   1.57   \n","LassoCV                                     -0.80      -0.80   1.57   \n","BayesianRidge                               -0.81      -0.80   1.57   \n","Ridge                                       -0.81      -0.81   1.57   \n","RidgeCV                                     -0.81      -0.81   1.57   \n","Lars                                        -0.81      -0.81   1.57   \n","TransformedTargetRegressor                  -0.81      -0.81   1.57   \n","LassoLarsIC                                 -0.81      -0.81   1.57   \n","LinearRegression                            -0.81      -0.81   1.57   \n","LinearSVR                                   -0.81      -0.81   1.58   \n","SGDRegressor                                -0.82      -0.81   1.58   \n","OrthogonalMatchingPursuitCV                 -0.82      -0.82   1.58   \n","HuberRegressor                              -0.85      -0.85   1.59   \n","OrthogonalMatchingPursuit                   -0.86      -0.86   1.60   \n","NuSVR                                       -1.05      -1.05   1.68   \n","MLPRegressor                                -1.07      -1.07   1.68   \n","GradientBoostingRegressor                   -1.20      -1.20   1.74   \n","KNeighborsRegressor                         -1.26      -1.25   1.76   \n","HistGradientBoostingRegressor               -1.27      -1.27   1.76   \n","LGBMRegressor                               -1.28      -1.27   1.77   \n","BaggingRegressor                            -1.31      -1.31   1.78   \n","RandomForestRegressor                       -1.31      -1.31   1.78   \n","XGBRegressor                                -1.36      -1.36   1.80   \n","SVR                                         -1.38      -1.38   1.81   \n","ExtraTreesRegressor                         -1.42      -1.41   1.82   \n","DecisionTreeRegressor                       -1.68      -1.68   1.92   \n","ExtraTreeRegressor                          -1.80      -1.80   1.96   \n","KernelRidge                                 -8.95      -8.94   3.69   \n","GaussianProcessRegressor               -126686.10 -126551.13 416.87   \n","\n","                               Time Taken  \n","Model                                      \n","DummyRegressor                       0.01  \n","LassoLars                            0.01  \n","Lasso                                0.02  \n","QuantileRegressor                  268.13  \n","ElasticNet                           0.01  \n","TweedieRegressor                     0.01  \n","RANSACRegressor                      0.13  \n","PoissonRegressor                     0.01  \n","PassiveAggressiveRegressor           0.01  \n","AdaBoostRegressor                    0.05  \n","ElasticNetCV                         0.06  \n","LarsCV                               0.03  \n","LassoLarsCV                          0.02  \n","LassoCV                              0.09  \n","BayesianRidge                        0.01  \n","Ridge                                0.01  \n","RidgeCV                              0.01  \n","Lars                                 0.02  \n","TransformedTargetRegressor           0.01  \n","LassoLarsIC                          0.01  \n","LinearRegression                     0.01  \n","LinearSVR                            0.01  \n","SGDRegressor                         0.01  \n","OrthogonalMatchingPursuitCV          0.01  \n","HuberRegressor                       0.02  \n","OrthogonalMatchingPursuit            0.01  \n","NuSVR                                0.38  \n","MLPRegressor                         2.46  \n","GradientBoostingRegressor            0.18  \n","KNeighborsRegressor                  0.02  \n","HistGradientBoostingRegressor        0.52  \n","LGBMRegressor                        0.10  \n","BaggingRegressor                     0.06  \n","RandomForestRegressor                0.71  \n","XGBRegressor                         0.15  \n","SVR                                  0.43  \n","ExtraTreesRegressor                  0.31  \n","DecisionTreeRegressor                0.01  \n","ExtraTreeRegressor                   0.01  \n","KernelRidge                          0.48  \n","GaussianProcessRegressor             1.38  "]},"execution_count":268,"metadata":{},"output_type":"execute_result"}],"source":["regresion_models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPUPgMw9k8V23lSkem9miXR","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"41ac88366789838e094cd1c6bdb6cdd75528fbdfac17cfaa697c2532672c6f1f"}}},"nbformat":4,"nbformat_minor":0}
