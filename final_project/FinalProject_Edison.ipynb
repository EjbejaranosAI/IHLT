{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"A7q5DjXenGU7"},"source":["# Semantic textual similarity\n","## Final Project IHLT - UPC 2022/2023\n","### Authors : Rob - Edison Bejarano\n","\n","1. Data\n","\n","2. What we are doing?\n","#### Techniques for preprocessing text for similarity comparison\n","\n","- Stemming: is a process that involves reducing words to their base form, or stem, in order to normalize the text and remove variations in word endings. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the stem \"run\" by a stemming algorithm.\n","\n","\n","- Lemmatization: is a process that involves reducing words to their base form, or lemma, in order to normalize the text and remove variations in word endings. Unlike stemming, lemmatization takes into account the context of the word in order to determine its lemma, resulting in more accurate and meaningful reductions. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the lemma \"run\" by a lemmatization algorithm.\n","\n","- Tf-idf weighting: Is a method for assigning a weight to each word in a document based on its relative importance. The weight is calculated by multiplying the term frequency (tf) of the word by the inverse document frequency (idf) of the word across all documents in a corpus. This weighting scheme gives higher weight to words that are more frequent within a document but less frequent across the corpus, making them more important for characterizing the document.\n","\n","- NES : Function used the Natural Language Toolkit (nltk) to identify named entities in a given sentence. The sentence parameter is the sentence in which named entities should be identified, and the binary parameter determines whether named entities should be grouped together or returned as individual tokens. The function returns a set of the named entities and individual words found in the sentence.\n","\n","\n","These techniques can be used in combination with each other or with stopwords removal to preprocess text and improve the accuracy of similarity comparison. For example, you could use stemming or lemmatization to normalize the words in the phrases, and then use tf-idf weighting to assign importance to each word based on its frequency within the phrases and across a larger corpus. This would allow you to compare the similarity of the phrases in a more meaningful and accurate way\n","\n","\n","3. Results"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Install packages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/home/ejbejaranos/IHLT/ihlt/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}],"source":["%pip install -q spacy nltk numpy pandas scikit-learn pyjarowinkler lazypredict"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Libraries"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1670661492620,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"67vwZWxBcGZF","outputId":"9baddf20-b957-4e36-840a-a4e819f7f2d5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/rob/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /home/rob/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/rob/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/rob/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/rob/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /home/rob/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package gutenberg to /home/rob/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package conll2000 to /home/rob/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n","[nltk_data] Downloading package brown to /home/rob/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package words to /home/rob/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import re\n","import nltk\n","import spacy\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","from itertools import chain\n","from functools import partial\n","from argparse import Namespace\n","from pyjarowinkler import distance\n","from collections.abc import Iterable\n","from nltk.wsd import lesk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.metrics import jaccard_distance\n","from nltk.corpus import stopwords, wordnet\n","from nltk import pos_tag, ne_chunk, Tree\n","\n","from scipy.stats import pearsonr\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import make_scorer\n","from typing import List\n","from lazypredict.Supervised import REGRESSORS, LazyRegressor\n","\n","nltk.download('wordnet')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","nltk.download('gutenberg')\n","nltk.download('conll2000')\n","nltk.download('brown')\n","nltk.download('words')"]},{"cell_type":"markdown","metadata":{"id":"TS52eWLnnJvr"},"source":["## Download data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1669293565602,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"k0hykeMvnNtn","outputId":"51914aeb-8ac8-45cb-d3ef-7d67bcd7a7c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2003  100  2003    0     0  47690      0 --:--:-- --:--:-- --:--:-- 47690\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  122k  100  122k    0     0   505k      0 --:--:-- --:--:-- --:--:--  503k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  115k  100  115k    0     0   345k      0 --:--:-- --:--:-- --:--:--  345k\n"]}],"source":["#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/trial.tgz https://gebakx.github.io/ihlt/sts/resources/trial.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/train.tgz https://gebakx.github.io/ihlt/sts/resources/train.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/test-gold.tgz https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz"]},{"cell_type":"markdown","metadata":{"id":"a91zUN7nIF_h"},"source":["# Bring data"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1203,"status":"ok","timestamp":1670661368611,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"jpDBKAHfpe-Z","outputId":"500addc2-c71a-4bfc-d5f4-ac57ba7d9dcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["train/\n","train/00-readme.txt\n","train/STS.output.MSRpar.txt\n","train/STS.input.SMTeuroparl.txt\n","train/STS.input.MSRpar.txt\n","train/STS.gs.MSRpar.txt\n","train/STS.input.MSRvid.txt\n","train/STS.gs.MSRvid.txt\n","train/correlation.pl\n","train/STS.gs.SMTeuroparl.txt\n","trial/\n","trial/STS.input.txt\n","trial/00-readme.txt\n","trial/STS.gs.txt\n","trial/STS.ouput.txt\n","test-gold/\n","test-gold/STS.input.MSRpar.txt\n","test-gold/STS.gs.MSRpar.txt\n","test-gold/STS.input.MSRvid.txt\n","test-gold/STS.gs.MSRvid.txt\n","test-gold/STS.input.SMTeuroparl.txt\n","test-gold/STS.gs.SMTeuroparl.txt\n","test-gold/STS.input.surprise.SMTnews.txt\n","test-gold/STS.gs.surprise.SMTnews.txt\n","test-gold/STS.input.surprise.OnWN.txt\n","test-gold/STS.gs.surprise.OnWN.txt\n","test-gold/STS.gs.ALL.txt\n","test-gold/00-readme.txt\n"]}],"source":["!tar zxvf ../final_project/train.tgz\n","!tar zxvf ../final_project/trial.tgz\n","!tar zxvf ../final_project/test-gold.tgz\n","\n","!rm ../final_project/train.tgz\n","!rm ../final_project/test-gold.tgz \n","!rm ../final_project/trial.tgz"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TLgFj_E7nKdQ"},"source":["# Usesful functions"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["# ------------------------------ #\n","# Jaccard similarity Function\n","# ------------------------------ #\n","def jaccard_similarity(s1: List[str], s2: List[str]):\n","    s1 = set(s1)\n","    s2 = set(s2)\n","    intersection = len(s1.intersection(s2))\n","    union = len(s1) + len(s2) - intersection\n","    return float(intersection) / float(union)\n","\n","# ------------------------------ #\n","# Jaccard Similarity List\n","# ------------------------------ #\n","def jaccard_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    sims = []\n","    for l1, l2 in zip(s1, s2):\n","        sim = jaccard_similarity(l1, l2)\n","        sims.append(sim)\n","    return np.array(sims)\n","\n","\n","def dice_similarity(s1, s2):\n","    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n","    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n","    s1 = set(s1)\n","    s2 = set(s2)\n","    intersection = s1.intersection(s2)\n","    return 2 * len(intersection) / (len(s1) + len(s2))\n","\n","def dice_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    sims = []\n","    for l1, l2 in zip(s1, s2):\n","        sim = dice_similarity(l1, l2)\n","        sims.append(sim)\n","    return np.array(sims)\n","\n","# ------------------------------ #\n","# Jarowinkler Similarity\n","# ------------------------------ #   \n","def calculateJarowinklerSimilarity(dataframe, column1, column2):\n","\n","    aux = []\n","    for row in dataframe.itertuples():\n","            \n","        # Longest one selected\n","        if len(row[column1]) >= len(row[column2]):\n","            sentence1 = row[column1]\n","            sentence2 = row[column2]\n","        else:\n","            sentence1 = row[column2]\n","            sentence2 = row[column1]\n","\n","        similarities_array = []\n","        for word1 in sentence1:\n","            max = 0\n","\n","        for word2 in sentence2:\n","            similarity = distance.get_jaro_distance(str(word1), str(word2), winkler=True, scaling=0.1)\n","            \n","            if max < similarity:\n","                max = similarity\n","            \n","        similarities_array.append(max)\n","\n","        aux.append(np.array(similarities_array).mean())\n","\n","    return aux"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"1_VF58N6nQre"},"outputs":[],"source":["# ------------------------------ #\n","#         Get Wordnet POS\n","# ------------------------------ #\n","def get_wordnet_pos(word):\n","  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","  tag = nltk.pos_tag([word])[0][1][0].upper()\n","  tag_dict = {\n","        \"NN\": \"n\",\n","        \"NNS\": \"n\",\n","        \"NNP\": \"n\",\n","        \"NNPS\": \"n\",\n","        \"VB\": \"v\",\n","        \"VBD\": \"v\",\n","        \"VBG\": \"v\",\n","        \"VBN\": \"v\",\n","        \"VBP\": \"v\",\n","        \"VBZ\": \"v\",\n","        \"RB\": \"r\",\n","        \"RBR\": \"r\",\n","        \"RBS\": \"r\",\n","        \"JJ\": \"a\",\n","        \"JJR\": \"a\",\n","        \"JJS\": \"a\",\n","  }\n","        \n","  return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","# ------------------------------ #\n","#   Function to tokenize\n","# ------------------------------ #\n","def tokenize_column(column):\n","    #put in lowercase\n","    tokenized = [nltk.word_tokenize(sentence) for sentence in column]\n","    #Lowercase the tokens\n","    tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n","    return tokenized\n","\n","\n","#--------------------------------------------#\n","#  Function to NES\n","#--------------------------------------------#\n","def NES(sentence: str, binary: bool):\n","    x = nltk.pos_tag(nltk.word_tokenize(sentence))\n","    res = nltk.ne_chunk(x, binary=binary)\n","    necs_and_words = set()\n","    for chunk in res:\n","        if hasattr(chunk, 'label'):\n","            # Add NE\n","            token = ' '.join(term[0] for term in chunk)\n","            necs_and_words.add(token)\n","        else:\n","            token = chunk[0]\n","            if token.isalnum():\n","                necs_and_words.add(token.lower())\n","    return necs_and_words\n","\n"," #--------------------------------------------#\n"," # Function to get entities from a column\n"," # -------------------------------------------# \n","def get_entities_new(column):\n","    entities = []\n","    for sentence in column:\n","        entities.append(NES(sentence, False))\n","    return entities\n","\n","\n","\n","# ------------------------------ #\n","# Lemmatization text process\n","# ------------------------------ #\n","lemmatizer = WordNetLemmatizer()\n","# ------------------------------ #\n","#   Function to lemmatize\n","# ------------------------------ #\n","def lemmatize(column):\n","  \n","  lemmas = []\n","\n","  for sentence in tqdm(column):\n","    sentence_lemmas = []\n","    for word in nltk.word_tokenize(sentence):\n","      sentence_lemmas.append(lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())))\n","    lemmas.append(sentence_lemmas)\n","\n","  return lemmas\n","\n","\n","# ------------------------------ #\n","#   Stopwords initialization\n","# ------------------------------ #\n","stopwords_list = nltk.corpus.stopwords.words(\"english\")\n","stopwords_list[:10]\n","stopwords_list += string.punctuation\n","stopwords_list += ['.', ',', ';', '.\"']\n","\n","# ------------------------------ #\n","#   Function to remove stopwords\n","# ------------------------------ #\n","def remove_stopwords(column):\n","  tokenized = [nltk.word_tokenize(sentence) for sentence in column]\n","  #Lowercase the tokens\n","  tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n","  return [ [ word for word in sentence if word not in stopwords_list ] for sentence in tokenized ]\n","\n","\n","# ------------------------------ #\n","#   Function to synonimize\n","# ------------------------------ #\n","def synonimize_column(column):\n","  #put in lowercase\n","  tokenized = [nltk.word_tokenize(sentence) for sentence in column]\n","  #Lowercase the tokens\n","  tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n","  #Synonimize\n","  synonimized = [ [ word for word in sentence if word not in stopwords ] for sentence in tokenized ]\n","\n","  return synonimized\n","\n","\n","# ------------------------------ #\n","#   Function to synset\n","# ------------------------------ #\n","def synset_column(column):\n","\n","  #Lowercase the tokens\n","  tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n","\n","  #add synonimized\n","  synonimized = [ [ word for word in sentence if word not in stopwords ] for sentence in tokenized ]\n","  #Synset\n","  synset = [ [ wordnet.synsets(word)[0] for word in tqdm(sentence)] for sentence in tqdm(synonimized)]\n","\n","  return synset\n","\n","\n","# ------------------------------ #\n","#  Function to NE(Name entities)\n","# ------------------------------ #\n","def apply_ne(sentence: str):\n","    # tokenize the sentence and find the POS tag for each token\n","    sentence = nltk.word_tokenize(sentence)\n","    \n","    sentences_ne = list(ne_chunk(pos_tag(sentence), binary=True))\n","    result = []\n","    for el in sentences_ne:\n","        if isinstance(el, Tree):\n","            leaves = el.leaves()\n","            result.append(\" \".join(word[0] for word in leaves))\n","        else:\n","            result.append(el[0])\n","    return result\n","\n","# used apply_ne function to get NE from a column\n","def get_name_entities(column):\n","  ne = []\n","  for sentence in column:\n","    ne.append(apply_ne(sentence))\n","  return ne\n","\n","\n","\n","# ------------------------------ #\n","#  Function to get ngrams\n","# ------------------------------ #\n","def get_ngrams_column(column, n):\n","  ngrams = []\n","  for sentence in column:\n","    ngrams.append(apply_ngram(sentence, n))\n","  return ngrams\n","\n","\n","def apply_ngram(sentence: list, n: int):\n","    if len(sentence) < n:\n","        return [tuple(sentence)]\n","    return list(nltk.ngrams(sentence, n))\n","\n","\n","# ------------------------------ #\n","#     Function to get lesk \n","# ------------------------------ #\n","def get_lesk_column(column):\n","  lesk_text = []\n","\n","  for sentence in column:\n","    synset = [lesk(sentence, word) for word in sentence]\n","    synset = {word for word in synset if word is not None}\n","    lesk_text.append(synset)\n","\n","  return lesk_text\n","\n","\n","  \n","def apply_jaccard_lesk(sentence1: str, sentence2: str):\n","\n","  # Apply lesk to sentence 1\n","  synset1 = [ lesk(sentence1, word) for word in sentence1 ]\n","  synset1 = { word for word in synset1 if word is not None }\n","\n","  # Apply lesk to sentence 1\n","  synset2 = [ lesk(sentence2, word) for word in sentence2 ]\n","  synset2 = { word for word in synset2 if word is not None }\n","\n","  # Calculate distance\n","  distance = jaccard_distance(synset1, synset2)\n","\n","  return distance"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['I', 'am', 'a', 'student', 'of', 'the', 'University', 'of', 'Granada', 'and', 'that', 'is', 'in', 'that', 'city', ',', 'that', 'is', 'in', 'Spain', ',', 'The', 'artificial', 'beach', 'named', 'angelica', 'is', 'going', 'to', 'be', 'super', 'cool', '.']\n"]}],"source":["# aply the function apply_ne to a phrase\n","phrase = \"I am a student of the University of Granada and that is in that city, that is in Spain, The artificial beach named angelica is going to be super cool.\"\n","#tokenize the phrase\n","ne = apply_ne(nltk.word_tokenize(phrase))\n","print(ne)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Functions of preprocessing\n","def read_data(text_datas: List[str], gs_datas: List[str]):\n","  all_df_text = []\n","  for text_data, gs_data in zip(text_datas, gs_datas):\n","    df_text = pd.read_csv(text_data, sep=r'\\t', engine='python', header=None)\n","    df_text.columns = [\"text1\", \"text2\"]\n","    df_text['gs'] = pd.read_csv(gs_data, sep='\\t', header=None)\n","    all_df_text.append(df_text.dropna())\n","  return pd.concat(all_df_text)\n","\n","def get_dataset(path: str) -> pd.DataFrame:\n","  files = sorted(os.listdir(path))\n","  input_files = [ os.path.join(path, file) for file in files if 'input' in file ]\n","  gs_files = [ os.path.join(path, file) for file in files if 'gs' in file ]\n","  df = read_data(input_files, gs_files)\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"wm2VxdulsYgE"},"source":["# Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"TfqKCsKhN3fR"},"source":["### Data information\n","- trial : includes the definition of the scores, a sample of 5 sentence pairs and the input and output formats. It is not needed, but it is useful for prototyping.\n","\n","- train : training data from paraphrasing data sets, input and output formats.\n","\n","- test : test data from paraphrasing data sets."]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1670661604957,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"QdEvE55Xsb0L"},"outputs":[],"source":["train_path = '../final_project/train'\n","trial_path = '../final_project/trial'\n","test_path  = '../final_project/test-gold'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Similarities**"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","train_dataset_pruebas = get_dataset(train_path)\n","test_dataset_pruebas = get_dataset(test_path)\n","df = train_dataset_pruebas\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:03<00:00, 570.17it/s] \n","100%|██████████| 2234/2234 [00:02<00:00, 774.27it/s] \n","100%|██████████| 2234/2234 [00:01<00:00, 1356.67it/s]\n"]}],"source":["# Tokenization features\n","tokenized_text1 = tokenize_column(df['text1'])\n","tokenized_text2 = tokenize_column(df['text2'])\n","\n","# Lemmatization features\n","lemmatize_text1 = lemmatize(df['text1'])\n","lemmatize_text2 = lemmatize(df['text2'])\n","\n","\n","#Use stopwords function to remove stopwords\n","stopwords_text1 = remove_stopwords(df['text1'])\n","stopwords_text2 = remove_stopwords(df['text2'])\n","\n","\n","\n","# Synonyms features\n","synonyms_text1 = []\n","synonyms_text2 = []\n","# Use sysnstesizer to get synonyms\n","for i in tqdm(range(len(tokenized_text1))):\n","    synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n","    synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n","\n","\n","# Synonyms features another way\n","synonimized_text1_new = synonimize_column(df['text1'])\n","synonimized_text2_new = synonimize_column(df['text2'])\n","\n","\n","# NES features\n","NES_column_text1 = get_entities_new(df['text1'])\n","NES_column_text2 = get_entities_new(df['text2'])\n","\n","# Name entities features\n","name_entities_text1 = get_name_entities(df['text1'])\n","name_entities_text2 = get_name_entities(df['text2'])\n","\n","ngrams_column_2_text1 = get_ngrams_column(tokenized_text1, 2)\n","ngrams_column_2_text2 = get_ngrams_column(tokenized_text2, 2)\n","\n","ngrams_column_3_text1 = get_ngrams_column(tokenized_text1, 3)\n","ngrams_column_3_text2 = get_ngrams_column(tokenized_text2, 3)\n","\n","# Lesk features\n","lesk_text1 = get_lesk_column(tokenized_text1)\n","lesk_text2 = get_lesk_column(tokenized_text2)\n","\n","\n","\n","# Synset features\n","#synset_text1 = synset_column(df['text1'])\n","#synset_text2 = synset_column(df['text2'])"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"data":{"text/plain":["array([0.70833333, 0.59259259, 0.51612903, ..., 1.        , 0.72340426,\n","       0.61538462])"]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["dice_similarity_list(tokenized_text1, tokenized_text2)"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Jaccard similarity tokenized:  [0.5483871  0.42105263 0.34782609]\n","Jaccard similarity lemmatize:  [0.5483871  0.42105263 0.34782609]\n","Jaccard similarity stopwords:  [0.47368421 0.46153846 0.33333333]\n","Jaccard similarity synonyms:  [0.67680608 0.30172414 0.38562092]\n","Jaccard similarity synonyms new:  [0.47368421 0.46153846 0.33333333]\n","Jaccard similarity name entities:  [0.5483871  0.42105263 0.33333333]\n","Jaccard similarity ngrams 2:  [0.37837838 0.13043478 0.20689655]\n","Jaccard similarity ngrams 3:  [0.32432432 0.04347826 0.1       ]\n","Jaccard similarity lesk:  [0.35714286 0.3125     0.22222222]\n"]}],"source":["# Jaccard similarity features\n","jaccard_similarity_tokenized = jaccard_similarity_list(tokenized_text1, tokenized_text2)\n","jaccard_similarity_synonyms_new = jaccard_similarity_list(synonimized_text1_new, synonimized_text2_new)\n","jaccard_similarity_NES = jaccard_similarity_list(NES_column_text1, NES_column_text2)\n","jaccard_similarity_lemmatize = jaccard_similarity_list(lemmatize_text1, lemmatize_text2)\n","jaccard_similarity_stopwords = jaccard_similarity_list(stopwords_text1, stopwords_text2)\n","jaccard_similarity_synonyms = jaccard_similarity_list(synonyms_text1, synonyms_text2)\n","jaccard_similarity_name_entities = jaccard_similarity_list(name_entities_text1, name_entities_text2)\n","jaccard_similarity_ngrams_2 = jaccard_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2)\n","jaccard_similarity_ngrams_3 = jaccard_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2)\n","jaccard_similarity_lesk = jaccard_similarity_list(lesk_text1, lesk_text2)\n","\n","print(\"Jaccard similarity tokenized: \", jaccard_similarity_tokenized[:3])\n","print(\"Jaccard similarity lemmatize: \", jaccard_similarity_lemmatize[:3])\n","print(\"Jaccard similarity stopwords: \", jaccard_similarity_stopwords[:3])\n","print(\"Jaccard similarity synonyms: \", jaccard_similarity_synonyms[:3])\n","print(\"Jaccard similarity synonyms new: \", jaccard_similarity_synonyms_new[:3])\n","print(\"Jaccard similarity name entities: \", jaccard_similarity_name_entities[:3])\n","print(\"Jaccard similarity ngrams 2: \", jaccard_similarity_ngrams_2[:3])\n","print(\"Jaccard similarity ngrams 3: \", jaccard_similarity_ngrams_3[:3])\n","print(\"Jaccard similarity lesk: \", jaccard_similarity_lesk[:3])"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":388,"status":"error","timestamp":1670662891072,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"0tx_t_KsL_Fl","outputId":"842eff73-ae95-4a78-d46c-ecf589fd656c"},"outputs":[],"source":["def get_features(df: pd.DataFrame):\n","\n","    #--------------------------------------------#\n","    # 0. NLTK Words features\n","    #--------------------------------------------#\n","    #print(\"NLTK Words features\")\n","    \n","    #nltk_words_text1 = []\n","    #nltk_words_text2 = []\n","\n","    #--------------------------------------------#\n","    # 1. Tokenize features\n","    #--------------------------------------------#    \n","    tokenized_text1 = tokenize_column(df['text1'])\n","    tokenized_text2 = tokenize_column(df['text2'])\n","\n","    #--------------------------------------------#\n","    # 2. Lemmatize features\n","    #--------------------------------------------#\n","    lemmatize_text1 = lemmatize(df['text1'])\n","    lemmatize_text2 = lemmatize(df['text2'])\n","\n","\n","    #--------------------------------------------#\n","    # 3. Stopwords features\n","    #--------------------------------------------#   \n","    stopwords_text1 = remove_stopwords(df['text1'])\n","    stopwords_text2 = remove_stopwords(df['text2'])\n","\n","\n","\n","    #--------------------------------------------#\n","    # 4. Synonims features\n","    #--------------------------------------------#\n","    synonyms_text1 = []\n","    synonyms_text2 = []\n","    # Use sysnstesizer to get synonyms\n","    for i in tqdm(range(len(tokenized_text1))):\n","        synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n","        synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n","\n","    \n","    #--------------------------------------------#\n","    # 5. NES features\n","    #--------------------------------------------#\n","    NES_column_text1 = get_entities_new(df['text1'])\n","    NES_column_text2 = get_entities_new(df['text2'])\n","\n","    \n","    #--------------------------------------------#\n","    # 6. Name entities features\n","    #--------------------------------------------#\n","    name_entities_text1 = get_name_entities(df['text1'])\n","    name_entities_text2 = get_name_entities(df['text2'])\n","\n","    #--------------------------------------------#\n","    # 7. Biagrams features\n","    #--------------------------------------------#\n","    ngrams_column_2_text1 = get_ngrams_column(tokenized_text1, 2)\n","    ngrams_column_2_text2 = get_ngrams_column(tokenized_text2, 2)\n","\n","    #--------------------------------------------#\n","    # 8. Triagrams features\n","    #--------------------------------------------#\n","    ngrams_column_3_text1 = get_ngrams_column(tokenized_text1, 3)\n","    ngrams_column_3_text2 = get_ngrams_column(tokenized_text2, 3)\n","\n","    #--------------------------------------------#\n","    # 9. Lesk features\n","    #--------------------------------------------#\n","    # Lesk features\n","    lesk_text1 = get_lesk_column(tokenized_text1)\n","    lesk_text2 = get_lesk_column(tokenized_text2)\n","\n","\n","\n","    \n","\n","    #--------------------------------------------#\n","    # 5. Synset features\n","    #--------------------------------------------#\n","    #print(\"Synset features\")\n","    #synset_text1 = [wordnet.synsets(phrase)[0] for phrase in tqdm(lemmatize_text1)]\n","    #synset_text2 = [wordnet.synsets(phrase)[0] for phrase in tqdm(lemmatize_text2)]\n","\n","\n","    #--------------------------------------------#\n","    # 6. Spacy words features\n","    #--------------------------------------------#\n","    #print(\"Spacy words features\")\n","    #spacy_words_text1 = []\n","    #spacy_words_text2 = []\n","\n","\n","    #--------------------------------------------#\n","    # 7. Ngrams features\n","    #--------------------------------------------#\n","    #print(\"Ngrams features\")\n","    #ngrams_text1 = []\n","\n","    #--------------------------------------------#\n","    # 8.Word synonyms features\n","    #--------------------------------------------#\n","    #print(\"Word synonyms features\")\n","\n","\n","    features = [\n","        # Jaccard similarity\n","        jaccard_similarity_list(tokenized_text1, tokenized_text2),\n","        jaccard_similarity_list(lemmatize_text1, lemmatize_text2),\n","        jaccard_similarity_list(stopwords_text1, stopwords_text2),\n","        jaccard_similarity_list(synonyms_text1, synonyms_text2),\n","        jaccard_similarity_list(NES_column_text1, NES_column_text2),\n","        jaccard_similarity_list(name_entities_text1, name_entities_text2),\n","        jaccard_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n","        jaccard_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n","        jaccard_similarity_list(lesk_text1, lesk_text2),\n","        #jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n","        #jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n","        #jaccard_similarity_list(ngrams_text1, ngrams_text2),\n","        #jaccard_similarity_list(synset_text1, synset_text2),\n","        \n","        \n","        # Dice similarity\n","        dice_similarity_list(tokenized_text1, tokenized_text2),\n","        dice_similarity_list(lemmatize_text1, lemmatize_text2),\n","        dice_similarity_list(stopwords_text1, stopwords_text2),\n","        dice_similarity_list(synonyms_text1, synonyms_text2),\n","        dice_similarity_list(NES_column_text1, NES_column_text2),\n","        dice_similarity_list(name_entities_text1, name_entities_text2),\n","        dice_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n","        dice_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n","        dice_similarity_list(lesk_text1, lesk_text2),\n","        #jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n","        #jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n","        #jaccard_similarity_list(ngrams_text1, ngrams_text2),\n","        #jaccard_similarity_list(synset_text1, synset_text2),\n","\n","    ]\n","    return np.array(features)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Training**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get training dataset"]},{"cell_type":"code","execution_count":99,"metadata":{"id":"fifhUTGhRKBS"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2234, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>4.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Micron has declared its first quarterly profit...</td>\n","      <td>Micron's numbers also marked the first quarter...</td>\n","      <td>3.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The fines are part of failed Republican effort...</td>\n","      <td>Perry said he backs the Senate's efforts, incl...</td>\n","      <td>2.80</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n","      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n","      <td>2.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  But other sources close to the sale said Viven...   \n","1  Micron has declared its first quarterly profit...   \n","2  The fines are part of failed Republican effort...   \n","3  The American Anglican Council, which represent...   \n","4  The tech-loaded Nasdaq composite rose 20.96 po...   \n","\n","                                               text2   gs  \n","0  But other sources close to the sale said Viven... 4.00  \n","1  Micron's numbers also marked the first quarter... 3.75  \n","2  Perry said he backs the Senate's efforts, incl... 2.80  \n","3  The American Anglican Council, which represent... 3.40  \n","4  The technology-laced Nasdaq Composite Index <.... 2.40  "]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = get_dataset(train_path)\n","print(train_dataset.shape)\n","train_dataset.head()"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"data":{"text/plain":["(2234,)"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["y_train = train_dataset['gs'].values\n","y_train.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the training dataset"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:03<00:00, 743.90it/s] \n","100%|██████████| 2234/2234 [00:02<00:00, 756.30it/s] \n","100%|██████████| 2234/2234 [00:00<00:00, 3202.38it/s]\n"]},{"data":{"text/plain":["(18, 2234)"]},"execution_count":102,"metadata":{},"output_type":"execute_result"}],"source":["X_train_features: np.ndarray = get_features(train_dataset)\n","X_train_features.shape"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"data":{"text/plain":["(18, 2234)"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["X_train_features.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Testing**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get the test dataset"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2817, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The problem likely will mean corrective change...</td>\n","      <td>He said the problem needs to be corrected befo...</td>\n","      <td>4.40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The technology-laced Nasdaq Composite Index .I...</td>\n","      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n","      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n","      <td>3.60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SEC Chairman William Donaldson said there is a...</td>\n","      <td>\"I think there's a building confidence that th...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n","      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n","      <td>1.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  The problem likely will mean corrective change...   \n","1  The technology-laced Nasdaq Composite Index .I...   \n","2  \"It's a huge black eye,\" said publisher Arthur...   \n","3  SEC Chairman William Donaldson said there is a...   \n","4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n","\n","                                               text2   gs  \n","0  He said the problem needs to be corrected befo... 4.40  \n","1  The broad Standard & Poor's 500 Index .SPX inc... 0.80  \n","2  \"It's a huge black eye,\" Arthur Sulzberger, th... 3.60  \n","3  \"I think there's a building confidence that th... 3.40  \n","4  In New York, Vivendi shares were 1.4 percent d... 1.40  "]},"execution_count":104,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset = get_dataset(test_path)\n","print(test_dataset.shape)\n","test_dataset.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the test dataset"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2817/2817 [00:02<00:00, 1105.02it/s]\n","100%|██████████| 2817/2817 [00:02<00:00, 1075.14it/s]\n","100%|██████████| 2817/2817 [00:00<00:00, 5078.05it/s]\n"]},{"data":{"text/plain":["(18, 2817)"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["X_test_features: np.ndarray = get_features(test_dataset)\n","X_test_features.shape"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"data":{"text/plain":["(2817,)"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["y_test = test_dataset['gs'].values\n","y_test.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Normalize all features"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["# Normalize the data\n","scaler = StandardScaler()\n","scaler.fit(X_train_features.T)\n","X_train_features_norm = scaler.transform(X_train_features.T)\n","X_test_features_norm = scaler.transform(X_test_features.T)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train the model"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train_features shape:  (2234, 18)\n","y_train shape:  (2234,)\n","X_test_features shape:  (2817, 18)\n","y_test shape:  (2817,)\n"]}],"source":["# Print all shapes\n","print(\"X_train_features shape: \", X_train_features_norm.shape)\n","print(\"y_train shape: \", y_train.shape)\n","print(\"X_test_features shape: \", X_test_features_norm.shape)\n","print(\"y_test shape: \", y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Train a simple regression model"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":119,"metadata":{},"output_type":"execute_result"}],"source":["# Train\n","reg = LinearRegression()\n","reg.fit(X_train_features.T, y_train)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train pearson:  0.7110311945994099\n","Test pearson:  0.02565926912069541\n"]}],"source":["# Evaluate\n","y_pred_train = reg.predict(X_train_features)\n","y_pred_test = reg.predict(X_test_features)\n","\n","print(\"Train pearson: \", pearsonr(y_train, y_pred_train)[0])\n","print(\"Test pearson: \", pearsonr(y_test, y_pred_test)[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Train multiple regression models"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of regressors: 41\n"]}],"source":["# Select all of the models that we are going to use\n","REGRESSORS = [ c for c in REGRESSORS if c[0] != 'QuantileRegressor' ]\n","print(\"Number of regressors:\", len(REGRESSORS))"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[],"source":["# Build pearson score function\n","def pearsonr_scorer(y_true, y_pred):\n","    assert len(y_true) == len(y_pred)\n","    score = pearsonr(y_true, y_pred)[0]\n","    return score\n","\n","pearson_scorer = make_scorer(pearsonr_scorer)\n","pearson_scorer.__name__ = 'pearson_scorer'"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'tuple' object has no attribute '__name__'\n","Invalid Regressor(s)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 41/41 [00:13<00:00,  3.01it/s]\n"]}],"source":["# Fit all models\n","reg = LazyRegressor(predictions=True, regressors=REGRESSORS, custom_metric=pearsonr_scorer)\n","regresion_models, regresion_predictions = reg.fit(X_train_features_norm, X_test_features_norm, y_train, y_test)"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Adjusted R-Squared</th>\n","      <th>R-Squared</th>\n","      <th>RMSE</th>\n","      <th>Time Taken</th>\n","      <th>pearsonr_scorer</th>\n","    </tr>\n","    <tr>\n","      <th>Model</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>LassoLars</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.03</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>DummyRegressor</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.01</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>-0.02</td>\n","      <td>-0.02</td>\n","      <td>1.18</td>\n","      <td>0.03</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet</th>\n","      <td>-0.16</td>\n","      <td>-0.15</td>\n","      <td>1.26</td>\n","      <td>0.03</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>TweedieRegressor</th>\n","      <td>-0.55</td>\n","      <td>-0.54</td>\n","      <td>1.45</td>\n","      <td>0.37</td>\n","      <td>-0.07</td>\n","    </tr>\n","    <tr>\n","      <th>PoissonRegressor</th>\n","      <td>-0.66</td>\n","      <td>-0.65</td>\n","      <td>1.50</td>\n","      <td>0.87</td>\n","      <td>-0.04</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor</th>\n","      <td>-0.70</td>\n","      <td>-0.69</td>\n","      <td>1.52</td>\n","      <td>0.09</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>NuSVR</th>\n","      <td>-0.89</td>\n","      <td>-0.88</td>\n","      <td>1.61</td>\n","      <td>0.31</td>\n","      <td>-0.05</td>\n","    </tr>\n","    <tr>\n","      <th>SGDRegressor</th>\n","      <td>-0.90</td>\n","      <td>-0.89</td>\n","      <td>1.61</td>\n","      <td>0.03</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNetCV</th>\n","      <td>-0.91</td>\n","      <td>-0.90</td>\n","      <td>1.62</td>\n","      <td>0.21</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuit</th>\n","      <td>-0.92</td>\n","      <td>-0.91</td>\n","      <td>1.62</td>\n","      <td>0.01</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>BayesianRidge</th>\n","      <td>-0.93</td>\n","      <td>-0.92</td>\n","      <td>1.62</td>\n","      <td>0.07</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge</th>\n","      <td>-0.93</td>\n","      <td>-0.92</td>\n","      <td>1.62</td>\n","      <td>0.01</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuitCV</th>\n","      <td>-0.94</td>\n","      <td>-0.93</td>\n","      <td>1.63</td>\n","      <td>0.01</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>LassoCV</th>\n","      <td>-0.94</td>\n","      <td>-0.93</td>\n","      <td>1.63</td>\n","      <td>0.29</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>RidgeCV</th>\n","      <td>-0.94</td>\n","      <td>-0.93</td>\n","      <td>1.63</td>\n","      <td>0.05</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsIC</th>\n","      <td>-0.95</td>\n","      <td>-0.94</td>\n","      <td>1.63</td>\n","      <td>0.03</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsCV</th>\n","      <td>-0.95</td>\n","      <td>-0.94</td>\n","      <td>1.63</td>\n","      <td>0.06</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>TransformedTargetRegressor</th>\n","      <td>-0.95</td>\n","      <td>-0.94</td>\n","      <td>1.63</td>\n","      <td>0.03</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>LinearRegression</th>\n","      <td>-0.95</td>\n","      <td>-0.94</td>\n","      <td>1.63</td>\n","      <td>0.01</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>MLPRegressor</th>\n","      <td>-1.08</td>\n","      <td>-1.06</td>\n","      <td>1.68</td>\n","      <td>4.24</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>SVR</th>\n","      <td>-1.09</td>\n","      <td>-1.08</td>\n","      <td>1.69</td>\n","      <td>0.42</td>\n","      <td>-0.05</td>\n","    </tr>\n","    <tr>\n","      <th>HuberRegressor</th>\n","      <td>-1.10</td>\n","      <td>-1.08</td>\n","      <td>1.69</td>\n","      <td>0.24</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>RandomForestRegressor</th>\n","      <td>-1.17</td>\n","      <td>-1.16</td>\n","      <td>1.72</td>\n","      <td>1.62</td>\n","      <td>-0.07</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor</th>\n","      <td>-1.19</td>\n","      <td>-1.17</td>\n","      <td>1.73</td>\n","      <td>0.22</td>\n","      <td>-0.06</td>\n","    </tr>\n","    <tr>\n","      <th>LarsCV</th>\n","      <td>-1.19</td>\n","      <td>-1.17</td>\n","      <td>1.73</td>\n","      <td>0.12</td>\n","      <td>-0.01</td>\n","    </tr>\n","    <tr>\n","      <th>LinearSVR</th>\n","      <td>-1.20</td>\n","      <td>-1.19</td>\n","      <td>1.73</td>\n","      <td>0.04</td>\n","      <td>-0.00</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor</th>\n","      <td>-1.20</td>\n","      <td>-1.19</td>\n","      <td>1.73</td>\n","      <td>0.60</td>\n","      <td>-0.09</td>\n","    </tr>\n","    <tr>\n","      <th>BaggingRegressor</th>\n","      <td>-1.22</td>\n","      <td>-1.20</td>\n","      <td>1.74</td>\n","      <td>0.17</td>\n","      <td>-0.08</td>\n","    </tr>\n","    <tr>\n","      <th>LGBMRegressor</th>\n","      <td>-1.25</td>\n","      <td>-1.23</td>\n","      <td>1.75</td>\n","      <td>0.20</td>\n","      <td>-0.07</td>\n","    </tr>\n","    <tr>\n","      <th>HistGradientBoostingRegressor</th>\n","      <td>-1.25</td>\n","      <td>-1.23</td>\n","      <td>1.75</td>\n","      <td>0.85</td>\n","      <td>-0.07</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreesRegressor</th>\n","      <td>-1.26</td>\n","      <td>-1.24</td>\n","      <td>1.75</td>\n","      <td>0.65</td>\n","      <td>-0.08</td>\n","    </tr>\n","    <tr>\n","      <th>XGBRegressor</th>\n","      <td>-1.42</td>\n","      <td>-1.40</td>\n","      <td>1.82</td>\n","      <td>0.40</td>\n","      <td>-0.08</td>\n","    </tr>\n","    <tr>\n","      <th>DecisionTreeRegressor</th>\n","      <td>-1.90</td>\n","      <td>-1.88</td>\n","      <td>1.99</td>\n","      <td>0.05</td>\n","      <td>-0.07</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreeRegressor</th>\n","      <td>-1.99</td>\n","      <td>-1.97</td>\n","      <td>2.02</td>\n","      <td>0.04</td>\n","      <td>-0.09</td>\n","    </tr>\n","    <tr>\n","      <th>PassiveAggressiveRegressor</th>\n","      <td>-2.16</td>\n","      <td>-2.14</td>\n","      <td>2.07</td>\n","      <td>0.01</td>\n","      <td>0.07</td>\n","    </tr>\n","    <tr>\n","      <th>Lars</th>\n","      <td>-5.99</td>\n","      <td>-5.94</td>\n","      <td>3.09</td>\n","      <td>0.02</td>\n","      <td>0.07</td>\n","    </tr>\n","    <tr>\n","      <th>KernelRidge</th>\n","      <td>-9.31</td>\n","      <td>-9.24</td>\n","      <td>3.75</td>\n","      <td>0.20</td>\n","      <td>0.02</td>\n","    </tr>\n","    <tr>\n","      <th>RANSACRegressor</th>\n","      <td>-31.53</td>\n","      <td>-31.32</td>\n","      <td>6.66</td>\n","      <td>0.19</td>\n","      <td>-0.02</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianProcessRegressor</th>\n","      <td>-7059.39</td>\n","      <td>-7014.26</td>\n","      <td>98.15</td>\n","      <td>0.76</td>\n","      <td>0.02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               Adjusted R-Squared  R-Squared  RMSE  \\\n","Model                                                                \n","LassoLars                                   -0.02      -0.02  1.18   \n","DummyRegressor                              -0.02      -0.02  1.18   \n","Lasso                                       -0.02      -0.02  1.18   \n","ElasticNet                                  -0.16      -0.15  1.26   \n","TweedieRegressor                            -0.55      -0.54  1.45   \n","PoissonRegressor                            -0.66      -0.65  1.50   \n","AdaBoostRegressor                           -0.70      -0.69  1.52   \n","NuSVR                                       -0.89      -0.88  1.61   \n","SGDRegressor                                -0.90      -0.89  1.61   \n","ElasticNetCV                                -0.91      -0.90  1.62   \n","OrthogonalMatchingPursuit                   -0.92      -0.91  1.62   \n","BayesianRidge                               -0.93      -0.92  1.62   \n","Ridge                                       -0.93      -0.92  1.62   \n","OrthogonalMatchingPursuitCV                 -0.94      -0.93  1.63   \n","LassoCV                                     -0.94      -0.93  1.63   \n","RidgeCV                                     -0.94      -0.93  1.63   \n","LassoLarsIC                                 -0.95      -0.94  1.63   \n","LassoLarsCV                                 -0.95      -0.94  1.63   \n","TransformedTargetRegressor                  -0.95      -0.94  1.63   \n","LinearRegression                            -0.95      -0.94  1.63   \n","MLPRegressor                                -1.08      -1.06  1.68   \n","SVR                                         -1.09      -1.08  1.69   \n","HuberRegressor                              -1.10      -1.08  1.69   \n","RandomForestRegressor                       -1.17      -1.16  1.72   \n","KNeighborsRegressor                         -1.19      -1.17  1.73   \n","LarsCV                                      -1.19      -1.17  1.73   \n","LinearSVR                                   -1.20      -1.19  1.73   \n","GradientBoostingRegressor                   -1.20      -1.19  1.73   \n","BaggingRegressor                            -1.22      -1.20  1.74   \n","LGBMRegressor                               -1.25      -1.23  1.75   \n","HistGradientBoostingRegressor               -1.25      -1.23  1.75   \n","ExtraTreesRegressor                         -1.26      -1.24  1.75   \n","XGBRegressor                                -1.42      -1.40  1.82   \n","DecisionTreeRegressor                       -1.90      -1.88  1.99   \n","ExtraTreeRegressor                          -1.99      -1.97  2.02   \n","PassiveAggressiveRegressor                  -2.16      -2.14  2.07   \n","Lars                                        -5.99      -5.94  3.09   \n","KernelRidge                                 -9.31      -9.24  3.75   \n","RANSACRegressor                            -31.53     -31.32  6.66   \n","GaussianProcessRegressor                 -7059.39   -7014.26 98.15   \n","\n","                               Time Taken  pearsonr_scorer  \n","Model                                                       \n","LassoLars                            0.03              NaN  \n","DummyRegressor                       0.01              NaN  \n","Lasso                                0.03              NaN  \n","ElasticNet                           0.03            -0.06  \n","TweedieRegressor                     0.37            -0.07  \n","PoissonRegressor                     0.87            -0.04  \n","AdaBoostRegressor                    0.09            -0.06  \n","NuSVR                                0.31            -0.05  \n","SGDRegressor                         0.03             0.01  \n","ElasticNetCV                         0.21             0.02  \n","OrthogonalMatchingPursuit            0.01            -0.06  \n","BayesianRidge                        0.07             0.02  \n","Ridge                                0.01             0.02  \n","OrthogonalMatchingPursuitCV          0.01            -0.06  \n","LassoCV                              0.29             0.01  \n","RidgeCV                              0.05             0.03  \n","LassoLarsIC                          0.03             0.03  \n","LassoLarsCV                          0.06             0.03  \n","TransformedTargetRegressor           0.03             0.03  \n","LinearRegression                     0.01             0.03  \n","MLPRegressor                         4.24            -0.06  \n","SVR                                  0.42            -0.05  \n","HuberRegressor                       0.24             0.01  \n","RandomForestRegressor                1.62            -0.07  \n","KNeighborsRegressor                  0.22            -0.06  \n","LarsCV                               0.12            -0.01  \n","LinearSVR                            0.04            -0.00  \n","GradientBoostingRegressor            0.60            -0.09  \n","BaggingRegressor                     0.17            -0.08  \n","LGBMRegressor                        0.20            -0.07  \n","HistGradientBoostingRegressor        0.85            -0.07  \n","ExtraTreesRegressor                  0.65            -0.08  \n","XGBRegressor                         0.40            -0.08  \n","DecisionTreeRegressor                0.05            -0.07  \n","ExtraTreeRegressor                   0.04            -0.09  \n","PassiveAggressiveRegressor           0.01             0.07  \n","Lars                                 0.02             0.07  \n","KernelRidge                          0.20             0.02  \n","RANSACRegressor                      0.19            -0.02  \n","GaussianProcessRegressor             0.76             0.02  "]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["regresion_models"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPUPgMw9k8V23lSkem9miXR","provenance":[]},"kernelspec":{"display_name":"Python 3.8.15 ('ihlt')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"1837759eb970ca00fab7939d441a2c40fff3e44bbf988c5ed37db000776f3ff5"}}},"nbformat":4,"nbformat_minor":0}
