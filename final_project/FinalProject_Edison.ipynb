{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"A7q5DjXenGU7"},"source":["# Semantic textual similarity\n","## Final Project IHLT - UPC 2022/2023\n","### Authors : Rob - Edison Bejarano\n","\n","1. Data\n","\n","2. What we are doing?\n","#### Techniques for preprocessing text for similarity comparison\n","\n","- Stemming: is a process that involves reducing words to their base form, or stem, in order to normalize the text and remove variations in word endings. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the stem \"run\" by a stemming algorithm.\n","\n","\n","- Lemmatization: is a process that involves reducing words to their base form, or lemma, in order to normalize the text and remove variations in word endings. Unlike stemming, lemmatization takes into account the context of the word in order to determine its lemma, resulting in more accurate and meaningful reductions. For example, the words \"running,\" \"runs,\" and \"ran\" would all be reduced to the lemma \"run\" by a lemmatization algorithm.\n","\n","- Tf-idf weighting: Is a method for assigning a weight to each word in a document based on its relative importance. The weight is calculated by multiplying the term frequency (tf) of the word by the inverse document frequency (idf) of the word across all documents in a corpus. This weighting scheme gives higher weight to words that are more frequent within a document but less frequent across the corpus, making them more important for characterizing the document.\n","\n","- NES : Function used the Natural Language Toolkit (nltk) to identify named entities in a given sentence. The sentence parameter is the sentence in which named entities should be identified, and the binary parameter determines whether named entities should be grouped together or returned as individual tokens. The function returns a set of the named entities and individual words found in the sentence.\n","\n","\n","These techniques can be used in combination with each other or with stopwords removal to preprocess text and improve the accuracy of similarity comparison. For example, you could use stemming or lemmatization to normalize the words in the phrases, and then use tf-idf weighting to assign importance to each word based on its frequency within the phrases and across a larger corpus. This would allow you to compare the similarity of the phrases in a more meaningful and accurate way\n","\n","\n","3. Results"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Install packages"]},{"cell_type":"code","execution_count":313,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/bin/fish: /home/rob/miniconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `CXXABI_1.3.13' not found (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/bin/fish)\n","Note: you may need to restart the kernel to use updated packages.\n","/usr/bin/fish: /home/rob/miniconda3/lib/libtinfo.so.6: no version information available (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `CXXABI_1.3.13' not found (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/bin/fish)\n","/usr/bin/fish: /home/rob/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /usr/bin/fish)\n"]}],"source":["%pip install -q spacy nltk numpy pandas scikit-learn pyjarowinkler lazypredict\n","!python3 -m spacy download en_core_web_sm"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Libraries"]},{"cell_type":"code","execution_count":263,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":379,"status":"ok","timestamp":1670661492620,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"67vwZWxBcGZF","outputId":"9baddf20-b957-4e36-840a-a4e819f7f2d5"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /home/rob/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /home/rob/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package punkt to /home/rob/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/rob/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /home/rob/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /home/rob/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package gutenberg to /home/rob/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n","[nltk_data] Downloading package conll2000 to /home/rob/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n","[nltk_data] Downloading package brown to /home/rob/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package words to /home/rob/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":263,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import re\n","import nltk\n","import spacy\n","import string\n","import numpy as np\n","import pandas as pd\n","\n","from tqdm import tqdm\n","from itertools import chain\n","from functools import partial\n","from argparse import Namespace\n","from pyjarowinkler import distance\n","from collections.abc import Iterable\n","from nltk.wsd import lesk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.metrics import jaccard_distance\n","from nltk.corpus import stopwords, wordnet\n","from nltk import pos_tag, ne_chunk, Tree\n","from nltk.metrics.distance import jaccard_distance\n","from scipy.stats import pearsonr\n","\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import make_scorer\n","from typing import List\n","from lazypredict.Supervised import REGRESSORS, LazyRegressor\n","\n","nltk.download('wordnet')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","nltk.download('gutenberg')\n","nltk.download('conll2000')\n","nltk.download('brown')\n","nltk.download('words')"]},{"cell_type":"markdown","metadata":{"id":"TS52eWLnnJvr"},"source":["## Download data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1669293565602,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"k0hykeMvnNtn","outputId":"51914aeb-8ac8-45cb-d3ef-7d67bcd7a7c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  2003  100  2003    0     0  47690      0 --:--:-- --:--:-- --:--:-- 47690\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  122k  100  122k    0     0   505k      0 --:--:-- --:--:-- --:--:--  503k\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  115k  100  115k    0     0   345k      0 --:--:-- --:--:-- --:--:--  345k\n"]}],"source":["#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/trial.tgz https://gebakx.github.io/ihlt/sts/resources/trial.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/train.tgz https://gebakx.github.io/ihlt/sts/resources/train.tgz\n","#!curl -o /content/drive/MyDrive/Colab_Notebooks/2.IHLT/final_project/test-gold.tgz https://gebakx.github.io/ihlt/sts/resources/test-gold.tgz"]},{"cell_type":"markdown","metadata":{"id":"a91zUN7nIF_h"},"source":["# Bring data"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1203,"status":"ok","timestamp":1670661368611,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"jpDBKAHfpe-Z","outputId":"500addc2-c71a-4bfc-d5f4-ac57ba7d9dcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["train/\n","train/00-readme.txt\n","train/STS.output.MSRpar.txt\n","train/STS.input.SMTeuroparl.txt\n","train/STS.input.MSRpar.txt\n","train/STS.gs.MSRpar.txt\n","train/STS.input.MSRvid.txt\n","train/STS.gs.MSRvid.txt\n","train/correlation.pl\n","train/STS.gs.SMTeuroparl.txt\n","trial/\n","trial/STS.input.txt\n","trial/00-readme.txt\n","trial/STS.gs.txt\n","trial/STS.ouput.txt\n","test-gold/\n","test-gold/STS.input.MSRpar.txt\n","test-gold/STS.gs.MSRpar.txt\n","test-gold/STS.input.MSRvid.txt\n","test-gold/STS.gs.MSRvid.txt\n","test-gold/STS.input.SMTeuroparl.txt\n","test-gold/STS.gs.SMTeuroparl.txt\n","test-gold/STS.input.surprise.SMTnews.txt\n","test-gold/STS.gs.surprise.SMTnews.txt\n","test-gold/STS.input.surprise.OnWN.txt\n","test-gold/STS.gs.surprise.OnWN.txt\n","test-gold/STS.gs.ALL.txt\n","test-gold/00-readme.txt\n"]}],"source":["!tar zxvf ../final_project/train.tgz\n","!tar zxvf ../final_project/trial.tgz\n","!tar zxvf ../final_project/test-gold.tgz\n","\n","!rm ../final_project/train.tgz\n","!rm ../final_project/test-gold.tgz \n","!rm ../final_project/trial.tgz"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TLgFj_E7nKdQ"},"source":["# Usesful functions"]},{"cell_type":"code","execution_count":406,"metadata":{},"outputs":[],"source":["# ------------------------------ #\n","# Jaccard similarity Function\n","# ------------------------------ #\n","def jaccard_similarity(s1: List[str], s2: List[str]):\n","    return 1 - jaccard_distance(set(s1), set(s2))\n","\n","# ------------------------------ #\n","# Jaccard Similarity List\n","# ------------------------------ #\n","def jaccard_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    sims = []\n","    for l1, l2 in zip(s1, s2):\n","        sim = jaccard_similarity(l1, l2)\n","        sims.append(sim)\n","    return np.array(sims)\n","\n","\n","def dice_similarity(s1, s2):\n","    assert isinstance(s1, Iterable), f\"s1 must be an iterable, not {type(s1)}\"\n","    assert isinstance(s2, Iterable), f\"s2 must be an iterable, not {type(s2)}\"\n","    s1 = set(s1)\n","    s2 = set(s2)\n","    intersection = s1.intersection(s2)\n","    return 2 * len(intersection) / (len(s1) + len(s2))\n","\n","def dice_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    sims = []\n","    for l1, l2 in zip(s1, s2):\n","        sim = dice_similarity(l1, l2)\n","        sims.append(sim)\n","    return np.array(sims)\n","\n","# ------------------------------ #\n","# Jarowinkler Similarity\n","# ------------------------------ #   \n","def calculateJarowinklerSimilarity(dataframe, column1, column2):\n","\n","    aux = []\n","    for row in dataframe.itertuples():\n","            \n","        # Longest one selected\n","        if len(row[column1]) >= len(row[column2]):\n","            sentence1 = row[column1]\n","            sentence2 = row[column2]\n","        else:\n","            sentence1 = row[column2]\n","            sentence2 = row[column1]\n","\n","        similarities_array = []\n","        for word1 in sentence1:\n","            max = 0\n","\n","        for word2 in sentence2:\n","            similarity = distance.get_jaro_distance(str(word1), str(word2), winkler=True, scaling=0.1)\n","            \n","            if max < similarity:\n","                max = similarity\n","            \n","        similarities_array.append(max)\n","\n","        aux.append(np.array(similarities_array).mean())\n","\n","    return aux\n","\n","# ------------------------------ #\n","#       Overlap Similarity\n","# ------------------------------ # \n","def overlap_distance(sentence1, sentence2):\n","  # Zip the characters from the two strings together\n","  pairs = zip(sentence1, sentence2)\n","\n","  # Initialize a counter for the overlap distance\n","  overlap = 0\n","\n","  # Iterate over the pairs of characters\n","  for a, b in pairs:\n","    # If the characters are the same, increment the overlap counter\n","    if a == b:\n","      overlap += 1\n","\n","  # Return the overlap distance\n","  return overlap\n","\n","# ------------------------------ #\n","#    Overlap Similarity list\n","# ------------------------------ # \n","\n","def overlap_similarity_list(s1: List[List[str]], s2: List[List[str]]):\n","    sims = []\n","    for l1, l2 in zip(s1, s2):\n","        sim = overlap_distance(l1, l2)\n","        sims.append(sim)\n","    return np.array(sims)\n","\n"]},{"cell_type":"code","execution_count":385,"metadata":{"id":"1_VF58N6nQre"},"outputs":[],"source":["tag_dict = {\n","        \"NN\": \"n\",\n","        \"NNS\": \"n\",\n","        \"NNP\": \"n\",\n","        \"NNPS\": \"n\",\n","        \"VB\": \"v\",\n","        \"VBD\": \"v\",\n","        \"VBG\": \"v\",\n","        \"VBN\": \"v\",\n","        \"VBP\": \"v\",\n","        \"VBZ\": \"v\",\n","        \"RB\": \"r\",\n","        \"RBR\": \"r\",\n","        \"RBS\": \"r\",\n","        \"JJ\": \"a\",\n","        \"JJR\": \"a\",\n","        \"JJS\": \"a\",\n","  }\n","\n","# ------------------------------ #\n","#         Get Wordnet POS\n","# ------------------------------ #\n","def get_wordnet_pos(word):\n","  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","  tag = nltk.pos_tag([word])[0][1][0].upper()\n","  \n","        \n","  return tag_dict.get(tag, wordnet.NOUN)\n","\n","\n","#Auxiliar spacy\n","nlp = spacy.load('en_core_web_sm')\n","special_pattern = re.compile(r\"[^ \\nA-Za-z0-9À-ÖØ-öø-ÿЀ-ӿ/]+\")\n","\n","# ------------------------------ #\n","#   Function to tokenize spacy\n","# ------------------------------ #\n","def spacy_tokenize(sentence):\n","  return [ word.text.lower() for word in nlp.tokenizer(sentence) ]\n","\n","def tokenize_column_spacy(column):\n","  tokenize = [spacy_tokenize(sentence) for sentence in column]\n","  \n","  return tokenize\n","  \n","# ------------------------------ #\n","#   Function to lemmatize spacy\n","# ------------------------------ #\n","def spacy_lemmatize(sentence: str):\n","  return [ word.lemma_.lower() for word in nlp.tokenizer(sentence) ]\n","  \n","# ------------------------------ #\n","#   Function to tokenize\n","# ------------------------------ #\n","def tokenize_column(column):\n","    #put in lowercase\n","    tokenizator = [nltk.word_tokenize(sentence) for sentence in column]\n","    #Lowercase the tokens\n","    return [ [ word.lower() for word in sentence ] for sentence in tokenizator ]\n","\n","\n","#--------------------------------------------#\n","#  Function to NES\n","#--------------------------------------------#\n","def NES(sentence: str, binary: bool):\n","    x = nltk.pos_tag(nltk.word_tokenize(sentence))\n","    res = nltk.ne_chunk(x, binary=binary)\n","    necs_and_words = set()\n","    for chunk in res:\n","        if hasattr(chunk, 'label'):\n","            # Add NE\n","            token = ' '.join(term[0] for term in chunk)\n","            necs_and_words.add(token)\n","        else:\n","            token = chunk[0]\n","            if token.isalnum():\n","                necs_and_words.add(token.lower())\n","    return necs_and_words\n","\n"," #--------------------------------------------#\n"," # Function to get entities from a column\n"," # -------------------------------------------# \n","def get_entities_new(column):\n","    entities = []\n","    for sentence in column:\n","        entities.append(NES(sentence, False))\n","    return entities\n","\n","# ------------------------------ #\n","# Lemmatization text process\n","# ------------------------------ #\n","lemmatizer = WordNetLemmatizer()\n","# ------------------------------ #\n","#   Function to lemmatize\n","# ------------------------------ #\n","def lemmatize(tokenized_text: List[List[str]]):\n","  \n","  lemmas = []\n","\n","  for sentence in tqdm(tokenized_text):\n","    sentence_lemmas = []\n","    for word in sentence:\n","      sentence_lemmas.append(lemmatizer.lemmatize(word.lower(), get_wordnet_pos(word.lower())))\n","    lemmas.append(sentence_lemmas)\n","\n","  return lemmas\n","\n","# ------------------------------ #\n","#   Stopwords initialization\n","# ------------------------------ #\n","stopwords_list = set(nltk.corpus.stopwords.words(\"english\"))\n","stopwords_list = stopwords_list.union(set(string.punctuation))\n","stopwords_list = stopwords_list.union(set(['.', ',', ';', '.\"']))\n","\n","# ------------------------------ #\n","#   Function to remove stopwords\n","# ------------------------------ #\n","def remove_stopwords(column: List[List[str]]):\n","  #Lowercase the tokens\n","  return [ [ word.lower() for word in sentence if word not in stopwords_list ]  for sentence in column ]\n","\n","\n","# ------------------------------ #\n","#   Function to synonimize\n","# ------------------------------ #\n","def synonimize_column(column):\n","  #put in lowercase\n","  tokenized = [nltk.word_tokenize(sentence) for sentence in column]\n","  #Lowercase the tokens\n","  tokenized = [ [ word.lower() for word in sentence ] for sentence in tokenized ]\n","  #Synonimize\n","  synonimized = [ [ word for word in sentence if word not in stopwords_list ] for sentence in tokenized ]\n","\n","  return synonimized\n","\n","\n","# ------------------------------ #\n","#   Function to synset\n","# ------------------------------ #\n","def get_synset_column(tokenized_text: List[List[str]]):\n","  synset = []\n","  for sentence in tokenized_text:\n","    pos = nltk.pos_tag(sentence)\n","    lemmas = []\n","    for pair in pos:\n","      if pair[1][0] in tag_dict.keys():\n","        lemma = wnl.lemmatize(p[0].lower(), pos=p[1][0].lower())\n","        lemmas.append(lemma)\n","      else:\n","        lemma = pair[0]\n","        lemmas.append(lemma)\n","    synset.append(lemmas)\n","  \n","  return synset\n","\n","\n","# ------------------------------ #\n","#  Function to NE(Name entities)\n","# ------------------------------ #\n","def apply_ne(tokenized_text: List[str]):\n","    # tokenize the sentence and find the POS tag for each token\n","    sentences_ne = list(ne_chunk(pos_tag(tokenized_text), binary=True))\n","    result = []\n","    for el in sentences_ne:\n","        if isinstance(el, Tree):\n","            leaves = el.leaves()\n","            result.append(\" \".join(word[0] for word in leaves))\n","        else:\n","            result.append(el[0])\n","    return result\n","\n","# used apply_ne function to get NE from a column\n","def get_name_entities(column: List[List[str]]):\n","  ne = []\n","  for sentence in column:\n","    ne.append(apply_ne(sentence))\n","  return ne\n","\n","\n","\n","# ------------------------------ #\n","#  Function to get ngrams\n","# ------------------------------ #the\n","def get_ngrams_column(column: List[List[str]], n: int):\n","  ngrams = []\n","  for sentence in column:\n","    ngrams.append(apply_ngram(sentence, n))\n","  return ngrams\n","\n","\n","def apply_ngram(sentence: List[str], n: int):\n","    if len(sentence) < n:\n","        return [tuple(sentence)]\n","    return list(nltk.ngrams(sentence, n))\n","\n","\n","# ------------------------------ #\n","#     Function to get lesk \n","# ------------------------------ #\n","def get_lesk_column(column):\n","  lesk_text = []\n","\n","  for sentence in column:\n","    synset = [lesk(sentence, word) for word in sentence]\n","    synset = {word for word in synset if word is not None}\n","    lesk_text.append(synset)\n","\n","  return lesk_text\n","\n","\n","  \n","def apply_jaccard_lesk(sentence1: str, sentence2: str):\n","\n","  # Apply lesk to sentence 1\n","  synset1 = [ lesk(sentence1, word) for word in sentence1 ]\n","  synset1 = { word for word in synset1 if word is not None }\n","\n","  # Apply lesk to sentence 1\n","  synset2 = [ lesk(sentence2, word) for word in sentence2 ]\n","  synset2 = { word for word in synset2 if word is not None }\n","\n","  # Calculate distance\n","  distance = jaccard_distance(synset1, synset2)\n","\n","  return distance\n","\n","\n","def lemma_spacy(sentences):\n","  sentences = [special_chars_out(s) for s in sentences]\n","  token_lemmatize = [spacy_lemmatize(phrase) for phrase in sentences]\n","  return token_lemmatize\n","\n","\n","def special_chars_out(sentence: str):\n","  \n","  sentence = sentence.replace(\"'ve\", \" have\")\n","  sentence = sentence.replace(\"n't\", \" not\")\n","  sentence = sentence.replace(\"'ll\", \" will\")  \n","  sentence = sentence.replace(\"'m\", \" am\")  \n","  sentence = sentence.replace(\"'re\", \" are\")\n","  \n","  sentence = re.sub(special_pattern, \" \", sentence)  \n","\n","  return sentence"]},{"cell_type":"code","execution_count":266,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['My', 'Bonnie', 'White', 'lies', 'over', 'the', 'ocean', ',', 'in', 'Picadilli', 'Circus', 'at', '3:00pm', '.']\n","['My', 'Bonnie', 'lied', 'over', 'the', 'sea', '!', 'Over', 'the', 'sea', '...']\n","0.21052631578947367\n"]}],"source":["first = \"My Bonnie White lies over the ocean, in Picadilli Circus at 3:00pm.\"\n","second = \"My Bonnie lied over the sea! Over the sea...\"\n","\n","\n","p1 = nltk.word_tokenize(first)\n","p2 = nltk.word_tokenize(second)\n","\n","print(p1)\n","print(p2)\n","\n","test_frase_benet = jaccard_similarity(p1,p2)\n","print(test_frase_benet)"]},{"cell_type":"code","execution_count":267,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['I', 'am', 'a', 'student', 'of', 'the', 'University', 'of', 'Granada', 'and', 'that', 'is', 'in', 'that', 'city', ',', 'that', 'is', 'in', 'Spain', ',', 'The', 'artificial', 'beach', 'named', 'angelica', 'is', 'going', 'to', 'be', 'super', 'cool', '.']\n"]}],"source":["# aply the function apply_ne to a phrase\n","phrase = \"I am a student of the University of Granada and that is in that city, that is in Spain, The artificial beach named angelica is going to be super cool.\"\n","#tokenize the phrase\n","ne = apply_ne(phrase)\n","print(ne)"]},{"cell_type":"code","execution_count":268,"metadata":{},"outputs":[],"source":["# Functions of preprocessing\n","def read_data(text_datas: List[str], gs_datas: List[str]):\n","  all_df_text = []\n","  for text_data, gs_data in zip(text_datas, gs_datas):\n","    df_text = pd.read_csv(text_data, sep=r'\\t', engine='python', header=None)\n","    df_text.columns = [\"text1\", \"text2\"]\n","    df_text['gs'] = pd.read_csv(gs_data, sep='\\t', header=None)\n","    all_df_text.append(df_text.dropna())\n","  return pd.concat(all_df_text)\n","\n","def get_dataset(path: str) -> pd.DataFrame:\n","  files = sorted(os.listdir(path))\n","  input_files = [ os.path.join(path, file) for file in files if 'input' in file ]\n","  gs_files = [ os.path.join(path, file) for file in files if 'gs' in file ]\n","  df = read_data(input_files, gs_files)\n","  return df"]},{"cell_type":"markdown","metadata":{"id":"wm2VxdulsYgE"},"source":["# Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"TfqKCsKhN3fR"},"source":["### Data information\n","- trial : includes the definition of the scores, a sample of 5 sentence pairs and the input and output formats. It is not needed, but it is useful for prototyping.\n","\n","- train : training data from paraphrasing data sets, input and output formats.\n","\n","- test : test data from paraphrasing data sets."]},{"cell_type":"code","execution_count":306,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1670661604957,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"QdEvE55Xsb0L"},"outputs":[],"source":["train_path = '../final_project/train'\n","trial_path = '../final_project/trial'\n","test_path  = '../final_project/test-gold'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Similarities**"]},{"cell_type":"code","execution_count":378,"metadata":{},"outputs":[],"source":["\n","train_dataset_pruebas = get_dataset(train_path)\n","test_dataset_pruebas = get_dataset(test_path)\n","df = train_dataset_pruebas\n"]},{"cell_type":"code","execution_count":379,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:02<00:00, 763.01it/s] \n","100%|██████████| 2234/2234 [00:02<00:00, 765.09it/s] \n","100%|██████████| 2234/2234 [00:00<00:00, 3198.12it/s]\n"]}],"source":["# Tokenization features\n","tokenized_text1 = tokenize_column(df['text1'])\n","tokenized_text2 = tokenize_column(df['text2'])\n","\n","# Lemmatization features\n","lemmatize_text1 = lemmatize(tokenized_text1)\n","lemmatize_text2 = lemmatize(tokenized_text1)\n","\n","\n","#Use stopwords function to remove stopwords\n","stopwords_text1 = remove_stopwords(tokenized_text1)\n","stopwords_text2 = remove_stopwords(tokenized_text2)\n","\n","\n","\n","# Synonyms features\n","synonyms_text1 = []\n","synonyms_text2 = []\n","# Use sysnstesizer to get synonyms\n","for i in tqdm(range(len(tokenized_text1))):\n","    synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n","    synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n","\n","\n","# Synonyms features another way\n","synonimized_text1_new = synonimize_column(df['text1'])\n","synonimized_text2_new = synonimize_column(df['text2'])\n","\n","\n","# NES features\n","NES_column_text1 = get_entities_new(df['text1'])\n","NES_column_text2 = get_entities_new(df['text2'])\n","\n","# Name entities features\n","name_entities_text1 = get_name_entities(tokenized_text1)\n","name_entities_text2 = get_name_entities(tokenized_text2)\n","\n","ngrams_column_2_text1 = get_ngrams_column(tokenized_text1, 2)\n","ngrams_column_2_text2 = get_ngrams_column(tokenized_text2, 2)\n","\n","ngrams_column_3_text1 = get_ngrams_column(tokenized_text1, 3)\n","ngrams_column_3_text2 = get_ngrams_column(tokenized_text2, 3)\n","\n","# Lesk features\n","lesk_text1 = get_lesk_column(tokenized_text1)\n","lesk_text2 = get_lesk_column(tokenized_text2)\n","\n","# Spacy features\n","spacy_tok_text1 = tokenize_column_spacy(df['text1'])\n","spacy_tok_text2 = tokenize_column_spacy(df['text2'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:03<00:00, 700.94it/s] \n","100%|██████████| 2234/2234 [00:02<00:00, 745.92it/s] \n"]},{"name":"stdout","output_type":"stream","text":["['but', 'other', 'source', 'close', 'to', 'the', 'sale', 'said', 'vivendi', 'wa', 'keeping', 'the', 'door', 'open', 'to', 'further', 'bid', 'and', 'hoped', 'to', 'see', 'bidder', 'interested', 'in', 'individual', 'asset', 'team', 'up', '.']\n","['but', 'other', 'source', 'close', 'to', 'the', 'sale', 'said', 'vivendi', 'wa', 'keeping', 'the', 'door', 'open', 'for', 'further', 'bid', 'in', 'the', 'next', 'day', 'or', 'two', '.']\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 3132.87it/s]\n"]}],"source":["\n","# Lemmatization features\n","lemmatize_text1 = lemmatize(df['text1'])\n","lemmatize_text2 = lemmatize(df['text2'])\n","print(lemmatize_text1[0])\n","print(lemmatize_text2[0])\n","\n","#Use stopwords function to remove stopwords\n","stopwords_text1 = remove_stopwords(df['text1'])\n","stopwords_text2 = remove_stopwords(df['text2'])\n","\n","\n","\n","# Synonyms features\n","synonyms_text1 = []\n","synonyms_text2 = []\n","# Use sysnstesizer to get synonyms\n","for i in tqdm(range(len(tokenized_text1))):\n","    synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n","    synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n","\n","\n","# Synonyms features another way\n","synonimized_text1_new = synonimize_column(df['text1'])\n","synonimized_text2_new = synonimize_column(df['text2'])\n","\n","\n","# NES features\n","NES_column_text1 = get_entities_new(df['text1'])\n","NES_column_text2 = get_entities_new(df['text2'])\n","\n","# Name entities features\n","name_entities_text1 = get_name_entities(df['text1'])\n","name_entities_text2 = get_name_entities(df['text2'])\n","\n","ngrams_column_2_text1 = get_ngrams_column(tokenized_text1, 2)\n","ngrams_column_2_text2 = get_ngrams_column(tokenized_text2, 2)\n","\n","ngrams_column_3_text1 = get_ngrams_column(tokenized_text1, 3)\n","ngrams_column_3_text2 = get_ngrams_column(tokenized_text2, 3)\n","\n","# Lesk features\n","lesk_text1 = get_lesk_column(tokenized_text1)\n","lesk_text2 = get_lesk_column(tokenized_text2)\n","\n","\n","\n","# Synset features\n","#synset_text1 = synset_column(df['text1'])\n","#synset_text2 = synset_column(df['text2'])"]},{"cell_type":"code","execution_count":380,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 2715.35it/s]\n"]}],"source":["lemma_synonyms_text1 = []\n","lemma_synonyms_text2 = []\n","# Use sysnstesizer to get synonyms\n","for i in tqdm(range(len(tokenized_text1))):\n","    lemma_synonyms_text1.append([syn for w in lemmatize_text1[i] for syn in wordnet.synsets(w)])\n","    lemma_synonyms_text2.append([syn for w in lemmatize_text2[i] for syn in wordnet.synsets(w)])\n"]},{"cell_type":"code","execution_count":410,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["16\n"]}],"source":["d = overlap_distance(tokenized_text1[0], tokenized_text2[0])\n","#o = overlap_similarity_list(tokenized_text1, tokenized_text2)\n","\n","print(d)\n","#print(o[0])"]},{"cell_type":"code","execution_count":272,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Jaccard similarity tokenized:  [0.5483871  0.42105263 0.34782609]\n","Jaccard similarity lemmatize:  [1. 1. 1.]\n","Jaccard similarity stopwords:  [0.47368421 0.46153846 0.33333333]\n","Jaccard similarity synonyms:  [0.67680608 0.30172414 0.38562092]\n","Jaccard similarity synonyms new:  [0.47368421 0.46153846 0.33333333]\n","Jaccard similarity name entities:  [0.5483871  0.42105263 0.33333333]\n","Jaccard similarity ngrams 2:  [0.37837838 0.13043478 0.20689655]\n","Jaccard similarity ngrams 3:  [0.32432432 0.04347826 0.1       ]\n","Jaccard similarity lesk:  [0.35714286 0.3125     0.22222222]\n"]}],"source":["# Jaccard similarity features\n","jaccard_similarity_tokenized = jaccard_similarity_list(tokenized_text1, tokenized_text2)\n","jaccard_similarity_synonyms_new = jaccard_similarity_list(synonimized_text1_new, synonimized_text2_new)\n","jaccard_similarity_NES = jaccard_similarity_list(NES_column_text1, NES_column_text2)\n","jaccard_similarity_lemmatize = jaccard_similarity_list(lemmatize_text1, lemmatize_text2)\n","jaccard_similarity_stopwords = jaccard_similarity_list(stopwords_text1, stopwords_text2)\n","jaccard_similarity_synonyms = jaccard_similarity_list(synonyms_text1, synonyms_text2)\n","jaccard_similarity_name_entities = jaccard_similarity_list(name_entities_text1, name_entities_text2)\n","jaccard_similarity_ngrams_2 = jaccard_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2)\n","jaccard_similarity_ngrams_3 = jaccard_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2)\n","jaccard_similarity_lesk = jaccard_similarity_list(lesk_text1, lesk_text2)\n","\n","print(\"Jaccard similarity tokenized: \", jaccard_similarity_tokenized[:3])\n","print(\"Jaccard similarity lemmatize: \", jaccard_similarity_lemmatize[:3])\n","print(\"Jaccard similarity stopwords: \", jaccard_similarity_stopwords[:3])\n","print(\"Jaccard similarity synonyms: \", jaccard_similarity_synonyms[:3])\n","print(\"Jaccard similarity synonyms new: \", jaccard_similarity_synonyms_new[:3])\n","print(\"Jaccard similarity name entities: \", jaccard_similarity_name_entities[:3])\n","print(\"Jaccard similarity ngrams 2: \", jaccard_similarity_ngrams_2[:3])\n","print(\"Jaccard similarity ngrams 3: \", jaccard_similarity_ngrams_3[:3])\n","print(\"Jaccard similarity lesk: \", jaccard_similarity_lesk[:3])"]},{"cell_type":"code","execution_count":390,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":388,"status":"error","timestamp":1670662891072,"user":{"displayName":"Edison Bejarano Sepulveda","userId":"16974971435170261412"},"user_tz":-60},"id":"0tx_t_KsL_Fl","outputId":"842eff73-ae95-4a78-d46c-ecf589fd656c"},"outputs":[],"source":["def get_features(df: pd.DataFrame):\n","\n","    #--------------------------------------------#\n","    # 0. NLTK Words features\n","    #--------------------------------------------#\n","    #print(\"NLTK Words features\")\n","    \n","    #nltk_words_text1 = []\n","    #nltk_words_text2 = []\n","\n","    #--------------------------------------------#\n","    # 1. Tokenize features\n","    #--------------------------------------------#    \n","    tokenized_text1 = tokenize_column(df['text1'])\n","    tokenized_text2 = tokenize_column(df['text2'])\n","\n","    #--------------------------------------------#\n","    # 2. Lemmatize features\n","    #--------------------------------------------#\n","    lemmatize_text1 = lemmatize(tokenized_text1)\n","    lemmatize_text2 = lemmatize(tokenized_text2)\n","\n","\n","    #--------------------------------------------#\n","    # 3. Stopwords features\n","    #--------------------------------------------#   \n","    stopwords_text1 = remove_stopwords(tokenized_text1)\n","    stopwords_text2 = remove_stopwords(tokenized_text2)\n","\n","    #--------------------------------------------#\n","    # 4. Synonims features\n","    #--------------------------------------------#\n","    synonyms_text1 = []\n","    synonyms_text2 = []\n","    # Use sysnstesizer to get synonyms\n","    for i in tqdm(range(len(tokenized_text1))):\n","        synonyms_text1.append([syn for w in tokenized_text1[i] for syn in wordnet.synsets(w)])\n","        synonyms_text2.append([syn for w in tokenized_text2[i] for syn in wordnet.synsets(w)])\n","\n","    \n","    #--------------------------------------------#\n","    # 5. NES features\n","    #--------------------------------------------#\n","    NES_column_text1 = get_entities_new(df['text1'])\n","    NES_column_text2 = get_entities_new(df['text2'])\n","\n","    \n","    #--------------------------------------------#\n","    # 6. Name entities features\n","    #--------------------------------------------#\n","    name_entities_text1 = get_name_entities(tokenized_text1)\n","    name_entities_text2 = get_name_entities(tokenized_text2)\n","\n","    #--------------------------------------------#\n","    # 7. Ngrams features\n","    #--------------------------------------------#\n","\n","    ngrams_column_2_text1 = get_ngrams_column(tokenized_text1, 2)\n","    ngrams_column_2_text2 = get_ngrams_column(tokenized_text2, 2)\n","\n","    ngrams_column_3_text1 = get_ngrams_column(tokenized_text1, 3)\n","    ngrams_column_3_text2 = get_ngrams_column(tokenized_text2, 3)\n","\n","    ngrams_column_4_text1 = get_ngrams_column(tokenized_text1, 4)\n","    ngrams_column_4_text2 = get_ngrams_column(tokenized_text2, 4)\n","\n","    ngrams_column_5_text1 = get_ngrams_column(tokenized_text1, 5)\n","    ngrams_column_5_text2 = get_ngrams_column(tokenized_text2, 5)\n","\n","    ngrams_column_6_text1 = get_ngrams_column(tokenized_text1, 6)\n","    ngrams_column_6_text2 = get_ngrams_column(tokenized_text2, 6)\n","\n","    ngrams_column_7_text1 = get_ngrams_column(tokenized_text1, 7)\n","    ngrams_column_7_text2 = get_ngrams_column(tokenized_text2, 7)\n","\n","    ngrams_column_8_text1 = get_ngrams_column(tokenized_text1, 8)\n","    ngrams_column_8_text2 = get_ngrams_column(tokenized_text2, 8)\n","\n","    ngrams_column_9_text1 = get_ngrams_column(tokenized_text1, 9)\n","    ngrams_column_9_text2 = get_ngrams_column(tokenized_text2, 9)\n","\n","    #--------------------------------------------#\n","    # 8. Lesk features\n","    #--------------------------------------------#\n","    # Lesk features\n","    lesk_text1 = get_lesk_column(tokenized_text1)\n","    lesk_text2 = get_lesk_column(tokenized_text2)\n","\n","    # --------------------------------------------#\n","    # 9. Spacy words features\n","    # --------------------------------------------#\n","    print(\"Spacy words features\")\n","    spacy_words_text1 = tokenize_column_spacy(df['text1'])\n","    spacy_words_text2 = tokenize_column_spacy(df['text2'])\n","\n","    # --------------------------------------------#\n","    # 10. Spacy lemmatize features\n","    # --------------------------------------------#\n","    print(\"Spacy lemmatize features\")\n","    spacy_lemmatize_text1 = lemma_spacy(df['text1'])\n","    spacy_lemmatize_text2 = lemma_spacy(df['text2'])\n","\n","    #--------------------------------------------#\n","    # 11.Lemma synonyms features\n","    #--------------------------------------------#\n","    lemma_synonyms_text1 = []\n","    lemma_synonyms_text2 = []\n","    # Use sysnstesizer to get synonyms\n","    for i in tqdm(range(len(tokenized_text1))):\n","        lemma_synonyms_text1.append([syn for w in lemmatize_text1[i] for syn in wordnet.synsets(w)])\n","        lemma_synonyms_text2.append([syn for w in lemmatize_text2[i] for syn in wordnet.synsets(w)])\n","\n","    #print(\"Word synonyms features\"\n","\n","    #--------------------------------------------#\n","    # 12. Synset features\n","    #--------------------------------------------#\n","    print(\"Synset features\")\n","    synset_text1 = get_synset_column(tokenized_text1)\n","    synset_text2 = get_synset_column(tokenized_text2)\n","\n","\n","    features = [\n","        # Jaccard similarity\n","        jaccard_similarity_list(tokenized_text1, tokenized_text2),\n","        jaccard_similarity_list(lemmatize_text1, lemmatize_text2),\n","        jaccard_similarity_list(stopwords_text1, stopwords_text2),\n","        jaccard_similarity_list(synonyms_text1, synonyms_text2),\n","        jaccard_similarity_list(NES_column_text1, NES_column_text2),\n","        jaccard_similarity_list(name_entities_text1, name_entities_text2),\n","        jaccard_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n","        jaccard_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n","        jaccard_similarity_list(ngrams_column_4_text1, ngrams_column_4_text2),\n","        jaccard_similarity_list(ngrams_column_5_text1, ngrams_column_5_text2),\n","        jaccard_similarity_list(ngrams_column_6_text1, ngrams_column_6_text2),\n","        jaccard_similarity_list(ngrams_column_7_text1, ngrams_column_7_text2),\n","        jaccard_similarity_list(ngrams_column_8_text1, ngrams_column_8_text2),\n","        jaccard_similarity_list(ngrams_column_9_text1, ngrams_column_9_text2),\n","        jaccard_similarity_list(lesk_text1, lesk_text2),\n","        jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n","        jaccard_similarity_list(spacy_lemmatize_text1, spacy_lemmatize_text2),\n","\n","\n","        # jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n","        # jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n","        # jaccard_similar\n","        jaccard_similarity_list(lemma_synonyms_text1,lemma_synonyms_text2),\n","        jaccard_similarity_list(synset_text1, synset_text2),\n","        #jaccard_similarity_list(synset_text1, synset_text2),\n","        \n","        \n","        # Dice similarity\n","        dice_similarity_list(tokenized_text1, tokenized_text2),\n","        dice_similarity_list(lemmatize_text1, lemmatize_text2),\n","        dice_similarity_list(stopwords_text1, stopwords_text2),\n","        dice_similarity_list(synonyms_text1, synonyms_text2),\n","        dice_similarity_list(NES_column_text1, NES_column_text2),\n","        dice_similarity_list(name_entities_text1, name_entities_text2),\n","        dice_similarity_list(ngrams_column_2_text1, ngrams_column_2_text2),\n","        dice_similarity_list(ngrams_column_3_text1, ngrams_column_3_text2),\n","        dice_similarity_list(ngrams_column_4_text1, ngrams_column_4_text2),\n","        dice_similarity_list(ngrams_column_5_text1, ngrams_column_5_text2),\n","        dice_similarity_list(ngrams_column_6_text1, ngrams_column_6_text2),\n","        dice_similarity_list(ngrams_column_7_text1, ngrams_column_7_text2),\n","        dice_similarity_list(ngrams_column_8_text1, ngrams_column_8_text2),\n","        dice_similarity_list(ngrams_column_9_text1, ngrams_column_9_text2),\n","        dice_similarity_list(lesk_text1, lesk_text2),\n","        dice_similarity_list(spacy_words_text1, spacy_words_text2),\n","        dice_similarity_list(spacy_lemmatize_text1, spacy_lemmatize_text2),\n","\n","        #jaccard_similarity_list(nltk_words_text1, nltk_words_text2),\n","        #jaccard_similarity_list(spacy_words_text1, spacy_words_text2),\n","        #jaccard_similarity_\n","        dice_similarity_list(lemma_synonyms_text1,lemma_synonyms_text2),\n","        dice_similarity_list(synset_text1, synset_text2),\n","        #jaccard_similarity_list(synset_text1, synset_text2),\n","\n","    ]\n","    return np.array(features)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Training**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get training dataset"]},{"cell_type":"code","execution_count":391,"metadata":{"id":"fifhUTGhRKBS"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2234, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>But other sources close to the sale said Viven...</td>\n","      <td>4.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Micron has declared its first quarterly profit...</td>\n","      <td>Micron's numbers also marked the first quarter...</td>\n","      <td>3.75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The fines are part of failed Republican effort...</td>\n","      <td>Perry said he backs the Senate's efforts, incl...</td>\n","      <td>2.80</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>The American Anglican Council, which represent...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The tech-loaded Nasdaq composite rose 20.96 po...</td>\n","      <td>The technology-laced Nasdaq Composite Index &lt;....</td>\n","      <td>2.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  But other sources close to the sale said Viven...   \n","1  Micron has declared its first quarterly profit...   \n","2  The fines are part of failed Republican effort...   \n","3  The American Anglican Council, which represent...   \n","4  The tech-loaded Nasdaq composite rose 20.96 po...   \n","\n","                                               text2   gs  \n","0  But other sources close to the sale said Viven... 4.00  \n","1  Micron's numbers also marked the first quarter... 3.75  \n","2  Perry said he backs the Senate's efforts, incl... 2.80  \n","3  The American Anglican Council, which represent... 3.40  \n","4  The technology-laced Nasdaq Composite Index <.... 2.40  "]},"execution_count":391,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset = get_dataset(train_path)\n","print(train_dataset.shape)\n","train_dataset.head()"]},{"cell_type":"code","execution_count":392,"metadata":{},"outputs":[{"data":{"text/plain":["(2234,)"]},"execution_count":392,"metadata":{},"output_type":"execute_result"}],"source":["y_train = train_dataset['gs'].values\n","y_train.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the training dataset"]},{"cell_type":"code","execution_count":393,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:02<00:00, 796.32it/s] \n","100%|██████████| 2234/2234 [00:02<00:00, 832.77it/s] \n","100%|██████████| 2234/2234 [00:00<00:00, 3349.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Spacy words features\n","Spacy lemmatize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2234/2234 [00:00<00:00, 2983.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Synset features\n"]},{"data":{"text/plain":["(38, 2234)"]},"execution_count":393,"metadata":{},"output_type":"execute_result"}],"source":["X_train_features: np.ndarray = get_features(train_dataset)\n","X_train_features.shape"]},{"cell_type":"code","execution_count":394,"metadata":{},"outputs":[{"data":{"text/plain":["(38, 2234)"]},"execution_count":394,"metadata":{},"output_type":"execute_result"}],"source":["X_train_features.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Testing**"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get the test dataset"]},{"cell_type":"code","execution_count":395,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3108, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text1</th>\n","      <th>text2</th>\n","      <th>gs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The problem likely will mean corrective change...</td>\n","      <td>He said the problem needs to be corrected befo...</td>\n","      <td>4.40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The technology-laced Nasdaq Composite Index .I...</td>\n","      <td>The broad Standard &amp; Poor's 500 Index .SPX inc...</td>\n","      <td>0.80</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"It's a huge black eye,\" said publisher Arthur...</td>\n","      <td>\"It's a huge black eye,\" Arthur Sulzberger, th...</td>\n","      <td>3.60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SEC Chairman William Donaldson said there is a...</td>\n","      <td>\"I think there's a building confidence that th...</td>\n","      <td>3.40</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Vivendi shares closed 1.9 percent at 15.80 eur...</td>\n","      <td>In New York, Vivendi shares were 1.4 percent d...</td>\n","      <td>1.40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               text1  \\\n","0  The problem likely will mean corrective change...   \n","1  The technology-laced Nasdaq Composite Index .I...   \n","2  \"It's a huge black eye,\" said publisher Arthur...   \n","3  SEC Chairman William Donaldson said there is a...   \n","4  Vivendi shares closed 1.9 percent at 15.80 eur...   \n","\n","                                               text2   gs  \n","0  He said the problem needs to be corrected befo... 4.40  \n","1  The broad Standard & Poor's 500 Index .SPX inc... 0.80  \n","2  \"It's a huge black eye,\" Arthur Sulzberger, th... 3.60  \n","3  \"I think there's a building confidence that th... 3.40  \n","4  In New York, Vivendi shares were 1.4 percent d... 1.40  "]},"execution_count":395,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset = get_dataset(test_path)\n","print(test_dataset.shape)\n","test_dataset.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Get features of the test dataset"]},{"cell_type":"code","execution_count":396,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3108/3108 [00:02<00:00, 1213.19it/s]\n","100%|██████████| 3108/3108 [00:02<00:00, 1239.73it/s]\n","100%|██████████| 3108/3108 [00:00<00:00, 4544.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Spacy words features\n","Spacy lemmatize features\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3108/3108 [00:00<00:00, 5050.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Synset features\n"]},{"data":{"text/plain":["(38, 3108)"]},"execution_count":396,"metadata":{},"output_type":"execute_result"}],"source":["X_test_features: np.ndarray = get_features(test_dataset)\n","X_test_features.shape"]},{"cell_type":"code","execution_count":397,"metadata":{},"outputs":[{"data":{"text/plain":["(3108,)"]},"execution_count":397,"metadata":{},"output_type":"execute_result"}],"source":["y_test = test_dataset['gs'].values\n","y_test.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Normalize all features"]},{"cell_type":"code","execution_count":398,"metadata":{},"outputs":[],"source":["# Normalize the data\n","scaler = StandardScaler()\n","scaler.fit(X_train_features.T)\n","X_train_features_norm = scaler.transform(X_train_features.T)\n","X_test_features_norm = scaler.transform(X_test_features.T)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train the model"]},{"cell_type":"code","execution_count":399,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train_features shape:  (2234, 38)\n","y_train shape:  (2234,)\n","X_test_features shape:  (3108, 38)\n","y_test shape:  (3108,)\n"]}],"source":["# Print all shapes\n","print(\"X_train_features shape: \", X_train_features_norm.shape)\n","print(\"y_train shape: \", y_train.shape)\n","print(\"X_test_features shape: \", X_test_features_norm.shape)\n","print(\"y_test shape: \", y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Train a simple regression model"]},{"cell_type":"code","execution_count":400,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-19 {color: black;background-color: white;}#sk-container-id-19 pre{padding: 0;}#sk-container-id-19 div.sk-toggleable {background-color: white;}#sk-container-id-19 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-19 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-19 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-19 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-19 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-19 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-19 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-19 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-19 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-19 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-19 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-19 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-19 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-19 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-19 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-19 div.sk-item {position: relative;z-index: 1;}#sk-container-id-19 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-19 div.sk-item::before, #sk-container-id-19 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-19 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-19 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-19 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-19 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-19 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-19 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-19 div.sk-label-container {text-align: center;}#sk-container-id-19 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-19 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-19\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" checked><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"],"text/plain":["LinearRegression()"]},"execution_count":400,"metadata":{},"output_type":"execute_result"}],"source":["# Train\n","reg = LinearRegression()\n","reg.fit(X_train_features_norm, y_train)"]},{"cell_type":"code","execution_count":401,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train pearson:  0.7316572243439259\n","Test pearson:  0.5870344728661504\n"]}],"source":["# Evaluate\n","y_pred_train = reg.predict(X_train_features_norm)\n","y_pred_test = reg.predict(X_test_features_norm)\n","\n","print(\"Train pearson: \", pearsonr(y_train, y_pred_train)[0])\n","print(\"Test pearson: \", pearsonr(y_test, y_pred_test)[0])"]},{"cell_type":"markdown","metadata":{},"source":["### Train multiple regression models"]},{"cell_type":"code","execution_count":402,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of regressors: 41\n"]}],"source":["# Select all of the models that we are going to use\n","REGRESSORS = [ c for c in REGRESSORS if c[0] != 'QuantileRegressor' ]\n","print(\"Number of regressors:\", len(REGRESSORS))"]},{"cell_type":"code","execution_count":403,"metadata":{},"outputs":[],"source":["# Build pearson score function\n","def pearsonr_scorer(y_true, y_pred):\n","    assert len(y_true) == len(y_pred)\n","    score = pearsonr(y_true, y_pred)[0]\n","    return score\n","\n","pearson_scorer = make_scorer(pearsonr_scorer)\n","pearson_scorer.__name__ = 'pearson_scorer'"]},{"cell_type":"code","execution_count":404,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["'tuple' object has no attribute '__name__'\n","Invalid Regressor(s)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 41/41 [00:21<00:00,  1.88it/s]\n"]}],"source":["# Fit all models\n","reg = LazyRegressor(predictions=True, regressors=REGRESSORS, custom_metric=pearsonr_scorer)\n","regresion_models, regresion_predictions = reg.fit(X_train_features_norm, X_test_features_norm, y_train, y_test)"]},{"cell_type":"code","execution_count":405,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Adjusted R-Squared</th>\n","      <th>R-Squared</th>\n","      <th>RMSE</th>\n","      <th>Time Taken</th>\n","      <th>pearsonr_scorer</th>\n","    </tr>\n","    <tr>\n","      <th>Model</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>RandomForestRegressor</th>\n","      <td>0.35</td>\n","      <td>0.35</td>\n","      <td>1.10</td>\n","      <td>2.53</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreesRegressor</th>\n","      <td>0.33</td>\n","      <td>0.34</td>\n","      <td>1.11</td>\n","      <td>1.02</td>\n","      <td>0.66</td>\n","    </tr>\n","    <tr>\n","      <th>GradientBoostingRegressor</th>\n","      <td>0.33</td>\n","      <td>0.34</td>\n","      <td>1.11</td>\n","      <td>1.05</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>LGBMRegressor</th>\n","      <td>0.31</td>\n","      <td>0.32</td>\n","      <td>1.13</td>\n","      <td>0.22</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>HistGradientBoostingRegressor</th>\n","      <td>0.30</td>\n","      <td>0.31</td>\n","      <td>1.14</td>\n","      <td>5.53</td>\n","      <td>0.65</td>\n","    </tr>\n","    <tr>\n","      <th>BaggingRegressor</th>\n","      <td>0.31</td>\n","      <td>0.32</td>\n","      <td>1.13</td>\n","      <td>0.30</td>\n","      <td>0.64</td>\n","    </tr>\n","    <tr>\n","      <th>AdaBoostRegressor</th>\n","      <td>0.35</td>\n","      <td>0.36</td>\n","      <td>1.10</td>\n","      <td>0.17</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>NuSVR</th>\n","      <td>0.32</td>\n","      <td>0.33</td>\n","      <td>1.12</td>\n","      <td>0.35</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>XGBRegressor</th>\n","      <td>0.24</td>\n","      <td>0.25</td>\n","      <td>1.19</td>\n","      <td>0.35</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>SVR</th>\n","      <td>0.29</td>\n","      <td>0.30</td>\n","      <td>1.14</td>\n","      <td>0.50</td>\n","      <td>0.63</td>\n","    </tr>\n","    <tr>\n","      <th>KNeighborsRegressor</th>\n","      <td>0.28</td>\n","      <td>0.28</td>\n","      <td>1.16</td>\n","      <td>0.10</td>\n","      <td>0.62</td>\n","    </tr>\n","    <tr>\n","      <th>LinearSVR</th>\n","      <td>0.22</td>\n","      <td>0.23</td>\n","      <td>1.20</td>\n","      <td>0.12</td>\n","      <td>0.60</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsIC</th>\n","      <td>0.30</td>\n","      <td>0.31</td>\n","      <td>1.14</td>\n","      <td>0.05</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLarsCV</th>\n","      <td>0.30</td>\n","      <td>0.30</td>\n","      <td>1.14</td>\n","      <td>0.07</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>LassoCV</th>\n","      <td>0.27</td>\n","      <td>0.28</td>\n","      <td>1.16</td>\n","      <td>0.41</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>BayesianRidge</th>\n","      <td>0.27</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.10</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>Ridge</th>\n","      <td>0.27</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.01</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>KernelRidge</th>\n","      <td>-6.01</td>\n","      <td>-5.92</td>\n","      <td>3.61</td>\n","      <td>0.41</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>LarsCV</th>\n","      <td>0.30</td>\n","      <td>0.31</td>\n","      <td>1.14</td>\n","      <td>0.12</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuit</th>\n","      <td>0.28</td>\n","      <td>0.29</td>\n","      <td>1.15</td>\n","      <td>0.01</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNetCV</th>\n","      <td>0.27</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.48</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>HuberRegressor</th>\n","      <td>0.24</td>\n","      <td>0.24</td>\n","      <td>1.19</td>\n","      <td>0.23</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>RidgeCV</th>\n","      <td>0.26</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.06</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>LinearRegression</th>\n","      <td>0.26</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.01</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>TransformedTargetRegressor</th>\n","      <td>0.26</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.01</td>\n","      <td>0.59</td>\n","    </tr>\n","    <tr>\n","      <th>OrthogonalMatchingPursuitCV</th>\n","      <td>0.27</td>\n","      <td>0.28</td>\n","      <td>1.16</td>\n","      <td>0.04</td>\n","      <td>0.58</td>\n","    </tr>\n","    <tr>\n","      <th>ElasticNet</th>\n","      <td>0.20</td>\n","      <td>0.21</td>\n","      <td>1.22</td>\n","      <td>0.05</td>\n","      <td>0.58</td>\n","    </tr>\n","    <tr>\n","      <th>SGDRegressor</th>\n","      <td>0.25</td>\n","      <td>0.26</td>\n","      <td>1.18</td>\n","      <td>0.05</td>\n","      <td>0.58</td>\n","    </tr>\n","    <tr>\n","      <th>MLPRegressor</th>\n","      <td>0.20</td>\n","      <td>0.21</td>\n","      <td>1.22</td>\n","      <td>4.31</td>\n","      <td>0.57</td>\n","    </tr>\n","    <tr>\n","      <th>PoissonRegressor</th>\n","      <td>0.26</td>\n","      <td>0.27</td>\n","      <td>1.17</td>\n","      <td>0.99</td>\n","      <td>0.55</td>\n","    </tr>\n","    <tr>\n","      <th>DecisionTreeRegressor</th>\n","      <td>-0.07</td>\n","      <td>-0.06</td>\n","      <td>1.41</td>\n","      <td>0.10</td>\n","      <td>0.55</td>\n","    </tr>\n","    <tr>\n","      <th>TweedieRegressor</th>\n","      <td>0.25</td>\n","      <td>0.26</td>\n","      <td>1.18</td>\n","      <td>0.48</td>\n","      <td>0.54</td>\n","    </tr>\n","    <tr>\n","      <th>ExtraTreeRegressor</th>\n","      <td>-0.16</td>\n","      <td>-0.14</td>\n","      <td>1.47</td>\n","      <td>0.04</td>\n","      <td>0.49</td>\n","    </tr>\n","    <tr>\n","      <th>PassiveAggressiveRegressor</th>\n","      <td>-0.33</td>\n","      <td>-0.31</td>\n","      <td>1.57</td>\n","      <td>0.01</td>\n","      <td>0.47</td>\n","    </tr>\n","    <tr>\n","      <th>RANSACRegressor</th>\n","      <td>-0.61</td>\n","      <td>-0.59</td>\n","      <td>1.73</td>\n","      <td>0.32</td>\n","      <td>0.46</td>\n","    </tr>\n","    <tr>\n","      <th>Lars</th>\n","      <td>-259.24</td>\n","      <td>-256.06</td>\n","      <td>21.97</td>\n","      <td>0.03</td>\n","      <td>0.03</td>\n","    </tr>\n","    <tr>\n","      <th>GaussianProcessRegressor</th>\n","      <td>-31436.62</td>\n","      <td>-31052.13</td>\n","      <td>241.49</td>\n","      <td>1.04</td>\n","      <td>-0.03</td>\n","    </tr>\n","    <tr>\n","      <th>Lasso</th>\n","      <td>-0.05</td>\n","      <td>-0.04</td>\n","      <td>1.39</td>\n","      <td>0.04</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>DummyRegressor</th>\n","      <td>-0.05</td>\n","      <td>-0.04</td>\n","      <td>1.39</td>\n","      <td>0.01</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>LassoLars</th>\n","      <td>-0.05</td>\n","      <td>-0.04</td>\n","      <td>1.39</td>\n","      <td>0.02</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                               Adjusted R-Squared  R-Squared   RMSE  \\\n","Model                                                                 \n","RandomForestRegressor                        0.35       0.35   1.10   \n","ExtraTreesRegressor                          0.33       0.34   1.11   \n","GradientBoostingRegressor                    0.33       0.34   1.11   \n","LGBMRegressor                                0.31       0.32   1.13   \n","HistGradientBoostingRegressor                0.30       0.31   1.14   \n","BaggingRegressor                             0.31       0.32   1.13   \n","AdaBoostRegressor                            0.35       0.36   1.10   \n","NuSVR                                        0.32       0.33   1.12   \n","XGBRegressor                                 0.24       0.25   1.19   \n","SVR                                          0.29       0.30   1.14   \n","KNeighborsRegressor                          0.28       0.28   1.16   \n","LinearSVR                                    0.22       0.23   1.20   \n","LassoLarsIC                                  0.30       0.31   1.14   \n","LassoLarsCV                                  0.30       0.30   1.14   \n","LassoCV                                      0.27       0.28   1.16   \n","BayesianRidge                                0.27       0.27   1.17   \n","Ridge                                        0.27       0.27   1.17   \n","KernelRidge                                 -6.01      -5.92   3.61   \n","LarsCV                                       0.30       0.31   1.14   \n","OrthogonalMatchingPursuit                    0.28       0.29   1.15   \n","ElasticNetCV                                 0.27       0.27   1.17   \n","HuberRegressor                               0.24       0.24   1.19   \n","RidgeCV                                      0.26       0.27   1.17   \n","LinearRegression                             0.26       0.27   1.17   \n","TransformedTargetRegressor                   0.26       0.27   1.17   \n","OrthogonalMatchingPursuitCV                  0.27       0.28   1.16   \n","ElasticNet                                   0.20       0.21   1.22   \n","SGDRegressor                                 0.25       0.26   1.18   \n","MLPRegressor                                 0.20       0.21   1.22   \n","PoissonRegressor                             0.26       0.27   1.17   \n","DecisionTreeRegressor                       -0.07      -0.06   1.41   \n","TweedieRegressor                             0.25       0.26   1.18   \n","ExtraTreeRegressor                          -0.16      -0.14   1.47   \n","PassiveAggressiveRegressor                  -0.33      -0.31   1.57   \n","RANSACRegressor                             -0.61      -0.59   1.73   \n","Lars                                      -259.24    -256.06  21.97   \n","GaussianProcessRegressor                -31436.62  -31052.13 241.49   \n","Lasso                                       -0.05      -0.04   1.39   \n","DummyRegressor                              -0.05      -0.04   1.39   \n","LassoLars                                   -0.05      -0.04   1.39   \n","\n","                               Time Taken  pearsonr_scorer  \n","Model                                                       \n","RandomForestRegressor                2.53             0.67  \n","ExtraTreesRegressor                  1.02             0.66  \n","GradientBoostingRegressor            1.05             0.65  \n","LGBMRegressor                        0.22             0.65  \n","HistGradientBoostingRegressor        5.53             0.65  \n","BaggingRegressor                     0.30             0.64  \n","AdaBoostRegressor                    0.17             0.63  \n","NuSVR                                0.35             0.63  \n","XGBRegressor                         0.35             0.63  \n","SVR                                  0.50             0.63  \n","KNeighborsRegressor                  0.10             0.62  \n","LinearSVR                            0.12             0.60  \n","LassoLarsIC                          0.05             0.59  \n","LassoLarsCV                          0.07             0.59  \n","LassoCV                              0.41             0.59  \n","BayesianRidge                        0.10             0.59  \n","Ridge                                0.01             0.59  \n","KernelRidge                          0.41             0.59  \n","LarsCV                               0.12             0.59  \n","OrthogonalMatchingPursuit            0.01             0.59  \n","ElasticNetCV                         0.48             0.59  \n","HuberRegressor                       0.23             0.59  \n","RidgeCV                              0.06             0.59  \n","LinearRegression                     0.01             0.59  \n","TransformedTargetRegressor           0.01             0.59  \n","OrthogonalMatchingPursuitCV          0.04             0.58  \n","ElasticNet                           0.05             0.58  \n","SGDRegressor                         0.05             0.58  \n","MLPRegressor                         4.31             0.57  \n","PoissonRegressor                     0.99             0.55  \n","DecisionTreeRegressor                0.10             0.55  \n","TweedieRegressor                     0.48             0.54  \n","ExtraTreeRegressor                   0.04             0.49  \n","PassiveAggressiveRegressor           0.01             0.47  \n","RANSACRegressor                      0.32             0.46  \n","Lars                                 0.03             0.03  \n","GaussianProcessRegressor             1.04            -0.03  \n","Lasso                                0.04              NaN  \n","DummyRegressor                       0.01              NaN  \n","LassoLars                            0.02              NaN  "]},"execution_count":405,"metadata":{},"output_type":"execute_result"}],"source":["regresion_models.sort_values(by='pearsonr_scorer', ascending=False)"]},{"cell_type":"code","execution_count":423,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 3.03948815\n","Iteration 2, loss = 1.52205760\n","Iteration 3, loss = 1.01258261\n","Iteration 4, loss = 0.77674076\n","Iteration 5, loss = 0.67618250\n","Iteration 6, loss = 0.59800196\n","Iteration 7, loss = 0.55090673\n","Iteration 8, loss = 0.52676833\n","Iteration 9, loss = 0.50642458\n","Iteration 10, loss = 0.51563406\n","Iteration 11, loss = 0.49087451\n","Iteration 12, loss = 0.46937004\n","Iteration 13, loss = 0.44942276\n","Iteration 14, loss = 0.43873619\n","Iteration 15, loss = 0.42683126\n","Iteration 16, loss = 0.44512938\n","Iteration 17, loss = 0.41409897\n","Iteration 18, loss = 0.40046986\n","Iteration 19, loss = 0.40419851\n","Iteration 20, loss = 0.40088978\n","Iteration 21, loss = 0.39059042\n","Iteration 22, loss = 0.38274293\n","Iteration 23, loss = 0.38530324\n","Iteration 24, loss = 0.37764409\n","Iteration 25, loss = 0.37416559\n","Iteration 26, loss = 0.37676852\n","Iteration 27, loss = 0.38983758\n","Iteration 28, loss = 0.39971261\n","Iteration 29, loss = 0.36995542\n","Iteration 30, loss = 0.37056348\n","Iteration 31, loss = 0.37068264\n","Iteration 32, loss = 0.36893599\n","Iteration 33, loss = 0.35560580\n","Iteration 34, loss = 0.34378705\n","Iteration 35, loss = 0.34633427\n","Iteration 36, loss = 0.34329354\n","Iteration 37, loss = 0.35156420\n","Iteration 38, loss = 0.33787491\n","Iteration 39, loss = 0.33485592\n","Iteration 40, loss = 0.33487051\n","Iteration 41, loss = 0.34756919\n","Iteration 42, loss = 0.33004760\n","Iteration 43, loss = 0.33950704\n","Iteration 44, loss = 0.34157595\n","Iteration 45, loss = 0.34748469\n","Iteration 46, loss = 0.33419137\n","Iteration 47, loss = 0.33629580\n","Iteration 48, loss = 0.32540149\n","Iteration 49, loss = 0.33343779\n","Iteration 50, loss = 0.32370370\n","Iteration 51, loss = 0.32295619\n","Iteration 52, loss = 0.30743418\n","Iteration 53, loss = 0.31640096\n","Iteration 54, loss = 0.30875582\n","Iteration 55, loss = 0.33377343\n","Iteration 56, loss = 0.31915034\n","Iteration 57, loss = 0.31303735\n","Iteration 58, loss = 0.31938894\n","Iteration 59, loss = 0.31793330\n","Iteration 60, loss = 0.29585316\n","Iteration 61, loss = 0.30370724\n","Iteration 62, loss = 0.30003179\n","Iteration 63, loss = 0.29052779\n","Iteration 64, loss = 0.28697067\n","Iteration 65, loss = 0.29058539\n","Iteration 66, loss = 0.30059200\n","Iteration 67, loss = 0.29108826\n","Iteration 68, loss = 0.30190484\n","Iteration 69, loss = 0.29593416\n","Iteration 70, loss = 0.29469650\n","Iteration 71, loss = 0.30571231\n","Iteration 72, loss = 0.28606360\n","Iteration 73, loss = 0.28383614\n","Iteration 74, loss = 0.28553418\n","Iteration 75, loss = 0.29152889\n","Iteration 76, loss = 0.26803502\n","Iteration 77, loss = 0.27702525\n","Iteration 78, loss = 0.27159707\n","Iteration 79, loss = 0.27507137\n","Iteration 80, loss = 0.28507435\n","Iteration 81, loss = 0.26950527\n","Iteration 82, loss = 0.26437455\n","Iteration 83, loss = 0.25958774\n","Iteration 84, loss = 0.26313253\n","Iteration 85, loss = 0.27534164\n","Iteration 86, loss = 0.27981764\n","Iteration 87, loss = 0.26473188\n","Iteration 88, loss = 0.27512994\n","Iteration 89, loss = 0.26379267\n","Iteration 90, loss = 0.27143084\n","Iteration 91, loss = 0.27110369\n","Iteration 92, loss = 0.25995588\n","Iteration 93, loss = 0.25448766\n","Iteration 94, loss = 0.25438711\n","Iteration 95, loss = 0.25374916\n","Iteration 96, loss = 0.25524230\n","Iteration 97, loss = 0.26479830\n","Iteration 98, loss = 0.26264549\n","Iteration 99, loss = 0.26543514\n","Iteration 100, loss = 0.25655032\n","Iteration 101, loss = 0.24414343\n","Iteration 102, loss = 0.25141135\n","Iteration 103, loss = 0.24871090\n","Iteration 104, loss = 0.25102197\n","Iteration 105, loss = 0.24846701\n","Iteration 106, loss = 0.29050446\n","Iteration 107, loss = 0.25880816\n","Iteration 108, loss = 0.23365748\n","Iteration 109, loss = 0.23749736\n","Iteration 110, loss = 0.24349545\n","Iteration 111, loss = 0.23963924\n","Iteration 112, loss = 0.22836931\n","Iteration 113, loss = 0.23662861\n","Iteration 114, loss = 0.24125440\n","Iteration 115, loss = 0.25771264\n","Iteration 116, loss = 0.22588911\n","Iteration 117, loss = 0.23393633\n","Iteration 118, loss = 0.22663077\n","Iteration 119, loss = 0.23167965\n","Iteration 120, loss = 0.22881716\n","Iteration 121, loss = 0.23100343\n","Iteration 122, loss = 0.22631664\n","Iteration 123, loss = 0.22181231\n","Iteration 124, loss = 0.24077504\n","Iteration 125, loss = 0.22239523\n","Iteration 126, loss = 0.22383310\n","Iteration 127, loss = 0.23549670\n","Iteration 128, loss = 0.21602528\n","Iteration 129, loss = 0.21371982\n","Iteration 130, loss = 0.21131607\n","Iteration 131, loss = 0.21421650\n","Iteration 132, loss = 0.20817009\n","Iteration 133, loss = 0.19803780\n","Iteration 134, loss = 0.20327733\n","Iteration 135, loss = 0.20930639\n","Iteration 136, loss = 0.20339683\n","Iteration 137, loss = 0.20166048\n","Iteration 138, loss = 0.19882517\n","Iteration 139, loss = 0.19920352\n","Iteration 140, loss = 0.19973454\n","Iteration 141, loss = 0.20908698\n","Iteration 142, loss = 0.20194557\n","Iteration 143, loss = 0.21360720\n","Iteration 144, loss = 0.21425027\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Test pearson:  0.5824131507248917\n"]}],"source":["# Train MLP model\n","mlp = MLPRegressor(hidden_layer_sizes=(200, 100, 50), learning_rate='adaptive', max_iter=1000, verbose=True)\n","mlp.fit(X_train_features_norm, y_train)\n","y_pred_test = mlp.predict(X_test_features_norm)\n","print(\"Test pearson: \", pearsonr(y_test, y_pred_test)[0])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPUPgMw9k8V23lSkem9miXR","provenance":[]},"kernelspec":{"display_name":"Python 3.8.15 ('ihlt')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"1837759eb970ca00fab7939d441a2c40fff3e44bbf988c5ed37db000776f3ff5"}}},"nbformat":4,"nbformat_minor":0}
