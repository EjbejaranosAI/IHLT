{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EjbejaranosAI/IHLT/blob/main/YT_Finetuning_Dolly_GPT_j6B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Dolly GPT-J-6B with LoRa\n",
        "\n",
        "LoRa paper - https://arxiv.org/abs/2106.09685"
      ],
      "metadata": {
        "id": "1jWr34kdl3AP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCrBi94qAfFY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/gururise/AlpacaDataCleaned.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_N29PQadBd18"
      },
      "outputs": [],
      "source": [
        "ls AlpacaDataCleaned/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lebYnT2AAukB"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip uninstall transformers\n",
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install git+https://github.com/huggingface/peft.git\n",
        "!pip -q install bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Tokenizer"
      ],
      "metadata": {
        "id": "jAbHqhQamMAN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKPVZV6IBHhi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "data = load_dataset(\"json\", data_files=\"./AlpacaDataCleaned/alpaca_data.json\")\n",
        "\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    # taken from https://github.com/tloen/alpaca-lora\n",
        "    if data_point[\"instruction\"]:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Input:\n",
        "{data_point[\"input\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{data_point[\"instruction\"]}\n",
        "\n",
        "### Response:\n",
        "{data_point[\"output\"]}\"\"\"\n",
        "\n",
        "\n",
        "data = data.map(lambda data_point: {\"prompt\": tokenizer(generate_prompt(data_point))})\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRHBoBIIBtqB"
      },
      "source": [
        "## Finetuning Dolly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSfkRYg9BZkU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, GPTJForCausalLM\n",
        "\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3gD0PMGVtLxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfRmmxGBB0-h"
      },
      "outputs": [],
      "source": [
        "# Settings for A100 - For 3090 \n",
        "MICRO_BATCH_SIZE = 8  # change to 4 for 3090\n",
        "BATCH_SIZE = 128\n",
        "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
        "EPOCHS = 2  # paper uses 3\n",
        "LEARNING_RATE = 2e-5  \n",
        "CUTOFF_LEN = 256  \n",
        "LORA_R = 4\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZSJQ3PtDQVU"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\",\n",
        "                                          add_eos_token=True, \n",
        "                                          )\n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\",\n",
        "                                  load_in_8bit=True,\n",
        "                                  device_map=\"auto\", \n",
        "                                  )\n",
        "\n",
        "\n",
        "model = prepare_model_for_int8_training(model, use_gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS4jUAJpDf-u"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "\n",
        "data = load_dataset(\"json\", data_files=\"./AlpacaDataCleaned/alpaca_data_cleaned.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jWp32UaDy1E"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle().map(\n",
        "    lambda data_point: tokenizer(\n",
        "        generate_prompt(data_point),\n",
        "        truncation=True,\n",
        "        max_length=CUTOFF_LEN,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8iz9ku1FIIW"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlw7IxfUED0a"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=100,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"lora-dolly\",\n",
        "        save_total_limit=3,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train(resume_from_checkpoint=False)\n",
        "\n",
        "model.save_pretrained(\"gptj6b-lora-dolly\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaHvPdAUERM5"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LNokCSvFgm7"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"ejbejaranos/dolly-lora-EN-ArepacaV1\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(\"ejbejaranos/dolly-ArepacaV1\", use_auth_token=True)"
      ],
      "metadata": {
        "id": "4OvkurCOl4QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1yDXCFNmM5"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"hackathon-somos-nlp-2023/dolly-lora-ArepacaV1\", use_auth_token=True)\n",
        "tokenizer.push_to_hub(\"hackathon-somos-nlp-2023/dolly-ArepacaV1\", use_auth_token=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}